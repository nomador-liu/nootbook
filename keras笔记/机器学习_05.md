# 机器学习

[TOC]



## 卷积神经网络（CNN）入门

1. 为什么学习卷积神经网络？

计算机视觉（Computer Vision）是近年来深度学习推动下发展最快的领域之一。
它的应用随处可见：

* **自动驾驶**：识别周围的车辆和行人，从而避免碰撞。
* **人脸识别**：如今解锁手机甚至开门，都可以仅靠“刷脸”完成。
* **推荐与筛选**：手机上的美食、酒店、风景照片展示，背后都有深度学习帮你筛选“最吸引人”的图片。
* **艺术创作**：神经网络还能创造新的艺术风格图像。

学习计算机视觉的两个主要理由：

1. **开启全新应用**：许多几年前还不可能的应用，如今已成为现实。掌握这些工具，你也能创造新的产品。
2. **启发跨领域创新**：即使不直接从事计算机视觉，CV 领域的创新网络结构和算法也能给语音识别、自然语言处理等其他方向带来灵感。

---

2. 计算机视觉中的典型任务

2.1 **图像分类（Image Classification）**
   输入一张图片（如 64×64），判断它是猫还是狗。

2.2 **目标检测（Object Detection）**
   不仅识别图像中有什么对象，还要定位它们。例如自动驾驶中，不只是知道有“车”，还要知道“车在哪里”，并用边框框出多个目标。

2.3 **风格迁移（Neural Style Transfer）**
   把一幅图像的“内容”与另一幅图像的“风格”结合。例如用毕加索的画风重新绘制一张照片。

这些任务展示了 CNN 不仅能识别，还能创造和重构。

---

3. 为什么要用卷积？

3.1 图像数据规模问题

* 小图像（64×64，RGB 三通道）：输入特征数 = 64×64×3 = 12,288。
* 大图像（1000×1000，RGB）：输入特征数 = 1000×1000×3 = 3,000,000。

如果直接用全连接网络：

* 假设第一层有 1000 个隐藏单元
* 权重矩阵大小 = 1000 × 3,000,000 = 30 亿参数！

问题：

* 数据量不足，极易过拟合
* 计算和存储需求极高，不可行

3.2 卷积操作的优势

卷积运算（Convolution）是 CNN 的核心构件。它通过 **局部连接** 和 **参数共享** 大幅减少参数数量，使得大图像也能高效处理。

---

4. 小结

* 卷积神经网络解决了传统全连接网络在图像任务上的参数爆炸问题。
* 它能支撑图像分类、目标检测、风格迁移等多种任务。
* 学习 CNN 不仅能帮助你进入计算机视觉领域，还能启发其他 AI 研究方向。

## 卷积操作

CNN最重要的是构建块是卷积层：直接连接图片输入的卷积层不会连接到图像中的每个像素，而只是只与某个框定范围内的像素相连接；同样，第二卷积层的每个神经元也仅连接第一层中某个框定范围内的神经元；这种允许神经网络关注第一个隐藏层的低阶特征，然后在下一个隐藏层中将它们组装成高阶特征，以此类推。

CNN在图像识别方便效果好的原因之一是现实中的图像也有层次结构。

![矩形范围内连接的CNN层](./images/CNN/p1.png)

注意：之前研究的所有多层神经网络都具有由一长串神经元组成的层，必须将输入图像展平为一维，然后再将其输入神经网络。在CNN中，每一层都以二维形式表示，这使得将神经元与其相应的输入进行匹配变得更加容易。

![图片的层次结构1](./images/CNN/p2.png)

### 边缘检测

![卷积操作例子](./images/CNN/p3.png)

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.signal import convolve2d, correlate2d  # correlate2d 互相关
import tensorflow as tf

K_vertical = np.array([[ 1,  0, -1],
                       [ 1,  0, -1],
                       [ 1,  0, -1]], dtype=np.float32)

some_data = np.array([[3,0,1,2,7,4],
                      [1,5,8,9,3,1],
                      [2,7,2,5,1,3],
                      [0,1,3,1,7,8],
                      [4,2,1,6,2,8],
                      [2,4,5,2,3,9]])

print(convolve2d(some_data, K_vertical, mode="valid"))   # 严格数学的卷积
print(correlate2d(some_data, K_vertical, mode="valid"))  # 深度学习的卷积其实是 交叉相关（correlate2d）

# 深度学习框架的卷积
some_data_tensor = tf.constant(some_data.reshape(1,6,6,1), dtype=tf.float32)
K_vertical_tensor = tf.constant(K_vertical.reshape(3,3,1,1), dtype=tf.float32)

# tf.nn.conv2d: tensorflow的卷积操作
y = tf.nn.conv2d(some_data_tensor, K_vertical_tensor, strides=1, padding='VALID')
y.numpy().squeeze()
```

<img alt="垂直边缘检测" height="500" src="./images/CNN/p4.png" width="500"/>

```python
from sklearn.datasets import load_sample_images
image = load_sample_images()["images"][1]
```

```python
plt.imshow(image)
plt.axis("off")
plt.show()
```

```python
gray_image =  (0.299*image[...,0] + 0.587*image[...,1] + 0.114*image[...,2]).astype(np.float32)  # 彩色图片并转为灰度
gray_image.shape
```

```python
plt.imshow(gray_image,cmap="gray")
plt.axis("off")
plt.show()
```

```python
# 构建一个3*3的卷积窗口（滤波器）

K_vertical = np.array([[ 1,  0, -1],
                       [ 1,  0, -1],
                       [ 1,  0, -1]], dtype=np.float32)

K_horizontal = np.array([[ 1,  1,  1],
                         [ 0,  0,  0],
                         [-1, -1, -1]], dtype=np.float32)


# 构建 7*7的卷积窗口
d = np.array([-3, -2, -1, 0, 1, 2, 3], dtype=np.float32)
s = np.array([ 1,  6, 15,20,15, 6, 1], dtype=np.float32)
K_vertical_fancy  = np.outer(s, d)   # 7x7 垂直边缘检测核
K_horizontal_fancy = K_vertical_fancy.T             # 7x7 水平边缘核

edge_v = correlate2d(gray_image, K_vertical_fancy, mode='valid')  # mode="valid" 不做任何填充
edge_h = convolve2d(gray_image, K_horizontal_fancy, mode='valid')
```

```python
plt.figure(figsize=(10,10))
plt.subplot(121)
plt.imshow(edge_v, cmap="gray")
plt.axis("off")
plt.title("vertical edge")

plt.subplot(122)
plt.imshow(edge_h, cmap="gray")
plt.axis("off")
plt.title("horizontal edge")

plt.show()
```

|上面例子可以看出，可以自己构造滤波器（filter）去对图片做卷积操作，实现对图片的特征提取（垂直边缘检测/水平边缘检测）， 在神经网络中，滤波器的参数是通过反向传播去更新优化的，除了初始化，不需要去自己指定；所以它在优化过程中会根据任务学会怎么提取图片的特征

### Padding（填充）

- 卷积中的 Padding（填充）

在卷积神经网络中，卷积操作通常会使输入图像的尺寸缩小。例如，一个 $6 \times 6$ 的图像与一个 $3 \times 3$ 卷积核做卷积，得到的输出是 $4 \times 4$。一般公式为：

$$
\text{输出大小} = n - f + 1
$$

其中 $n$ 是输入大小，$f$ 是卷积核大小。

- 不使用填充的问题

1. **尺寸不断缩小**：每次卷积都会减少图像的边长，深层网络中图像可能很快缩小到无法使用。
2. **边缘信息损失**：图像边缘或角落的像素在卷积中被利用的次数远少于中心区域，导致边缘特征信息被“弱化”。

- 填充的解决方案

为了解决上述问题，可以在输入的四周补上一圈像素（通常为 0），即 **padding**。

* 例如，将 $6 \times 6$ 的输入补一圈（p=1），变为 $8 \times 8$，再用 $3 \times 3$ 卷积，就能得到与原输入相同大小的 $6 \times 6$ 输出。
* 一般公式为：

$$
\text{输出大小} = n + 2p - f + 1
$$

- 两种常见的卷积方式

1. **Valid 卷积**：不做填充 ($p=0$)，输出尺寸会变小。
2. **Same 卷积**：选择合适的填充，使输出与输入保持相同大小。

   * 当卷积核大小 $f$ 为奇数时，所需填充量为：

   $$
   p = \frac{f-1}{2}
   $$

   例如，$f=3$ 时，$p=1$；$f=5$ 时，$p=2$。

- 为什么卷积核常取奇数大小

* 奇数核有自然的中心像素点，方便定义卷积的“中心”。
* 避免了左右/上下不对称填充的麻烦。
* 因此常见的卷积核有 $3 \times 3$、$5 \times 5$、$7 \times 7$ 等。

---

总结：**Padding 的作用是避免卷积后图像过快缩小，同时保留边缘信息。Valid 卷积不填充，输出更小；Same 卷积自动填充，保证输出与输入同尺寸。**

```python
# p =1，  p=？
# 随堂练习：思考下padding怎么用numpy实现 自己代码尝试


some_data = np.array([[3,0,1,2,7,4],
                      [1,5,8,9,3,1],
                      [2,7,2,5,1,3],
                      [0,1,3,1,7,8],
                      [4,2,1,6,2,8],
                      [2,4,5,2,3,9]])

np.pad(some_data, (1,2))
```

### Stride（步幅）

- 卷积中的 Stride（步幅）

在卷积操作中，**stride**（步幅）表示卷积核每次移动的距离：

* **stride = 1**：卷积核逐像素滑动，输出尺寸较大。
* **stride > 1**：卷积核“跳着走”，每次跨过多个像素，输出尺寸明显缩小。

- 输出大小的公式

若输入是 $n \times n$，卷积核大小为 $f \times f$，填充为 $p$，步幅为 $s$，则输出大小为：

$$
\text{输出尺寸} = \left\lfloor \frac{n + 2p - f}{s} \right\rfloor + 1
$$

其中：

* $n$：输入大小
* $f$：卷积核大小
* $p$：padding（填充大小）
* $s$：stride（步幅大小）
* $\lfloor \cdot \rfloor$：向下取整（floor）

- 举例

* 输入 $7 \times 7$，卷积核 $3 \times 3$，无填充 ($p=0$)：

  * 当 $s=1$ → 输出大小：$(7 - 3)/1 + 1 = 5$，即 $5 \times 5$。
  * 当 $s=2$ → 输出大小：$\lfloor (7 - 3)/2 \rfloor + 1 = 3$，即 $3 \times 3$。

- 作用总结

1. **控制输出特征图的大小**：步幅越大，输出越小。
2. **减少计算量**：更少的位置参与卷积运算。
3. **调节特征提取粒度**：小步幅 → 细致特征；大步幅 → 粗略特征。

---

总结：
**Stride 决定卷积核每次移动的步长，公式 $\lfloor (n + 2p - f)/s \rfloor + 1$ 可以计算输出大小。步幅越大，特征图越小。**

### 在三维数组（体积）上卷积

 ![可视化在体积上卷积](./images/CNN/p5.png)



- 为什么需要体积卷积

在灰度图像中，输入通常是二维矩阵，例如 $6 \times 6$。
但在彩色图像中，数据包含 **三个通道（RGB）**，所以图像可以表示为 **$6 \times 6 \times 3$**。

* 前两个维度：高度和宽度
* 第三个维度：通道数（channels）

这类输入就不再是一个平面，而是一个 **体积（Volume）**。


- 三维卷积核（filter）

为了处理多通道图像，卷积核本身也必须是三维的。

* 例如输入是 $6 \times 6 \times 3$，卷积核可以是 **$3 \times 3 \times 3$**。
* **注意：卷积核的通道数必须与输入一致**，即这里的“3”必须相等。

一个 $3 \times 3 \times 3$ 卷积核包含 $27$ 个参数。

卷积的过程：

1. 把卷积核放在输入的一个位置上（覆盖 $3 \times 3 \times 3$ 的小立方块）。
2. 对应元素逐一相乘（27 次乘法），再相加，得到一个标量。
3. 把卷积核滑动到下一个位置，重复以上步骤。
4. 最终得到一个二维矩阵（例如 $4 \times 4$）。

结果：

$$
6 \times 6 \times 3 \;\;\;\ast\;\;\; 3 \times 3 \times 3 \;\;\;\rightarrow\;\;\; 4 \times 4 \times 1
$$



- 不同的卷积核 → 不同的特征

卷积核的参数决定了它能检测的特征。

* **只检测红色通道的边缘**：卷积核在红色通道放置边缘检测模板，其他通道全为零。
* **检测任意颜色的边缘**：卷积核在 RGB 三个通道都放置相同的边缘检测模板。

通过这种方式，不同的卷积核可以提取不同特征：垂直边缘、水平边缘、对角线纹理、颜色变化等。


- 多个卷积核（filters）

在实际的卷积层中，我们不会只用一个卷积核，而是会用 **多个卷积核** 来同时检测不同特征。

* 每个卷积核产生一个二维输出（feature map）。
* 所有输出 **按通道堆叠**，形成一个新的体积。

例如：

* 输入：$6 \times 6 \times 3$
* 使用 2 个 $3 \times 3 \times 3$ 卷积核
* 输出：$4 \times 4 \times 2$

公式：

$$
n \times n \times n_C \;\;\;\ast\;\;\; f \times f \times n_C \;\;\;\rightarrow\;\;\; (n-f+1) \times (n-f+1) \times n_C'
$$

其中：

* $n$：输入宽/高
* $n_C$：输入通道数（必须与卷积核通道一致）
* $f$：卷积核宽/高
* $n_C'$：卷积核个数（决定输出通道数）

（这里假设 stride=1，padding=0）

- 术语说明

* **Channels（通道数）**：输入或输出的第三个维度。
* **Depth（深度）**：有时文献里用来表示通道数，但容易与“网络深度”混淆，所以更推荐用“通道数”。

---

- 总结

1. 卷积不仅能在二维图像上操作，还能作用于多通道输入（体积）。
2. 卷积核必须与输入的通道数一致。
3. 每个卷积核会输出一个二维特征图，多个卷积核输出的结果会堆叠成新的体积。
4. 输出体积的通道数 = 使用的卷积核数。

**体积卷积让我们能在多通道图像上提取特征，每个卷积核生成一个特征图，多个卷积核堆叠后形成新的输出体积。**

### 单个卷积层

1. 输入与输出

* **输入体积（Input）**：

  $$
  n_H^{[l-1]} \times n_W^{[l-1]} \times n_C^{[l-1]}
  $$

  高度 × 宽度 × 通道数

* **输出体积（Output）**：

  $$
  n_H^{[l]} \times n_W^{[l]} \times n_C^{[l]}
  $$

  其中：

  * 高度与宽度由公式决定
  * 深度（通道数）等于 **卷积核个数**


2. 卷积层的关键参数

* $f^{[l]}$：filter size（卷积核大小）
* $p^{[l]}$：padding（填充）
* $s^{[l]}$：stride（步幅）
* $n_C^{[l]}$：number of filters（卷积核个数 = 输出通道数）



3. 卷积运算

* **每个卷积核** 大小为：

  $$
  f^{[l]} \times f^{[l]} \times n_C^{[l-1]}
  $$

* 卷积核在输入上滑动，计算：

  $$
  Z^{[l]} = W^{[l]} * A^{[l-1]} + b^{[l]}
  $$

  * $W^{[l]}$：卷积核权重
  * $A^{[l-1]}$：前一层的激活
  * $b^{[l]}$：偏置


4. 输出尺寸公式

输出的高度/宽度：

$$
n_H^{[l]} = \left\lfloor \frac{n_H^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} \right\rfloor + 1
$$

$$
n_W^{[l]} = \left\lfloor \frac{n_W^{[l-1]} + 2p^{[l]} - f^{[l]}}{s^{[l]}} \right\rfloor + 1
$$

输出通道数：

$$
n_C^{[l]} = \text{卷积核个数}
$$



5. 参数数量

* 每个卷积核参数量：

  $$
  f^{[l]} \times f^{[l]} \times n_C^{[l-1]} + 1 \quad (\text{+1 表示偏置})
  $$
* 总参数量：

  $$
  \left(f^{[l]} \times f^{[l]} \times n_C^{[l-1]} + 1\right) \times n_C^{[l]}
  $$

**例子：**

* 输入：$6 \times 6 \times 3$
* 卷积核：$3 \times 3 \times 3$，共 10 个
* 每个卷积核：$27 + 1 = 28$
* 总参数量：$28 \times 10 = 280$



6. 测试

**题目：**
输入是 $32 \times 32 \times 3$，使用 16 个 $5 \times 5 \times 3$ 卷积核，每个核带 1 个偏置。
请计算参数总数是多少？


7. 小结

单个卷积层完成的流程：

1. 输入体积与卷积核滑动相乘并加偏置
2. 应用非线性激活函数（如 ReLU）
3. 输出新的体积，通道数 = 卷积核个数
4. 参数数量与 **卷积核大小、数量、输入通道数** 有关，与输入图像尺寸无关

```python
# 批次数量：m
#  输入：m * 32*32*3

#  卷积层： 卷积核： f*f*3*卷积核的数量(n_c),  f*f*3*n_c
#  输出（不带偏置）的形状   m * （32-f+1）* （32-f+1） * n_c
#    偏置的形状 ：        1 *   1      *    1      * n_c

#  输出的形状（带上偏置）：  m * （32-f+1）* （32-f+1） * n_c

#   Flatten：           m * （（32-f+1）* （32-f+1） * n_c = 把卷积层的单个样本输出， 直接摊平成 一维度的）
#   Dense(10, activation = "softmax")


# 1. 搭一个带有卷积层的神经网络 tf.keras.layers.Conv2D()
# 输入 -> 卷积 -> 摊平 -> 全连接

# 2. 去手写数字/时尚衣服/彩图数据 试一下，能不能训练

```

### 内存需求

1. 训练期间卷积层需要大量内存

CNN 的一个挑战是卷积层需要大量的 RAM，在训练期间尤其如此，因为反向传播需要在前向传播过程中计算出的所有中间值。

例如，考虑一个具有 200 个 $5 \times 5$ 滤波器、步幅为 1 且采用 "same" 填充的卷积层。
如果输入是 $150 \times 100$ 的 RGB 图像（3 个通道），则参数数量为：
$$
(5 \times 5 \times 3 + 1) \times 200 = 15200
$$

（+1 表示偏置项）。与全连接层相比，它很小。

然而，200 个特征图中的每个特征图都包含 $150 \times 100$ 个神经元，
并且每个神经元都需要计算 $5 \times 5 \times 3 = 75$ 个输入的加权和：
$$
总共 = 2.25 亿次浮点运算
$$

虽然不如全连接层那么糟糕，但仍然需要进行大量的计算。

此外，如果使用 32 位浮点数来表示神经元，则单层占用的 RAM 为：

$$
200 \times 150 \times 100 \times 32 \text{ 位} = 9600 \text{ 万位} \approx 12 \text{ MB}
$$

这还只是一个实例，如果训练 100 个实例，则会占用约 **1.2 GB RAM**！


2. 推理期间与训练期间的区别

* **推理期间（即对新实例进行预测时）**：
  只需要计算完一层后，就可以释放前一层占用的 RAM，因此只需要两个连续层所需的 RAM。

* **训练期间**：
  需要保留前向传播过程中计算出的所有中间值以便进行反向传播，因此训练期间的 RAM 需求 = **所有层所需 RAM 的总量**。


3. 内存优化方法

* 如果训练因内存不足而崩溃，可以尝试：

  * 减小批量大小（batch size）
  * 使用步幅参数来降维，删除一些层
  * 使用 16 位浮点数而不是 32 位浮点数
  * 或将 CNN 分布在多个设备上

## 卷积神经网络示例

**输入：** $ 39 \times 39 \times 3 $

- 第 1 层卷积

* 卷积核大小：$f^{[1]} = 3$
* 步幅：$s^{[1]} = 1$
* 填充：$p^{[1]} = 0$
* 卷积核个数：10

**输出尺寸：** $ 37 \times 37 \times 10 $


- 第 2 层卷积

* 卷积核大小：$f^{[2]} = 5$
* 步幅：$s^{[2]} = 2$
* 填充：$p^{[2]} = 0$
* 卷积核个数：20

**输出尺寸：** $ 17 \times 17 \times 20 $

- 第 3 层卷积

* 卷积核大小：$f^{[3]} = 5$
* 步幅：$s^{[3]} = 2$
* 填充：$p^{[3]} = 0$
* 卷积核个数：40

**输出尺寸：** $ 7 \times 7 \times 40 $

- Flatten层: $ 7 \times 7 \times 40 = 1960 $

- 输出层
* 全连接层，输入维度 = 1960
* 输出维度 = 分类数 $K$
* 激活函数：

  * **逻辑回归 (logistic sigmoid)** → 二分类
  * **Softmax** → 多分类


- 总结：
这个网络结构是一个**ConvNet 示例**：输入图像 → 卷积层（3 层）→ Flatten → 全连接输出层（logistic/softmax）。

### 池化

<img alt="Max Pooling示意图" height="500" src="./images/CNN/p6.png" width="500"/>

一旦了解了卷积层如何工作，池化层就很容易掌握。它们的目标是对输入图像进行下采样（即缩小操作），以便减少计算量，内存使用量和参数数量（也能降低过拟合的风险）

在卷积神经网络（ConvNet）中，除了卷积层和全连接层，还常用 **池化层**。

它的主要作用是：

* **缩小特征图的尺寸**（降低计算量和内存消耗）
* **提高特征的鲁棒性**（让检测到的特征不依赖于具体位置）

1. Max Pooling（最大池化）

	**过程：**

	* 将输入划分为若干小区域（由滤波器大小 $f$ 和步幅 $s$ 决定）
	* 对每个区域取最大值，作为输出

	**示例 1：**
	输入 $4 \times 4$，使用 $f=2, s=2$ 的 max pooling：

	* 输出为 $2 \times 2$，每个位置等于对应 $2 \times 2$ 区域中的最大值

	**示例 2：**
	输入 $5 \times 5$，使用 $f=3, s=1$：

	* 输出为 $3 \times 3$，每个输出位置是对应 $3 \times 3$ 区域的最大值

	**特点：**

	* 如果一个区域里有某个特征被强烈激活（值大），max pooling 会保留下来
	* 直观理解：只要特征在区域内被检测到，就能传递下去
2. Average Pooling（平均池化）
	* 与 max pooling 类似，但计算区域内的 **平均值**
	* 现在较少使用，但在网络最后阶段，有时会用 average pooling 来压缩表示，例如从 $7 \times 7 \times 1000$ → $1 \times 1 \times 1000$
3. 高维输入的池化
	* 输入可以是三维体积（例如 $H \times W \times C$）
	* 池化操作对每个 **通道（channel）独立**进行
	* 所以池化层不会改变通道数：
	$$
	输入: n_H \times n_W \times n_C \;\;\rightarrow\;\; 输出: n_H' \times n_W' \times n_C
	$$
4. 超参数（Hyperparameters）
	* **滤波器大小 $f$**：常用 $f=2$
	* **步幅 $s$**：常用 $s=2$
	* **填充 $p$**：几乎总是 $p=0$
	* **类型**：max pooling 或 average pooling
	常见配置：
	* $f=2, s=2$：将特征图长宽缩小一半
	* $f=3, s=2$：缩小得更多
5. 池化层的特点
	* **没有可学习的参数**：

	  * 卷积层有权重和偏置需要训练
	  * 池化层只有固定的超参数（$f, s, p$），无参数需要梯度下降更新
	* **固定计算**：只是应用 max 或 average 运算，不会被训练过程改变
6. 总结
	* **Max pooling**：取区域内最大值，常用，能保留显著特征
	* **Average pooling**：取区域内平均值，较少用，但在最后阶段有时会使用
	* **池化层不会改变通道数，只缩小空间尺寸**
	* **没有可学习参数**，只是固定函数

**池化层就是在空间上压缩特征图，减少计算和参数量，同时保留关键信息。**

### 典型CNN架构

典型的CNN架构是先堆叠一些卷积层（通常每个卷积层都跟随一个ReLU层），接着放一个池化层，然后再堆叠另外几个卷积层(+ReLU)，再放另一个池化层，以此类推。随着图像不断经过卷积网络的各层，图像变得越来越小，但由于卷积层的存在，图像通常也越来越深（即具有更多的通道）。在顶部，添加一个常规前馈神经网络，该网络由几个全连接层(+ReLU)组成，最后一层（例如输出估计类别概率的softmax层）输出预测结果。

与其使用具有5×5核的卷积层，不如堆叠两层具有3×3核的卷积层：它使用较少的参数且需要较少的计算量，并且通常性能会更好。第一个卷积层是一个例外：它可以典型地具有较大的核（例如5×5），步幅通常为2或更大，这将减小图像的空间维度而不会丢失太多信息，由于输入图像通常只有3个通道，因此它不需要太多的计算量。

```python
mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = mnist
X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255  # expand_dims增加通道维度
X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255
X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]
```

```python
from functools import partial

DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding="same",
                        activation="relu", kernel_initializer="he_normal")
model = tf.keras.Sequential([
    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),   # 输入图片是灰度图，只有一个颜色通道，当加载数据时，确保每个图片都是[28,28,1]
    tf.keras.layers.MaxPool2D(),
    DefaultConv2D(filters=128),
    DefaultConv2D(filters=128),
    tf.keras.layers.MaxPool2D(),
    DefaultConv2D(filters=256),
    DefaultConv2D(filters=256),
    tf.keras.layers.MaxPool2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(units=128, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=64, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=10, activation="softmax")
])
```

```python
model.summary()
```

使用functools.partial()函数来定义DefaultConv2D，它的行为与Conv2D类似，但具有不同的默认参数：核大小为3、"same"、ReLU激活函数及相应的He初始化。

接下来，创建Sequential模型。它的第一层是DefaultConv2D，带有64个相当大的滤波器(7×7)。它使用默认步幅1，因为输入图像不是很大。它还设置input_shape=[28，28，1]，因为图像有28×28像素，具有一个颜色通道（即灰度）。当加载Fashion MNIST数据集时，请确保每个图像都具有此形状：可能需要使用np.reshape()或np.expanddims()添加通道维度。或者，可以使用Reshape层作为模型的第一层。

然后，添加一个使用默认池大小2的最大池化层，因此它将每个空间维度除以因子2。

重复相同的结构两次：两个卷积层后跟一个最大池化层。对于较大的图像，可以重复多次此结构，重复次数是可以调整的超参数。

注意，随着CNN向输出层延伸，滤波器的数量会翻倍（最初是64，然后是128，再然后是256）：这种增长是有意义的，因为低层特征的数量通常很少（例如小圆圈、水平线），但是有很多不同的方法可以将它们组合成更高层次的特征。通常的做法是在每个池化层之后将滤波器的数量加倍：由于池化层将每个空间维度除以2，因此我们能负担得起下一层特征图数量的加倍而不必担心参数数量、内存使用量或计算量的暴增。

接下来是全连接网络，由两个隐藏密集层和一个密集输出层组成。由于它是一个有10个类别的分类任务，因此输出层有10个单元，并且它使用softmax激活函数。请注意，必须在第一个密集层之前展平输入，因为它需要每个实例的一维特征数组。还添加了两个dropout层（每个层的dropout率为50%）以降低过拟合的风险。

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
score = model.evaluate(X_test, y_test)

X_new = X_test[:10]  # pretend new images
y_pred = model.predict(X_new)
```

### 为什么要CNN

1. 参数共享（Parameter Sharing）

* **直观理解**：一个边缘检测器（比如竖直边缘的 3×3 卷积核），在图像左上角和右下角都可能有用。
* **意义**：不需要在每个位置都学习独立的参数，而是让同一个卷积核在整张图像上滑动使用。
* **结果**：显著减少参数数量。例如，一个 32×32×3 的输入，如果直接全连接到下一层需要上千万参数，而卷积层可能只需要几百个。

2. 稀疏连接（Sparsity of Connections）

* 在卷积中，每个输出神经元只与输入中的局部区域（感受野）相连，而不是和所有像素都相连。
* **优点**：减少计算量，提升模型学习的效率，同时减少过拟合的风险。

3. 具备平移不变性（Translation Invariance）

* 一只猫即使向右平移几个像素，依然是“猫”。
* 由于卷积核在整张图像上共享并滑动，CNN 天然具备对小幅度平移的鲁棒性。
* 这让 CNN 更适合处理现实场景中存在的图像位置变化。

4. 更易训练，更适合图像任务

* 参数量减少 → 可以在更小的数据集上训练 → 不容易过拟合。
* 层次化结构 → 前几层学习边缘、纹理等低级特征，后几层逐渐组合成更复杂的形状和语义（如“眼睛”“猫脸”）。
* 适合搭配池化层（Pooling）与全连接层，形成完整的图像识别系统。


**总结**
CNN 的核心优势在于：

* **参数共享** → 大幅减少参数量
* **稀疏连接** → 计算和存储高效
* **平移不变性** → 识别鲁棒性更强

因此 CNN 在计算机视觉任务（图像分类、目标检测、风格迁移等）中成为主流方法。

### CNN应用--随堂练习

#### 微笑识别

```python
import h5py
from cnn_utils import *

def load_happy_dataset():
    train_dataset = h5py.File('datasets/train_happy.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('datasets/test_happy.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
```

```python
X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_happy_dataset()

# Normalize image vectors
X_train = X_train_orig/255.
X_test = X_test_orig/255.

# Reshape
Y_train = Y_train_orig.T
Y_test = Y_test_orig.T

print ("number of training examples = " + str(X_train.shape[0]))
print ("number of test examples = " + str(X_test.shape[0]))
print ("X_train shape: " + str(X_train.shape))
print ("Y_train shape: " + str(Y_train.shape))
print ("X_test shape: " + str(X_test.shape))
print ("Y_test shape: " + str(Y_test.shape))
```

```python
index = 124
plt.imshow(X_train_orig[index]) #display sample training image
plt.show()Z
```

```python
def happyModel():
    """
    Implements the forward propagation for the binary classification model:
    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE

    Note that for simplicity and grading purposes, you'll hard-code all the values
    such as the stride and kernel (filter) sizes.
    Normally, functions should take these values as function parameters.

    Arguments:
    None

    Returns:
    model -- TF Keras model (object containing the information for the entire training process)
    """
    model = tf.keras.Sequential([
            # YOUR CODE STARTS HERE
            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
            tf.keras.layers.ZeroPadding2D((3, 3), input_shape=(64,64,3)),
            ## Conv2D with 32 7x7 filters and stride of 1
            tf.keras.layers.Conv2D(filters=32, kernel_size=(7, 7), strides=1, padding="valid"),
            ## BatchNormalization for axis 3
            tf.keras.layers.BatchNormalization(axis=3),
            ## ReLU
            tf.keras.layers.ReLU(),
            ## Max Pooling 2D with default parameters
            tf.keras.layers.MaxPool2D(),
            ## Flatten layer
            tf.keras.layers.Flatten(),
            ## Dense layer with 1 unit for output & 'sigmoid' activation
            tf.keras.layers.Dense(units=1, activation="sigmoid"),
            # YOUR CODE ENDS HERE
        ])
    return model
```

```python
happy_model = happyModel()
# Print a summary for each layer
for layer in summary(happy_model):
    print(layer)

output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],
            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],
            ['BatchNormalization', (None, 64, 64, 32), 128],
            ['ReLU', (None, 64, 64, 32), 0],
            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],
            ['Flatten', (None, 32768), 0],
            ['Dense', (None, 1), 32769, 'sigmoid']]

comparator(summary(happy_model), output)
```

```python
# todo: 编译happy model， 注意这个是二元分类，损失函数
happy_model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])
```

```python
happy_model.summary()
```

```python
# todo：训练模型，10个轮次，批量大小16
happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)
```

```python
# todo：在测试集上评估模型
happy_model.evaluate(X_test, Y_test)
```

#### 手势识别

```python
def load_signs_dataset():
    train_dataset = h5py.File('datasets/train_signs.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('datasets/test_signs.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes


X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()
```

```python
index = 1
plt.imshow(X_train_orig[index])
print ("y = " + str(np.squeeze(Y_train_orig[:, index])))
```

```python
classes
```

```python
def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)].T
    return Y

X_train = X_train_orig/255.
X_test = X_test_orig/255.
Y_train = convert_to_one_hot(Y_train_orig, 6).T
Y_test = convert_to_one_hot(Y_test_orig, 6).T
print ("number of training examples = " + str(X_train.shape[0]))
print ("number of test examples = " + str(X_test.shape[0]))
print ("X_train shape: " + str(X_train.shape))
print ("Y_train shape: " + str(Y_train.shape))
print ("X_test shape: " + str(X_test.shape))
print ("Y_test shape: " + str(Y_test.shape))
```

```python
def convolutional_model(input_shape):
    """
    Implements the forward propagation for the model:
    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> DENSE

    Note that for simplicity and grading purposes, you'll hard-code some values
    such as the stride and kernel (filter) sizes.
    Normally, functions should take these values as function parameters.

    Arguments:
    input_img -- input dataset, of shape (input_shape)

    Returns:
    model -- TF Keras model (object containing the information for the entire training process)
    """
    ## CONV2D: 8 filters 4x4, stride of 1, padding 'SAME'
    # Z1 = None
    ## RELU
    # A1 = None
    ## MAXPOOL: window 8x8, stride 8, padding 'SAME'
    # P1 = None
    ## CONV2D: 16 filters 2x2, stride 1, padding 'SAME'
    # Z2 = None
    ## RELU
    # A2 = None
    ## MAXPOOL: window 4x4, stride 4, padding 'SAME'
    # P2 = None
    ## FLATTEN
    # F = None
    ## Dense layer
    ## 6 neurons in output layer. Hint: one of the arguments should be "activation='softmax'"
    # outputs = None
    # YOUR CODE STARTS HERE
    input_img = tf.keras.layers.Input(input_shape)
    Z1 = tf.keras.layers.Conv2D(filters=8, kernel_size=(4,4), strides=1, padding="same")(input_img)
    A1 = tf.keras.layers.ReLU()(Z1)
    P1 = tf.keras.layers.MaxPool2D(pool_size=(8,8), padding="same")(A1)

    Z2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding="same")(P1)
    A2 = tf.keras.layers.ReLU()(Z2)
    P2 = tf.keras.layers.MaxPool2D(pool_size=(4,4), padding="same")(A2)
    F = tf.keras.layers.Flatten()(P2)
    output = tf.keras.layers.Dense(6, activation="softmax")(F)
    model = tf.keras.Model(inputs=input_img, outputs=output)

    # YOUR CODE ENDS HERE

    return model
```

```python
conv_model = convolutional_model((64, 64, 3))
conv_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
conv_model.summary()

output = [['InputLayer', [(None, 64, 64, 3)], 0],
        ['Conv2D', (None, 64, 64, 8), 392, 'same', 'linear', 'GlorotUniform'],
        ['ReLU', (None, 64, 64, 8), 0],
        ['MaxPooling2D', (None, 8, 8, 8), 0, (8, 8), (8, 8), 'same'],
        ['Conv2D', (None, 8, 8, 16), 528, 'same', 'linear', 'GlorotUniform'],
        ['ReLU', (None, 8, 8, 16), 0],
        ['MaxPooling2D', (None, 2, 2, 16), 0, (4, 4), (4, 4), 'same'],
        ['Flatten', (None, 64), 0],
        ['Dense', (None, 6), 390, 'softmax']]

comparator(summary(conv_model), output)
```

```python
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(64)  # todo: 使用tf.data.Dataset.from_tensor_slices 打包训练特征和标签，并指定批量大小64
valid_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(64)  # todo: 使用tf.data.Dataset.from_tensor_slices 打包测试特征和标签，并指定批量大小64
history = conv_model.fit(train_dataset, epochs=100, validation_data=valid_dataset)
```

```python
import pandas as pd
df_loss_acc = pd.DataFrame(history.history)
df_loss= df_loss_acc[['loss','val_loss']].copy()
df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)

df_acc= df_loss_acc[['accuracy','val_accuracy']].copy()
df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)

df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')
df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')

plt.show()
```

## Numpy实现卷积神经网络

这是一个 **自己动手理解并实现卷积操作的练习**。

1. **不用 AI 直接写答案** —— 因为我已经用过 AI 生成过代码，知道它会怎么写。现在的重点不是依赖 AI，而是自己推导、理解。
2. **AI 的作用** —— 在学习过程中，可以用 AI 来帮助梳理思路，比如：要实现什么功能、实现的思路是什么、卷积的计算步骤如何展开。但不是让 AI 直接把代码写出来，而是在完全理解之后，自己动手实现。
3. **意义** —— CNN 是一个经典的神经网络架构。面试或简历上都可以涉及。即使面试官不会要求你用 keras 复现网络，也一定会问到“卷积到底在做什么”。因此，这个练习有助于理清卷积的原理和计算过程，把理解落实到细节。

### Zero-Padding

```python
import numpy as np
import matplotlib.pyplot as plt
import cnn_utils
```

```python
def zero_pad(X, pad):
    """
    任务：
    对数据集 X 中的所有图像进行零填充（padding）。填充应用于图像的高度和宽度
    参数：

    X —— 一个形状为 (m, n_H, n_W, n_C) 的 Python NumPy 数组，表示一批 m 张图像。

    pad —— 整数，表示在图像的垂直和水平方向上填充的宽度。

    返回值：

    X_pad —— 填充后的图像数组，形状为 (m, n_H + 2 * pad, n_W + 2 * pad, n_C)。
    """
    # Todo: 实现
    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant',constant_values=0)
    return X_pad
```

```python
# 运行 + 测试 zero_pad
np.random.seed(1)
x = np.random.randn(4, 3, 3, 2)
x_pad = zero_pad(x, 3)
print ("x.shape =\n", x.shape)
print ("x_pad.shape =\n", x_pad.shape)
print ("x[1,1] =\n", x[1, 1])
print ("x_pad[1,1] =\n", x_pad[1, 1])

fig, axarr = plt.subplots(1, 2)
axarr[0].set_title('x')
axarr[0].imshow(x[0, :, :, 0])
axarr[1].set_title('x_pad')
axarr[1].imshow(x_pad[0, :, :, 0])
cnn_utils.zero_pad_test(zero_pad)
```

### 单步卷积

```python
def conv_single_step(a_slice_prev, W, b):
    """
    **功能说明：**
    对前一层输出激活值中的一个切片（`a_slice_prev`）应用由参数 `W` 定义的一个卷积核（filter）。
    切片：卷积窗口当前所对应的输入，这里只需要实现卷积的一步操作，不需要滑动卷积窗口

    **参数：**

    * `a_slice_prev` —— 输入数据的一个切片，形状为 `(f, f, n_C_prev)`
    * `W` —— 卷积核的权重参数，窗口矩阵，形状为 `(f, f, n_C_prev)`
    * `b` —— 卷积核的偏置参数，窗口矩阵，形状为 `(1, 1, 1)`

    **返回值：**

    * `Z` —— 一个标量值，表示将卷积核 `(W, b)` 与输入数据的一个切片 `x` 进行卷积的结果。
    """

    # Todo: 实现
    Z_ = np.sum(a_slice_prev*W )+ b.flatten()[0]
    return Z_


```

```python
# 运行 + 测试conv_single_step
np.random.seed(1)
a_slice_prev = np.random.randn(4, 4, 3)
W = np.random.randn(4, 4, 3)
b = np.random.randn(1, 1, 1)

Z = conv_single_step(a_slice_prev, W, b)
print("Z =", Z)
cnn_utils.conv_single_step_test(conv_single_step)

assert (type(Z) == np.float64), "You must cast the output to numpy float 64"
assert np.isclose(Z, -6.999089450680221), "Wrong value"
```

### 完整卷积

```python
def conv_forward(A_prev, W, b, hparameters):
    """
    实现卷积函数的前向传播

    参数：
    A_prev -- 前一层的输出激活值，
              形状为 (m, n_H_prev, n_W_prev, n_C_prev) 的 numpy 数组
    W -- 卷积核的权重，
         形状为 (f, f, n_C_prev, n_C) 的 numpy 数组
    b -- 偏置，
         形状为 (1, 1, 1, n_C) 的 numpy 数组
    hparameters -- 包含超参数的 python 字典，键包括 "stride"和 "pad"， stride键对应的是步长，pad键对应的是填充

    返回值：
    Z -- 卷积后的输出，
         形状为 (m, n_H, n_W, n_C) 的 numpy 数组
    """
    # TODO： 实现
    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
    (f, f, n_C_prev, n_C) = W.shape
    stride = hparameters["stride"]
    pad = hparameters["pad"]

    n_H = np.floor((n_H_prev + 2*pad - f) / stride).astype(int) + 1
    n_W = np.floor((n_W_prev + 2*pad - f) / stride).astype(int) + 1


    Z = np.zeros((m,n_H, n_W, n_C))
    A_prev_pad = zero_pad(A_prev, pad)
    for i in range(m):

        # 选择第 i 个训练样本的填充后激活值
        a_prev_pad = A_prev_pad[i,:,:,:]

        # 遍历输出体的垂直方向
        for h in range(n_H):

            # 找到当前切片的垂直起始和结束位置 (≈2 行)
            vert_start = h*stride
            vert_end = vert_start + f

            # 遍历输出体的水平方向
            for w in range(n_W):

                # 找到当前切片的水平起始和结束位置 (≈2 行)
                horiz_start = w * stride
                horiz_end = horiz_start + f

                # 遍历输出体的通道数（= 卷积核数量）
                for c in range(n_C):

                    # 使用边界来定义 a_prev_pad 的 (3D) 切片 (提示见单元格上方) (≈1 行)
                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]

                    # 将 (3D) 切片与对应的卷积核 W 以及偏置 b 做卷积，得到一个输出神经元 (≈3 行)
                    weights = W[...,c]
                    biases = b[...,c]
                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)
    return Z


```

可以参考一个代码模板，启发实现思路：

```python
def conv_forward(A_prev, W, b, hparameters):
    # 从 A_prev 的形状中获取维度 (≈1 行)
    # (m, n_H_prev, n_W_prev, n_C_prev) = None

    # 从 W 的形状中获取维度 (≈1 行)
    # (f, f, n_C_prev, n_C) = None

    # 从 "hparameters" 中获取参数信息 (≈2 行)
    # stride = None
    # pad = None

    # 使用给定的公式计算卷积输出的维度。
    # 提示：使用 int() 来实现“向下取整”。(≈2 行)
    # n_H = None
    # n_W = None

    # 用零初始化输出体 Z (≈1 行)
    # Z = None

    # 通过对 A_prev 进行填充创建 A_prev_pad
    # A_prev_pad = None

    # 遍历整个训练样本批次
    # for i in range(None):

        # 选择第 i 个训练样本的填充后激活值
        # a_prev_pad = None

        # 遍历输出体的垂直方向
        # for h in range(None):

            # 找到当前切片的垂直起始和结束位置 (≈2 行)
            # vert_start = None
            # vert_end = None

            # 遍历输出体的水平方向
            # for w in range(None):

                # 找到当前切片的水平起始和结束位置 (≈2 行)
                # horiz_start = None
                # horiz_end = None

                # 遍历输出体的通道数（= 卷积核数量）
                # for c in range(None):

                    # 使用边界来定义 a_prev_pad 的 (3D) 切片 (提示见单元格上方) (≈1 行)
                    # a_slice_prev = None

                    # 将 (3D) 切片与对应的卷积核 W 以及偏置 b 做卷积，得到一个输出神经元 (≈3 行)
                    # weights = None
                    # biases = None
                    # Z[i, h, w, c] = None
    # return Z
```

```python
np.random.seed(1)
A_prev = np.random.randn(2, 5, 7, 4)
W = np.random.randn(3, 3, 4, 8)
b = np.random.randn(1, 1, 1, 8)
hparameters = {"pad" : 1,
               "stride": 2}

Z = conv_forward(A_prev, W, b, hparameters)
z_mean = np.mean(Z)
z_0_2_1 = Z[0, 2, 1]
print("Z's mean =\n", z_mean)
print("Z[0,2,1] =\n", z_0_2_1)

cnn_utils.conv_forward_test_1(z_mean, z_0_2_1)
cnn_utils.conv_forward_test_2(conv_forward)
```

### 池化

```python
def pool_forward(A_prev, hparameters, mode = "max"):
    """
    实现池化层的前向传播

    参数：
    A_prev -- 输入数据，形状为 (m, n_H_prev, n_W_prev, n_C_prev) 的 numpy 数组
    hparameters -- 包含超参数的 python 字典，键包括 "f"（池化窗口大小）和 "stride"（步幅）
    mode -- 要使用的池化方式，字符串类型，可选 "max" 或 "average"

    返回值：
    A -- 池化层的输出，形状为 (m, n_H, n_W, n_C) 的 numpy 数组
    """

    # TODO: 实现
```

可以参考一个代码模板，启发思路
```python
def pool_forward(A_prev, hparameters, mode = "max"):
    # 从输入的形状中获取维度
    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape

    # 从 "hparameters" 中获取超参数
    f = hparameters["f"]
    stride = hparameters["stride"]

    # 定义输出的维度
    n_H = int(1 + (n_H_prev - f) / stride)
    n_W = int(1 + (n_W_prev - f) / stride)
    n_C = n_C_prev

    # 初始化输出矩阵 A
    A = np.zeros((m, n_H, n_W, n_C))

    # 遍历训练样本
    # for i in range(None):

        # 遍历输出的垂直轴
        # for h in range(None):

            # 找到当前切片的垂直起始位置和结束位置 (≈2 行)
            # vert_start = None
            # vert_end = None

            # 遍历输出的水平轴
            # for w in range(None):

                # 找到当前切片的水平起始位置和结束位置 (≈2 行)
                # horiz_start = None
                # horiz_end = None

                # 遍历输出的各个通道
                # for c in range (None):

                    # 使用边界来定义第 i 个训练样本在通道 c 上的当前切片 (≈1 行)
                    # a_prev_slice = None

                    # 对切片执行池化操作。
                    # 使用 if 语句来区分不同的池化模式。
                    # 使用 np.max 和 np.mean。
                    # if mode == "max":
                        # A[i, h, w, c] = None
                    # elif mode == "average":
                        # A[i, h, w, c] = None
```

```python
# Case 1: 步幅为1
print("CASE 1:\n")
np.random.seed(1)
A_prev_case_1 = np.random.randn(2, 5, 5, 3)
hparameters_case_1 = {"stride" : 1, "f": 3}

A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = "max")
print("mode = max")
print("A.shape = " + str(A.shape))
print("A[1, 1] =\n", A[1, 1])
A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = "average")
print("mode = average")
print("A.shape = " + str(A.shape))
print("A[1, 1] =\n", A[1, 1])

cnn_utils.pool_forward_test_1(pool_forward)

# Case 2: 步幅为2
print("\n\033[0mCASE 2:\n")
np.random.seed(1)
A_prev_case_2 = np.random.randn(2, 5, 5, 3)
hparameters_case_2 = {"stride" : 2, "f": 3}

A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = "max")
print("mode = max")
print("A.shape = " + str(A.shape))
print("A[0] =\n", A[0])
print()

A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = "average")
print("mode = average")
print("A.shape = " + str(A.shape))
print("A[1] =\n", A[1])

cnn_utils.pool_forward_test_2(pool_forward)
```







# 经典的卷积神经网络

## LeNet-5

最广为人知的CNN架构，用Yann LeCun 1998年创建，被广泛应用于手写数字识别

著名的 **LeNet-5** 具有如下层次结构：

| 层   | 类型     | 特征图数（Maps） | 空间尺寸（Size） | 卷积核大小（Kernel size） | 步幅（Stride） | 激活函数 |
| ---- | -------- | ---------------: | ---------------- | ------------------------- | -------------: | -------- |
| Out  | 全连接   |                - | 10               | –                         |              – | RBF      |
| F6   | 全连接   |                - | 84               | –                         |              – | tanh     |
| C5   | 卷积     |              120 | 1×1              | 5×5                       |              1 | tanh     |
| S4   | 平均池化 |               16 | 5×5              | 2×2                       |              2 | tanh     |
| C3   | 卷积     |               16 | 10×10            | 5×5                       |              1 | tanh     |
| S2   | 平均池化 |                6 | 14×14            | 2×2                       |              2 | tanh     |
| C1   | 卷积     |                6 | 28×28            | 5×5                       |              1 | tanh     |
| In   | 输入     |                1 | 32×32            | –                         |              – | –        |

**一些细节补充说明：**

* MNIST 原图为 28×28，但在送入网络前会**零填充到 32×32** 并做归一化。此后网络其余部分**不使用 padding**，所以特征图尺寸会逐层减小。
* 平均池化层稍有不同：每个神经元先对感受野（卷积核）做**均值**，再乘以**可学习的系数**（每个特征图一个），并加上**可学习的偏置**（每个特征图一个），最后再通过激活函数。
* 在 C3 中，大多数神经元**并非与 S2 的全部 6 个特征图完全连接**，而是只连接其中的 3～4 个
* 输出层较特别：并非做常规的加权和，而是让每个输出单元给出**输入向量与其权向量的欧氏距离平方**，衡量图像属于某一数字类别的程度。如今更常用**交叉熵损失**，因为它对错误预测的惩罚更强，梯度更大、收敛更快。

## ResNet

何凯明(Kaiming He)等人使用残差网络(ResNet)赢得了ILSVRC 2015挑战赛，其前五位错误率低于3.6%。获胜的变体使用了由152层组成的非常深的CNN（其他变体具有34、50和101层）。它证实了一个趋势：计算机视觉模型变得越来越深，参数越来越少（层数多了，但每层的参数少了）。训练这种深层网络的关键是使用跳过连接(skip connection)［也称为快捷连接(shortcut connection)］：输入层的信号也将添加到位于堆叠层上方的层的输出中。

在训练神经网络时，目标是使其对目标函数h(x)建模。如果将输入x添加到网络的输出（即添加跳过连接），则网络将被迫建模f(x)=h(x)-x而不是h(x)。这称为残差学习

<img alt="残差学习" height="500" src="./images/CNN/p7.png" width="500"/>


当初始化一个常规神经网络时，它的权重接近于零，所以网络只输出接近于零的值。如果添加跳过连接，生成的网络只会输出其输入的副本。换句话说，它最初对恒等函数建模。如果目标函数非常接近恒等函数，这将大大加快训练速度。

另一种理解方式：如果 h(x) 本来就和 x 很接近，那么 h(x)-x 接近 零函数，网络参数在0附近只需稍微调整即可，大大简化了学习任务。当目标函数接近恒等函数时，网络实际上在做“残差修正”，而不是从零学习复杂的映射。

此外，如果添加许多跳过连接，即使有几个层尚未开始学习，网络也可以开始取得进展。由于跳过连接，信号可以轻松地穿过整个网络。深度残差网络可以看作残差单元(RU)的堆叠，其中每个残差单元都是一个带有跳过连接的小型神经网络。

<img alt="常规的深度神经网络 vs 深度残差网络" height="500" src="./images/CNN/p8.png" width="500"/>

ResNet架构非常简单，开头部分是输入，卷积和最大池化层

中间只是一堆非常深的残差单元。每个残差单元由两个卷积层（没有池化层）组成，具有批量归一化(BN)机制和ReLU激活函数，使用3×3核并保留空间维度（步幅1，"same"填充）。

结尾是全局平均池化（Global Average Pooling），全连接层，以及最后softmax激活的输出层。全局平均池化层把每个通道的特征图直接取平均值，变成一个数字，最后会输出长度为通道数的向量，可以取代一个全连接层，大幅减少参数

![ResNet架构](./images/CNN/p9.png)

注意，特征图的数量每隔几个残差单元就增加一倍，同时高度和宽度减半（使用步幅为2的卷积层）。当发生这种情况时，输入不能直接添加到残差单元的输出，因为它们的形状不同（此问题会影响图中虚线箭头所表示的跳过连接）。为了解决这个问题，输入将通过步幅为2且具有正确数量的输出特征图的1×1卷积层

![改变图大小和深度的跳过连接](./images/CNN/p10.png)

该架构存在不同的变体，它们具有不同的层数。ResNet-34是一个具有34层（仅计算带参数的卷积层和全连接层）的ResNet，它包含3个输出64个特征图的RU、4个输出128个特征图的RU、6个输出256个特征图的RU，以及3个输出512个特征图的RU。

更深的ResNet（例如ResNet-152）使用的残差单元略有不同。它们使用3个卷积层代替了2个具有256个特征图的3×3卷积层：第一个是1×1卷积层，仅具有64个特征图（减少为原来的四分之一），它充当了瓶颈层；然后是一个3×3卷积层，具有64个特征图；最后是另一个1×1卷积层，具有256个特征图（4乘以64），它恢复了原始深度。ResNet-152包含3个此类RU（可输出256个特征图）、8个具有512个特征图的RU、36个具有1024个特征图的RU，以及3个具有2048个特征图的RU。

瓶颈层的作用：“先压缩再恢复”，在保证模型表达力的同时，大幅降低参数量和计算量，使超深的 ResNet（50/101/152）变得可训练。

### 使用Keras实现ResNet-34 CNN

```python
from functools import partial
import tensorflow as tf

DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1, padding="same", kernel_initializer="he_normal", use_bias=False)

class ResidualUnit(tf.keras.layers.Layer):
    def __init__(self, filters, strides=1, activation="relu", **kwargs):
        super().__init__(**kwargs)

        self.activation = tf.keras.activations.get(activation)

        self.main_layers = [
            DefaultConv2D(filters, strides=strides),
            tf.keras.layers.BatchNormalization(),
            self.activation,
            DefaultConv2D(filters),
            tf.keras.layers.BatchNormalization()
        ]

        self.skip_layers = []
        if strides > 1:
            self.skip_layers = [
                DefaultConv2D(filters, kernel_size=1, strides=strides),
                tf.keras.layers.BatchNormalization()
            ]  # 跳过层只有在strides>1的时候才需要

    def call(self, inputs):
        Z = inputs
        for layer in self.main_layers:
            Z = layer(Z)

        skip_Z = inputs
        for layer in self.skip_layers:
            skip_Z = layer(skip_Z)
        return self.activation(Z + skip_Z)


def make_start(inputs):
    Z = DefaultConv2D(64, kernel_size=7, strides=2)(inputs)
    Z = tf.keras.layers.BatchNormalization(axis=3)(Z)
    Z = tf.keras.layers.Activation("relu")(Z)
    Z = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding="same")(Z)
    return Z


def make_block_with_stride1(inputs, filters):
    Z = DefaultConv2D(filters)(inputs)
    Z = tf.keras.layers.BatchNormalization()(Z)

    Z = tf.keras.layers.ReLU()(Z)
    Z = DefaultConv2D(filters)(Z)
    Z = tf.keras.layers.BatchNormalization()(Z)

    Z = tf.keras.layers.Add()([Z, inputs])
    output = tf.keras.layers.ReLU()(Z)
    return output

def make_block_with_stride2(inputs, filters):
    # residual
    Z = DefaultConv2D(filters, strides=2)(inputs)
    Z = tf.keras.layers.BatchNormalization()(Z)

    Z = tf.keras.layers.ReLU()(Z)
    Z = DefaultConv2D(filters)(Z)
    Z = tf.keras.layers.BatchNormalization()(Z)

    # skip
    Z_skip = DefaultConv2D(filters, kernel_size=1, strides=2)(inputs)
    Z_skip = tf.keras.layers.BatchNormalization()(Z_skip)

    Z = tf.keras.layers.Add()([Z, Z_skip])
    output = tf.keras.layers.ReLU()(Z)
    return output

input_ = tf.keras.layers.Input(shape=(64,64,3))
Z = make_start(input_)

prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    if filters == prev_filters:
        Z = make_block_with_stride1(Z, filters)
    else:
        Z = make_block_with_stride2(Z, filters)
    prev_filters = filters # 更新上一个滤波器刷零

Z = tf.keras.layers.GlobalAvgPool2D()(Z)
Z = tf.keras.layers.Flatten()(Z)
output = tf.keras.layers.Dense(6, activation="softmax")(Z)
#
my_resnet34_model = tf.keras.models.Model(input_, output)
```

```python
# 有了残差单元（RU）后，用Sequential模型来构建ResNet-34，把每个残差单元视为一个层
model = tf.keras.Sequential([
    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[64, 64, 3]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding="same")
])



prev_filters = 64
for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
    # 代码唯一有点绕的部分：3个RU有64个滤波器，4个RU有128个滤波器，6个RU有256个滤波器，3个RU有512个滤波器
    # 当滤波器的数量与之前的RU相同时，必须将步幅设置为1，否则将其设置为2
    strides = 1 if filters == prev_filters else 2
    model.add(ResidualUnit(filters, strides=strides))
    prev_filters = filters # 更新上一个滤波器刷零

model.add(tf.keras.layers.GlobalAvgPool2D())  # 注意全局平均这个层是keras自带的，
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(6, activation="softmax"))
```

### 练习

把ResNet-34 CNN，并应用于手势识别，看看效果

```python
import h5py
import numpy as np

def load_signs_dataset():
    train_dataset = h5py.File('datasets/train_signs.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('datasets/test_signs.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes


X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_signs_dataset()
```

```python
def convert_to_one_hot(Y, C):
    Y = np.eye(C)[Y.reshape(-1)].T
    return Y

X_train = X_train_orig/255.
X_test = X_test_orig/255.
Y_train = convert_to_one_hot(Y_train_orig, 6).T
Y_test = convert_to_one_hot(Y_test_orig, 6).T
print ("number of training examples = " + str(X_train.shape[0]))
print ("number of test examples = " + str(X_test.shape[0]))
print ("X_train shape: " + str(X_train.shape))
print ("Y_train shape: " + str(Y_train.shape))
print ("X_test shape: " + str(X_test.shape))
print ("Y_test shape: " + str(Y_test.shape))
```

```python
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(X_train, Y_train, epochs=5, validation_data=(X_test, Y_test))
```

## Inception

- 核心思想

Inception 的核心是 **“不必预先选一个卷积核尺寸，而是同时尝试多种尺寸，然后把结果拼接（concatenate）起来，让网络自己学习用哪种尺度的特征”**。
也就是在同一位置同时做 1×1、3×3、5×5 的卷积，以及池化，然后把这些输出在通道维度上拼接，得到一个更丰富的多尺度表示。

![Inception模块](./images/CNN/p13.png)

---

- 模块结构与细节

假设输入是 `28 × 28 × 192`：

* 同时计算：

  * 1×1 卷积 → 输出 `28 × 28 × 64`
  * 3×3 卷积（同样用 same padding）→ 输出 `28 × 28 × 128`
  * 5×5 卷积（same padding）→ 输出 `28 × 28 × 32`
  * 池化（stride=1, same padding）→ 输出 `28 × 28 × 32`
* 把四个输出在通道上拼接，得到 `28 × 28 × (64+128+32+32)=28 × 28 × 256`。

> 注意：为了能拼接，所有分支的 **空间尺寸（height, width）必须一致**，因此对池化也用 `padding="same"` 且 `stride=1`，使输出仍为 28×28。



- 为什么不用直接把所有卷积都堆起来？——计算量问题 & 1×1 瓶颈

直接在高通道输入上做大卷积（比如 5×5）代价很高。

* 直接对 `28×28×192` 做 5×5、输出 `28×28×32`，需要的乘法次数约 **120 million**（120M），非常昂贵。
* 解决思路：先用 **1×1 卷积** 把通道从 192 压缩到一个小的通道数（比如 16），再在这个小通道上做 5×5 卷积。

  * 1×1 卷积（192 → 16）的运算约 **2.4M**
  * 在 16 通道上做 5×5（得到 32 通道）的运算约 **10M**
  * 总和约 **12.4M**，比 120M 小约 10 倍。

这个 1×1 的“压缩”层通常被称为 **bottleneck（瓶颈）层**，既节省计算，又保留表达能力。


- 优点

* **多尺度特征**：局部（1×1）、小尺度（3×3）、大尺度（5×5）以及池化的特征都保留，网络可自适应选择不同尺度的组合。
* **计算高效**：通过 1×1 瓶颈层大幅降低后续大卷积的计算量，从而能在可接受的算力下使用“做多种卷积”的策略。
* **模块化**：把该模块堆叠起来就构成 Inception 网络，可以增加深度且控制计算量。


- 实践上的注意点

* 池化分支要用 `padding="same"` 且 `stride=1`（不是常规的降采样池化），以便拼接。
* 瓶颈的通道数是调优项：太小可能损失信息，太大则损失计算优势。
* Inception 是工程取向很强的设计（追求在精度和计算/参数之间的折中），后来有很多变体（Inception-v2/v3 等）继续改进瓶颈和 factorization（把 3×3 分解成两个 1×3 + 3×1 等）。

- 总结

Inception 就是“把多种尺度的卷积和池化并行起来，然后拼接输出；为了解决计算开销，用 1×1 卷积作瓶颈压缩通道”，从而在保持表达力的同时控制计算量。

```python
import tensorflow as tf


def inception_module(x, f1, f3reduce, f3, f5reduce, f5, pool_proj):
    # x: 输入张量
    # f1: 1x1 分支输出通道
    # f3reduce: 3x3 分支之前 1x1 压缩通道
    # f3: 3x3 卷积输出通道
    # f5reduce: 5x5 分支之前 1x1 压缩通道
    # f5: 5x5 卷积输出通道
    # pool_proj: 池化后 1x1 压缩输出通道

    branch1 = tf.keras.layers.Conv2D(f1, (1,1), padding='same', activation='relu')(x)

    branch3 = tf.keras.layers.Conv2D(f3reduce, (1,1), padding='same', activation='relu')(x)
    branch3 = tf.keras.layers.Conv2D(f3, (3,3), padding='same', activation='relu')(branch3)

    branch5 = tf.keras.layers.Conv2D(f5reduce, (1,1), padding='same', activation='relu')(x)
    branch5 = tf.keras.layers.Conv2D(f5, (5,5), padding='same', activation='relu')(branch5)

    branch_pool = tf.keras.layers.MaxPooling2D((3,3), strides=1, padding='same')(x)
    branch_pool = tf.keras.layers.Conv2D(pool_proj, (1,1), padding='same', activation='relu')(branch_pool)

    out = tf.keras.layers.Concatenate(axis=-1)([branch1, branch3, branch5, branch_pool])
    return out

# 示例用法：
inputs = tf.keras.Input(shape=(28,28,192))
x = inception_module(inputs, f1=64, f3reduce=96, f3=128, f5reduce=16, f5=32, pool_proj=32)
print(x.shape)  # -> (None, 28, 28, 256)
```

### Inception 架构

- 概览

带有 **Inception 模块** 的网络（典型代表：GoogLeNet / Inception）就是把若干个**并行多尺度分支**（1×1、3×3、5×5、pool→1×1）作为基本单元反复堆叠，在中间通过**下采样/Pooling**改变空间分辨率，并在中后层加入\*\*辅助分类器（side branches）\*\*以帮助训练与正则化。最终以全局平均池化 + 全连接（softmax）作为输出头。

---

- 架构分块（从低到高层次）

1. **Stem（输入处理层）**

   * 若干个常规卷积 + 池化，用来把原始图像降到较小的空间分辨率并升通道，为后续模块做准备。

2. **Inception Module（基本模块）**

   * 并行分支：

     * `1×1` 卷积（直接通道混合/压缩）
     * `1×1 -> 3×3`（先降维再做 3×3）
     * `1×1 -> 5×5`（先降维再做 5×5）
     * `3×3 pooling (stride=1, padding='same') -> 1×1`（保留空间尺寸并压缩通道）
   * 把各分支的输出在通道维拼接（concatenate）。
   * 示例：`28×28×192` → 一模块 → `28×28×256`。

3. **Pooling (池化层）**

   * 若干 Inception 模块之间用 MaxPool / Conv(stride=2) 降采样以减小 H×W

4. **Auxiliary Classifiers（辅助分类器 / 侧支）**

   * 在中间若干层接出一个小的分类头（小 conv -> FC -> softmax）。
   * 作用：增强中间层梯度流、起到正则化效果、加速训练收敛。

5. **Final Head（最终输出）**

   * 最末端常用 GlobalAveragePooling -> Dropout -> Dense(softmax)。
   * 全局平均替代了大量 FC 参数，使得参数量大幅下降（更轻量且鲁棒）。



## MobileNet

MobileNet是结构更简洁，运行更高效的深度卷积网络模型，旨在变得轻量级和快速，在计算能力有限的设备中，也能运行深度学习模型

- MobileNet降低计算开销的解决方案：深度可分离卷积（Depthwise Separable Convolution）
1. 传统卷积：6 * 6 * 3 -> 4 * 4 * 5, 乘法数量： 27 * 4 * 4 * 5 = 2160
2. 深度可分离卷积拆分为2步：第一步是Depthwise Convolution（逐通道卷积），每个通道单独做卷积，互不混合： 6 * 6 * 3 -> 4 * 4 * 3的乘法数量：9 * 4 * 4 * 3 = 432，第二步是Pointwise Convolution（逐点卷积，1×1 卷积），把所有通道重新组合，做跨通道的特征融合，4 * 4 * 3 -> 4 * 4 * 5的乘法数量：3 * 4 * 4 * 5 = 240， 432 + 240 = 672， 672/2160 大概为0.3
3. 所以通过把传统卷积，拆分为2个步骤，计算量显著降低； 符号推导它能省下多少计算量：

输入：n×n×n_c
输出：n×n×n_c'（假设 SAME padding、stride=1，使空间尺寸保持为 n）
卷积核：f×f

一、普通卷积（standard conv）
每个输出通道使用一个 f×f×n_c 卷积核
每个位置乘法数：f^2 · n_c
位置个数：n^2
输出通道数：n_c'
总乘法数：
Mult_std = n^2 · f^2 · n_c · n_c'

二、深度可分离卷积（Depthwise + Pointwise）

(1) Depthwise（逐通道卷积）
每个输入通道使用一个 f×f 卷积核（不混通道）
每个位置乘法数：f^2
位置个数：n^2
通道数：n_c
乘法数：
Mult_dw = n^2 · f^2 · n_c

(2) Pointwise（逐点 1×1 卷积）
1×1×n_c 卷积核做通道融合，产生 n_c' 个输出通道
每个位置乘法数：n_c
位置个数：n^2
输出通道数：n_c'
乘法数：
Mult_pw = n^2 · n_c · n_c'

合计（深度可分离）：
Mult_DS = Mult_dw + Mult_pw = n^2 · n_c · (f^2 + n_c')

三、计算量比值（节省比例）
Mult_DS / Mult_std
= [n^2 · n_c · (f^2 + n_c')] / [n^2 · f^2 · n_c · n_c']
= 1/n_c' + 1/f^2

典型设定：f=3 且 n_c' 很大（如 256~512），1/n_c' 很小，主导项为 1/9，故深度可分离卷积乘法量约为普通卷积的 ~1/9。

MobileNet v1 架构

1. **基本模块**

   * 每个标准卷积都替换成：

     * **Depthwise Convolution（逐通道卷积）**：每个通道单独提取空间特征；
     * **Pointwise Convolution（1×1 卷积）**：跨通道融合特征。

2. **堆叠方式**

   * v1 中堆叠了 **13 个这样的深度可分离卷积块**，从输入图像逐步提取特征。

3. **尾部结构**

   * **Pooling → 全连接层 → Softmax**，完成分类任务。

4. **效果**

   * 与普通卷积相比，计算量降低至约 **1/8 \~ 1/9**，在移动设备上表现良好。

---

<img alt="MobileNet v1 vs MobileNet v2" height="600" src="./images/CNN/p11.png" width="600"/>

MobileNet v2 在 v1 基础上做了两大改进：

1. **残差连接（Residual/Skip Connection）**

   * 借鉴 ResNet，使梯度更容易反向传播，训练更稳定。

2. **瓶颈块（Bottleneck Block）**

<img alt="MobileNet v2瓶颈块" height="600" src="./images/CNN/p12.png" width="600"/>

   * 结构：
     * **扩展层（Expansion，1×1 卷积）**：把通道数放大，例如扩展 6 倍，提升表达能力；
     * **Depthwise 卷积**：在扩展后的通道上提取空间特征；
     * **投影层（Projection，1×1 卷积）**：把通道数压回较小值，减少存储和传递的内存开销；
     * **残差相加**：若输入输出维度相同，直接跳连。
   * 好处：
     * **扩展 → 学习更复杂特征**；
     * **投影 → 控制内存和计算**；
     * **残差 → 稳定梯度**。

3. **网络规模**

   * v2 使用这种瓶颈块堆叠 **17 次**，再接上 Pooling → FC → Softmax 做分类。

---

总结

* **MobileNet v1**：用 **Depthwise + Pointwise** 替代普通卷积 → 高效轻量。
* **MobileNet v2**：在 v1 基础上增加 **残差连接 + 扩展/投影瓶颈块** → 更强的表达力，更高的精度。
* **适用场景**：移动端、IoT、边缘计算、需要低延迟推理的场合。

## 使用Keras的预训练模型

很多经典模型架构无需用代码重新实现，因为在tf.keras.applications包中只需要一行代码就可以轻松获得预训练的网络

```python
import matplotlib.pyplot as plt
import numpy as np

```

```python
# 加载 RestNet-50 模型
import tensorflow as tf
model = tf.keras.applications.ResNet50(weights="imagenet")
```

上面代码创建一个ResNet-50模型并下载在ImageNet数据集上预先训练的权重。要使用它，首先需要确保图像尺寸合适。ResNet-50模型需要224×224像素的图像（其他模型可能需要其他尺寸，例如299×299像素），所以使用Keras的Resizing层调整两个样本图像的大小（在将它们裁剪到目标纵横比之后）

```python
from sklearn.datasets import load_sample_images
images = load_sample_images()["images"]
images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)
```

```python
for x in images_resized:
    # print(x.numpy().shape)
    plt.imshow(x.numpy() / 255.)
    plt.axis('off')
    plt.show()
```

预先训练的模型假定以特定方式对图像进行预处理。在某些情况下，它们可能期望输入缩放到0～1或-1～1。每个模型都提供一个preprocess_input()函数，可以用来预处理图像。这些函数假设原始像素值的范围是0～255

```python
inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)
```

通常，Y_proba输出一个矩阵，每个图像一行，每个类别一列（在这个示例中，共有1000个类别）。如果要显示前K个预测，包括类别名和每个预测类别的估计概率，使用decode_predictions()函数。对于每个图像，它返回一个包含前K个预测的数组，其中每个预测都表示为一个包含类别标识符、其名称和对应置信度得分的数组

```python
Y_proba = model.predict(inputs)
Y_proba.shape
```

```python
top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)
for image_index in range(len(images)):
    print(f"Image #{image_index}")
    for class_id, name, y_proba in top_K[image_index]:
        print(f"  {class_id} - {name:12s} {y_proba:.2%}")
```

正确的类别是宫殿(palace)和大丽花(dahlia)，因此模型对于第一幅图像的预测是正确的，但对于第二幅图像的预测是错误的。那是因为大丽花不是1000个ImageNet类别之一。

使用预训练模型创建非常好的图像分类器非常容易,tf.keras.applications中提供了许多其他视觉模型，从轻量级快速模型到大型精确模型不等。

但是，如果想对不属于ImageNet的图像类别使用图像分类器,仍然可以通过预训练模型执行迁移学习来从中受益。

```python
from PIL import Image
pic = Image.open("images/test_resnet50.jpg")
pic = np.array(pic)

test_images = []
test_images.append(pic)
# test_images = pic
```

```python
test_images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(test_images)

for x in test_images_resized:
    # print(x.numpy().shape)
    plt.imshow(x.numpy() / 255.)
    plt.axis('off')
    plt.show()
```

```python
test_inputs = tf.keras.applications.resnet50.preprocess_input(test_images_resized)

Y_proba_test = model.predict(test_inputs)
Y_proba_test

```

```python
top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba_test, top=3)

# for class_id, name, y_proba in top_K[image_index]:
#         print(f"  {class_id} - {name:12s} {y_proba:.2%}")
top_K
```

```python
top_K
```

## 使用预训练模型进行迁移学习

如果想构建一个图像分类器，但没有足够的数据从头开始训练它，那么重用预训练模型的较低层通常是个好办法，例如，来训练模型对花的图片进行分类，并使用预先训练的Xception模型。

首先，用TensorFlow数据集加载花的数据集：

```python
import tensorflow_datasets as tfds
dataset, info = tfds.load("tf_flowers", as_supervised=True, with_info=True) # with_info=True获得有关数据集的信息 （大小和类的名称）
dataset_size = info.splits["train"].num_examples
class_names = info.features["label"].names
n_classes = info.features["label"].num_classes
```

```python
# 拆分训练集，将数据集的前10%用于测试，接下来的15%用于验证，剩下的75%用于训练

test_set_raw, valid_set_raw, train_set_raw = tfds.load(
    "tf_flowers",
    split=["train[:10%]", "train[10%:25%]", "train[25%:]"],
    as_supervised=True)
```

这3个数据集都包含单独的图像。需要对它们进行批处理，但首先需要确保它们都具有相同的大小，否则批处理将失败。为此，可以使用Resizing层。还必须调用tf.keras.applications.xception.preprocess_input()函数为Xception模型适当地预处理图像。最后，还将打乱训练集并预取样本：

```python
plt.figure(figsize=(12, 10))
index = 0
for image, label in valid_set_raw.take(9):
    index += 1
    plt.subplot(3, 3, index)
    plt.imshow(image)
    plt.title(f"Class: {class_names[label]}")
    plt.axis("off")

plt.show()
```

```python
# 每批包含32个图像，224*224个像素，像素值范围为-1 -- 1
batch_size = 32
preprocess = tf.keras.Sequential([
    tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True),
    tf.keras.layers.Lambda(tf.keras.applications.xception.preprocess_input)
])
train_set = train_set_raw.map(lambda X, y: (preprocess(X), y))
train_set = train_set.shuffle(1000, seed=42).batch(batch_size).prefetch(1)

valid_set = valid_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)
test_set = test_set_raw.map(lambda X, y: (preprocess(X), y)).batch(batch_size)
```

```python
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip(mode="horizontal", seed=42),
    tf.keras.layers.RandomRotation(factor=0.05, seed=42),
    tf.keras.layers.RandomContrast(factor=0.2, seed=42)
])
```

```python
plt.figure(figsize=(12, 12))
for X_batch, y_batch in valid_set.take(1):
    X_batch_augmented = data_augmentation(X_batch, training=True)
    for index in range(9):
        plt.subplot(3, 3, index + 1)
        # 必须将图像重新缩放到 imshow() 的 0-1 范围内，并且
        # 将结果裁剪到该范围内，因为数据增强可能会
        # 使某些值超出范围（例如，本例中的 RandomContrast）
        plt.imshow(np.clip((X_batch_augmented[index] + 1) / 2, 0, 1))
        plt.title(f"Class: {class_names[y_batch[index]]}")
        plt.axis("off")
plt.show()
```

tf.keras.preprocessing.image.ImageDataGenerator类可以轻松地从磁盘加载图像并以各种方式增强它们：你可以偏移每幅图像，旋转它、重新缩放它、水平或垂直翻转它、剪切它或应用想应用的变换函数。这对于简单的项目来说非常方便。但是，tf.data流水线并不复杂，而且通常速更快。此外，如果有GPU并且模型中包含预处理或数据增强层，它们将在训练期间受益于GPU加速。

```python
# 加载一个在ImageNet上预训练过的Xception模型。 通过设置include_top=False来排除网络顶部的层。这会排除全局平均池化层和密集输出层。
# 然后，使用softmax激活函数添加自己的全局平均池化层（将基础模型的输出提供给它），再添加一个密集输出层（每个类别一个单元）。最后，将所有这些包装在Keras Model中：
base_model = tf.keras.applications.xception.Xception(weights="imagenet",
                                                     include_top=False)  # include_top=False → 模型输出是最后一层卷积特征图 (batch_size, h, w, channels)。 这样可以自己决定是接 GAP(全局平均池化）、Flatten，再接上自己的 Dense 分类层。
avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)

output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)


model = tf.keras.Model(inputs=base_model.input, outputs=output)
```

```python
# 冻结预训练的权重

for layer in base_model.layers:
    layer.trainable = False

# base_model.trainable=False # 如果模型直接使用base model的层，而不是base model对象本身，设置base_model.trainable=False无效
```

```python
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(train_set, validation_data=valid_set, epochs=3)
```

```python
# 每当冻结或解冻层时，要编译模型；
# 还要确保使用低得多的学习率以避免破坏预训练的权重
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)

model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(train_set, validation_data=valid_set, epochs=10)
```

### 练习

使用MobileNetV2的预训练模型，对花的图片进行分类

## 分类和定位

定位图片中物体可以表示为回归任务：预测物体周围的边界框（一种常见的方法是预测物体中心的水平和垂直坐标，及其高度和宽度）。这意味着有4个数字需要预测。这不需要对模型进行太多修改，只需添加具有4个单元的第二个密集输出层（通常在全局平均池化层之上），就可以使用MSE损失对其进行训练：

```python
base_model = tf.keras.applications.xception.Xception(weights="imagenet",
                                                     include_top=False)
avg = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)
class_output = tf.keras.layers.Dense(n_classes, activation="softmax")(avg)


loc_output = tf.keras.layers.Dense(4)(avg)  # (x,y, 高度，宽度）

model = tf.keras.Model(inputs=base_model.input,
                       outputs=[class_output, loc_output])
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)  # added this line
model.compile(loss=["sparse_categorical_crossentropy", "mse"],
              loss_weights=[0.8, 0.2],  # 取决于更关注分类损失，还是更关注回归损失
              optimizer=optimizer, metrics=["accuracy", "mse"])
```

但是，花朵数据集中花朵周围没有边界框。因此，需要自己添加它们。这通常是机器学习项目中最难、成本最高的一个部分：获取标签。花时间寻找合适的工具是一个好办法。要使用边界框标注图像，可能需要使用开源图像标记工具（例如VGG ImageAnnotator、LabelImg、OpenLabeler或ImgLab）或者商业工具（例如LabelBox或Supervisely）。如果需要标注大量图像，建立众包平台（百度数据众包），准备发送给工人的表格，对工人进行监督并确保他们给出的边界框的质量良好，这是一项相当繁重的工作，因此要保证这样做是值得的。

即使不打算使用众包。如果只有几千甚至几百幅图像需要标记，而且不打算经常这样做，最好自己做：使用正确的工具，只需几天时间。在此过程中，还能更好地了解数据集和任务。

假设已经获得了花朵数据集中每幅图像的边界框（假设每幅图像有一个边界框），需要创建一个数据集，其数据项将是经过预处理的批量图像，以及它们的类别标签和边界框。每个数据项都应为以下形式的元组：(images，(class_labels，bounding_boxes))。然后，就可以训练模型了

应该对边界框进行归一化，以便水平和垂直坐标以及高度和宽度都在0～1的范围内。而且通常要预测高度和宽度的平方根，而不是直接的高度和宽度值：通过这种方式，大边界框的10像素误差将不会像小边界框的10像素误差一样受到惩罚。MSE通常作为代价函数可以很好地训练模型，但是评估模型对边界框的预测能力时它不是一个很好的指标。最常用的度量指标是“交并比”(Intersection over Union，IoU)：预测边界框和目标边界框之间的重叠面积除以它们的合并面积。在Keras中，它是由tf.keras.metrics.MeanIoU类实现的。

![边界框的交并比（IoU)指标](./images/CNN/p14.png)

```python
# 加个随机边框训练
def add_random_bounding_boxes(images, labels):
    fake_bboxes = tf.random.uniform([tf.shape(images)[0], 4])
    return images, (labels, fake_bboxes)

fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes)
model.fit(fake_train_set, epochs=2)
```

## 物体检测

对图像中的多个物体进行分类和定位的任务称为物体检测。曾经一种常见的方法是采用经过训练的CNN对图像中大致居中的单个物体进行分类和定位，然后将该CNN滑过图像并在每一步进行预测。通常，要训练CNN不仅可以预测类别概率和边界框，还可以预测存在性得分(objectness score)：这是图像确实包含以中间为中心的物体的估计概率。这是一个二元分类输出，它可以由具有单个单元的密集输出层（使用sigmoid激活函数并使用二元交叉熵损失进行训练）产生。

有时不使用存在性得分，反而会添加一个“无物体”类别，但总的来说，这种方法效果不太好：最好将“是否存在物体？”和“物体的类型是什么？”这两个问题分开回答。

这种滑动CNN的方法如下图所示。在这个示例中，图像被切割成5×7的网格，我们看到CNN（黑色粗矩形）在所有3×3的区域上滑动，并在每一步进行预测。

![在图像上滑动CNN检测多个物体](./images/CNN/p15.png)

在此过程中，CNN已经对其中三个3×3区域进行了预测：

- 当查看左上角的3×3区域（以位于第二行第二列的红色网格单元格为中心）时，它检测到图像左边的玫瑰。请注意，预测的边界框超出了这个3×3区域的边界。这绝对没问题：尽管CNN看不到玫瑰的底部，但它能够对它可能的位置做出合理的预测。它还预测到类别标签，给出了“玫瑰”类别的高置信度。最后，它预测到了相当高的存在性得分，因为边界框的中心位于中央单元格内（在此图中，存在性得分由边界框的颜色深度表示）。

- 当查看下一个3×3区域时，框架一个单元格（以蓝色网格单元格为中心的灰框），它没有检测到任何似乎位于该区域中心的玫瑰，因此它预测了一个非常低的存在性得分。因此，可以安全地忽略预测的边界框和类别预测。可以看到预测的边界框无论如何都不对齐。

- 最后，当查看当前黑框的3×3区域时，同样框起一个单元格（以绿色网格单元格为中心的黑框），它在顶端检测到玫瑰，尽管并不完美。这朵玫瑰实际上没有很好地位于该区域的中心，所以预测的存在性得分不是很高。

可以想象整个模型在整个图像上滑动CNN总共会输出15个预测的边界框，每个边界框伴随着估计的类别预测和存在性得分。由于物体有不同的大小，你可能需要多个CNN滑动窗口的大小（比如4×4区域），以获得良好的检测结果。

这种技术相当简单，但却有局限。它通常会在图像的不同位置多次检测到一个物体。需要进行一些后处理来移除不必要的边界框。一种常见的方法称为非极大抑制（non-max suppression），它已经这样工作的：

1. 首先，选择所有存在性得分低于某个阈值的边界框：由于CNN从头到尾滑动位置很多次，因此边界框是无用的。
2. 找到具有最高存在性得分的边界框，并去掉所有与它显著重叠的框（例如，IoU大于60%）。例如，在图中，具有最高存在性得分的边界框是最大的玫瑰边界框。所有与它有很大重叠的另外一些边界框就会被移除（尽管存在性得分已经在上一步被缩减掉了）。
3. 重复步骤2，直到没有显著重叠的边界框需要删除。

NMS 就像在朋友圈选照片：

一次旅行拍了 20 张同一个人的照片（好多框）。 不会全发，而是挑出最清晰、最好看的一张（最高分的框）。其他长得差不多的重复照片就删掉了（重叠框去掉）。

这种简单的物体检测方法效果很好，但是需要多次运行CNN（在本例中为15次），因此速度很慢。幸运的是，有一种更快的方法可以在图像上滑动CNN：使用全卷积网络（FCN）。

### 全卷积网络

FCN(全卷积网络）的概念在2015年发表的论文首次提出，主要用于语义分割（根据图像所属物体的类别对图像中的每个像素进行分类的任务）。作者指出，可以用卷积层代替CNN顶部的密集层。为了解这一点，来看一个示例：

假设一个具有200个神经元的密集层位于一个卷积层的顶部，该卷积层输出100个特征图，每个特征图的大小为7×7（这是特征图的大小，不是核大小）。每个神经元将计算来自卷积层的所有100×7×7激活的加权和（加上偏差项）。

现在，看看如果使用200个滤波器（每个滤波器的大小为7×7）并使用"valid"填充将卷积层替换为密集层，会发生什么情况。该层将输出200个特征图，每个特征图大小为1×1（因为核大小正好是输入特征图的大小，并且使用的是"valid"填充）。换句话说，它将输出200个数字，就像密集层一样。如果仔细观察卷积层执行的计算，你会发现这些数字与密集层产生的数字完全相同。唯一的区别是，密集层的输出是形状为［批量大小，200］的张量，而卷积层将输出形状为［批量大小，1，1，200］的张量。要将密集层转换为卷积层，卷积层中的滤波器数必须等于密集层中的单元数，滤波器大小必须等于输入特征图的大小，并且必须使用"valid"填充。步幅可以设置为1或更大。

为什么要把密集层改成卷积层？ 密集层需要特定的输入大小（因为每个输入特征只有一个权重），但卷积层可以愉快地处理任何大小的图像，只希望其输入具有特定数量的通道，因为每个核为每个输入通道包含一组不同的权重）。由于FCN仅包含卷积层（以及具有相同属性的池化层），因此可以在任何大小的图像上对其进行训练和执行！例如，假设我们已经训练了一个用于花卉分类和定位的CNN。它在224×224的图像上进行训练，并输出10个数字：
- 输出0到4通过softmax激活函数发送，这给出了类别概率（每个类别一个）。
- 输出5通过sigmoid激活函数发送，这给出了存在性得分。
- 输出6和7表示边界框的中心坐标，它们还通过sigmoid激活函数来确保它们的范围为0～1。
- 最后，输出8和9表示边界框的高度和宽度，它们不通过任何激活函数来允许边界框超出图像的边界。

现在可以将CNN的密集层转换为卷积层。事实上，甚至不需要重新训练它，只需将权重从密集层复制到卷积层！或者，可以在训练前将CNN转换为FCN。现在假设当网络被馈送入一个224×224图像时，输出层之前的最后一个卷积层（也称为瓶颈层）输出7×7特征图。如果向FCN提供448×448的图像，则瓶颈层将输出14×14的特征图。由于密集输出层已被卷积层替换，并且卷积层使用了10个大小为7×7的滤波器，填充为"valid"且步幅为1，因此卷积层输出将由10个特征图组成，每个特征图的大小为8×8（因为14-7+1=8）。换句话说，FCN将只处理一次整个图像，并且将输出一个8×8的网格，每个单元格包含10个数字（5个类别概率、1个存在性得分和4个边界框坐标）。这就像将一个原始的CNN以每行8步、每列8步的方式在图像上滑动一样。为了可视化这一过程，想象一下将原始图像多个卷积+池化层 降采样成14×14的网格（原始图片本身被看成了14*14的网格），然后在该网格上滑动7×7的窗口。该窗口将有8×8=64个可能的位置，因此有8×8个预测结果。但是，由于网络只查看一次图像，因此FCN方法效率更高。基于FCN，衍生出了“你只看一次”（You Only Look Once，YOLO）神经网络架构，流行于物体检测。

![相同的全卷积网络处理小图像和大图像](./images/CNN/p16.png)

### YOLO

YOLO是2015年提出的一种快速准确的物体检测架构，速度非常快，可以在视频上实时运行。YOLO内部是全卷积神经网络，架构上和刚才讨论类似，但是有一些算法细节上的重要区别：

- 对于每个单元格，YOLO只考虑边界框中心位于该单元格内的物体。边界框坐标是相对于该单元格的，(0,0)表示单元格的左上角，(1,1)表示右下角。但是边界框的高度和宽度可能会延伸到单元格之外。
- YOLO为每个网格单元格输出两个边界框（而不是1个），这允许模型处理两个物体彼此非常接近以至于它们的边界框中心位于同一单元格内的情况。每个边界框都有自己的存在性得分。
- YOLO还为每个网格单元格输出类别概率分布，预测每个单元格的20个类别概率，因为YOLO是在包含20个类别的数据集上训练的。注意，YOLO为每个网格单元格预测一个类别概率分布，而不是每个边界框预测一个. 但是，可以在后处理期间通过测量每个边界框与类别概率图中每个类的匹配程度来估计每个边界框的类别概率。例如，想象一张人站在汽车前的照片。有两个边界框：一个大的水平边界框（用于汽车）和一个较小的垂直边界框（用于人）。这些边界框的中心可能在同一个单元格中。那么，我们如何判断应该将哪个类别分配给哪个边界框呢？类别概率图将包含一个大区域，其中“汽车”类别占主导地位，而在其中将有一个较小的区域，其中“人”类别占主导地位。希望汽车的边界框与“汽车”区域大致匹配，而人的边界框与“人”区域大致匹配：这允许将正确的类别分配给每个边界框。

### 均值平均精度 (mAP)

在物体检测任务中使用的一个常见的指标是均值平均精度(mAP)。两个分类指标：准确率和召回率。这个权衡：召回率越高，准确率越低。我们可以在准确率/召回率曲线中将其可视化。为将该曲线总结为一个数字，我们可以计算其在曲线下的面积(AreaUnder Curve，AUC)。但是请注意，准确率/召回率曲线可能包含几个部分，在某个部分，当召回率增加时，准确率实际上会提高，尤其是在召回率值较低时。这是采用mAP指标的动机之一。假设分类器在10%的召回率下具有90%的准确率，但在20%的召回率下具有96%的准确率。这里实际上没有折中：以20%的召回率而不是10%的召回率使用分类器更合理，因为这将获得更高的召回率和更高的准确率。因此，我们不应该着眼于10%的召回率，而是应该着眼于分类器可以提供至少10%的召回率的最大准确率。这是96%，而不是90%。因此，要获得关于模型性能的合理概念的一种方法是计算召回率至少为0%、10%、20%，以此类推，直至100%，可以获得的最大准确率，然后计算这些最大准确率的平均值。这称为平均精度(AP)指标。当有两个以上类别时，我们可以为每个类别计算AP，然后计算均值AP(mAP)。在物体检测系统中，存在另外一层复杂度：如果系统检测到正确的类别但在错误的位置（即边界框完全没有物体），该怎么办？当然，我们不应将此视为正的预测。一种方法是定义IoU阈值：例如我们可以认为只有在IoU大于0.5且预测类别正确时，该预测才是正确的。通常将相应的mAP标记为mAP@0.5（或mAP @ 50%，有时也标记为AP50）。在某些比赛中（例如PASCAL VOC挑战赛），就是这样做的。在其他情况（例如COCO比赛）中，针对不同的IoU阈值(0.50，0.55，0.60，…，0.95)计算mAP，而最终指标是所有这些mAP（记为mAP @ [.50：.95]或mAP @ [.50：0.05：.95]）的平均值。是的，这是平均值的平均。

```python
def maximum_precisions(precisions):
    return np.flip(np.maximum.accumulate(np.flip(precisions)))
```

# 循环神经网络

RNN能够分析时间序列数据，例如网站上每天活跃用户的数量、城市每小时的温度、家里每日耗电量、附近汽车的轨迹等。一旦RNN学习了数据中过去的模式，它便能够利用这些知识来预测未来，当然，前提是过去的模式仍然在未来成立。

更一般地说，RNN能够针对任意长度的序列进行处理，而不是针对固定大小的输入。例如，它们可以将句子、文档或音频样本作为输入，因此在自然语言处理应用（比如自动翻译或语音识别）中非常有用。

介绍RNN的基本概念以及如何使用时域反向传播来对它们进行训练。

此外，将探讨RNN面临的两个主要问题：

- 不稳定的梯度，可以通过各种技术（包括循环Dropout和循环层归一化）来缓解。
- 非常有限的短期记忆，可以使用LSTM和GRU单元进行扩展。

然后，将使用它们来预测时间序列



## 循环神经元和层

前馈神经网络，其中激活仅在一个方向上流动，从输入层流向输出层。循环神经网络看起来非常像前馈神经网络，只不过它还具有反向的连接。

最简单的RNN：它由一个接收输入、产生输出并将输出反送回自身的神经元组成，如图（左）所示。在每个时间步长t（也称为帧），该循环神经元接收输入x(t)和前一个时间步长的输出ŷ(t-1)。由于在第一个时间步长中不涉及先前的输出，因此通常将其设置为0。可以沿时间轴来展开这个小网络，如图1（右）所示。这被称为时间展开网络（它是同一循环神经元在每个时间步长的表示）。

![循环神经元](./images/RNN/p1.png)

如下图所示，可以轻松地创建一个循环神经元层。在每个时间步长t，每个神经元接收输入向量x(t)和前一个时间步长的输出向量ŷ(t-1)。请注意，现在输入和输出都是向量（当只有一个神经元时，输出是标量）。

![一层循环神经元随时间展开](./images/RNN/p2.png)

每个循环神经元都有两组权重：一组用于输入x(t)，另一组用于前一个时间步长的输出ŷ(t-1)。我们称这些权重向量为wx和wŷ。如果考虑整个循环神经元层（简称“循环层”）而不仅仅是一个循环神经元，则可以将所有权重向量放在wx和wŷ这两个权重矩阵中。然后，可以如预期的那样计算整个循环层的输出向量，如公式所示，其中b是偏置向量，φ(·)是激活函数（例如ReLU)

- 公式：单个实例的循环层输出

$$
\hat{y}_{(t)} = \phi(W_x^{T}x_{(t)} + W_{\hat{y}}^{T}\hat{y}_{(t-1)} + b)
$$

就像前馈神经网络一样，可以通过将时间步长处的所有输入放在输入矩阵 $X_{(t)}$ 中，
来一次性计算出整个小批量的循环层输出（见下面公式 ）。

- 公式：一次传递中所有实例（小批量）的循环神经元层输出

$$
\hat{Y}_{(t)} = \phi(X_{(t)}W_x + \hat{Y}_{(t-1)}W_{\hat{y}} + b)
$$

也可以写成：

$$
\hat{Y}_{(t)} = \phi(([X_{(t)} \ \hat{Y}_{(t-1)}]W + b)), \quad
W =
\begin{bmatrix}
W_x \\
W_{\hat{y}}
\end{bmatrix}
$$

在此等式中：

- $\hat{Y}_{(t)}$ ：是一个 $m \times n_{\text{neurons}}$ 矩阵，包含小批量中每个实例在时间步长 $t$ 处该层的输出。
  （$m$ 是小批量中的实例数量，$n_{\text{neurons}}$ 是神经元数量。）

- $X_{(t)}$ ：是一个 $m \times n_{\text{inputs}}$ 矩阵，包含所有实例的输入。
  （$n_{\text{inputs}}$ 是输入特征的数量。）

- $W_x$ ：是一个 $n_{\text{inputs}} \times n_{\text{neurons}}$ 矩阵，包含当前时间步长的输入连接权重。

- $W_{\hat{y}}$ ：是一个 $n_{\text{neurons}} \times n_{\text{neurons}}$ 矩阵，包含前一时间步长的输出连接权重。

- $b$ ：是大小为 $n_{\text{neurons}}$ 的向量，包含每个神经元的偏置项。

- 权重矩阵 $W_x$ 和 $W_{\hat{y}}$ 经竖直重合并形成形状为 $(n_{\text{inputs}} + n_{\text{neurons}}) \times n_{\text{neurons}}$ 的单个权重矩阵 $W$（见公式 15-2 的第二行）。

- 符号 $[X_{(t)} \ \hat{Y}_{(t-1)}]$ 表示矩阵 $X_{(t)}$ 和 $\hat{Y}_{(t-1)}$ 的水平合并。


请注意：

- $\hat{Y}_{(t)}$ 是 $X_{(t)}$ 和 $\hat{Y}_{(t-1)}$ 的函数；
- 而 $\hat{Y}_{(t-1)}$ 是 $X_{(t-1)}$ 和 $\hat{Y}_{(t-2)}$ 的函数；
- $\hat{Y}_{(t-2)}$ 是 $X_{(t-2)}$ 和 $\hat{Y}_{(t-3)}$ 的函数；
以此类推。

这使 $\hat{Y}_{(t)}$ 成为自时间 $t=0$ 以来所有输入（即 $X_{(0)}, X_{(1)}, \ldots, X_{(t)}$）的函数。在第一个时间步长 $t=0$ 时，没有先前的输出，因此通常假定它们均为零。

### 记忆单元

由于在时间步长t时循环神经元的输出是先前时间步长中所有输入的函数，因此可以说它具有记忆的形式。神经网络中跨时间步长保留某些状态的部分称为记忆单元（简称单元）。单个循环神经元或循环神经元层是非常基本的单元，它只能学习短模式（通常约为10个步长）。后面将介绍一些能够学习更长模式（大约要长10倍）的更复杂、功能更强大的单元类型。单元在时间步长t的状态表示为h(t)［“h”代表“隐藏”(hidden)］，是该时间步长的某些输入和其前一个时间步长状态的函数：h(t)=f(h(t-1)，x(t))。它在时间步长t的输出表示为ŷ(t)，也是先前状态和当前输入的函数。就目前为止我们讨论的基本单元而言，输出等于状态。

### 输入序列和输出序列

RNN可以同时接收一个输入序列并产生一个输出序列。这种序列到序列的网络对于预测时间序列很有用，例如预测家里每日耗电量：将过去N天的数据输入它，然后训练它输出未来一天的耗电量（即从N-1天前到明天）。

或者，可以向网络提供一个输入序列，并忽略除了最后一个输出外的所有输出，这是一个序列到向量的网络。例如，可以向网络提供与电影评论相对应的单词序列，然后网络将输出一个情感得分［例如，从0（代表不喜欢）到1（代表喜欢）］。

相反，可以在每个时间步长中一次又一次地向网络提供相同的输入向量，并让其输出一个序列，这是一个向量到序列的网络。例如，输入可以是图像（或CNN的输出），而输出可以是该图像的描述文字。最后，可能有一个称为编码器的序列到向量的网络，后跟一个称为解码器的向量到序列的网络。例如，这可以用于将句子从一种语言翻译成另一种语言。可以用一种语言向网络输入一个句子，编码器会将其转换为单个向量表示，然后解码器会将此向量解码为另一种语言的句子。这种称为“编码器—解码器”(Encoder-Decoder)的两步模型比使用单个序列到序列的RNN进行即时翻译要好得多：句子的最后一个单词会影响翻译的第一个单词，因此在翻译之前需要等待，直到看完整个句子。

![序列到序列，序列到向量， 向量到序列，编码器-解码器](./images/RNN/p3.png)



### 随堂练习：简单RNN的前向传播

见其他文件夹

## 训练RNN

要训练RNN，诀窍是将其按照时间逐步展开，然后使用常规的反向传播。这种策略称为“时间反向传播”(BackPropagation Through Time，BPTT)。

就像在常规的反向传播中一样，首先通过展开的网络进行第一次前向传递（由虚线箭头表示）。然后，使用损失函数Loss(Y(0)，Y(1)，…，Y(T)；)（其中Y(i)）是第i个目标，Ŷ(i)是第i个预测结果，而T是最大时间步）评估输出序列。注意，此损失函数可能会忽略某些输出。例如，在序列到向量的RNN中，除了最后一个之外，所有输出都被忽略。在图中，仅基于最后三个输出计算损失函数。然后，将该损失函数的梯度通过展开的网络向后传播（由实线箭头表示）。在此示例中，由于输出Ŷ(0)和Ŷ(0)未用于计算损失，因此梯度不会向后流动，它们仅通过Ŷ(2)、Ŷ(3)和Ŷ(4)流动。此外，由于在每个时间步骤中使用相同的参数W和b，因此它们的梯度将在反向传播期间进行多次调整。完成反向阶段并计算出所有梯度后，BPTT可以执行梯度下降步骤来更新参数（这与常规反向传播没有区别）。

![RNN反向传播](./images/RNN/p1.jpg)

幸运的是，Keras为我们处理了所有这些复杂度

## RNN预测时间序列

在用RNN预测之前，先加载时间序列并使用经典工具开始分析它，以更好地理解要处理的问题，并获得一些基准指标。

```python
import tensorflow as tf
from pathlib import Path

filepath = tf.keras.utils.get_file(
    "ridership.tgz",
    "https://github.com/ageron/data/raw/main/ridership.tgz",
    cache_dir=".",
    extract=True
)
if "_extracted" in filepath:
    ridership_path = Path(filepath) / "ridership"
else:
    ridership_path = Path(filepath).with_name("ridership")
```

```python
import pandas as pd
from pathlib import Path

path = Path("datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv")
df = pd.read_csv(path, parse_dates=["service_date"])
df.columns = ["date", "day_type", "bus", "rail", "total"]  #  更短的名字
df = df.sort_values("date").set_index("date")
df = df.drop("total", axis=1)  # 不需要全部，只需要bus + rail
df = df.drop_duplicates()  # 删掉重复的月份 (2011-10 and 2014-07)
```

2001年1月1日，芝加哥有297192人乘坐公共汽车，126455人乘坐火车。day_type列包含W（表示工作日）、A（表示星期六），以及U（表示星期日或节假日）。

```python
df.head()
```

```python
import matplotlib.pyplot as plt

# 2019年几个月的公共汽车和铁路客运量数据
df.loc["2019-03":"2019-05"].plot(grid=True, marker=".", figsize=(8, 3.5))
plt.show()
```

这是一个时间序列：具有不同时间步的值的数据，通常间隔固定。更具体地说，由于每个时间步有多个值，因此这称为多元时间序列。如果只查看bus列，它将是一元时间序列，每个时间步都有一个值。预测未来值（即预测）是处理时间序列时最典型的任务。其他任务包括插补（填补过去的缺失值）、分类、异常检测等。

从图中可以看到每周都有明显类似的模式。这称为每周季节性(seasonality)。

这个模式很强大，以至于仅通过复制一周前的值来预测明天的乘客量便可产生相当不错的结果。这称为朴素预测：通过简单地复制过去的值来做出预测。朴素预测通常是一个很好的基准，在某些情况下甚至很难被击败。

为了可视化这些朴素预测，我们用虚线叠加两个时间序列（bus和rail）以及滞后一周（即向右移动）的相同时间序列。还将绘制两者之间的差异（即时间t处的值减去时间t-7处的值），这称为差分

```python
diff_7 = df[["bus", "rail"]].diff(7)["2019-03":"2019-05"]

fig, axs = plt.subplots(2, 1, sharex=True, figsize=(8, 5))
df.plot(ax=axs[0], legend=False, marker=".")  #  原始时间序列
df.shift(7).plot(ax=axs[0], grid=True, legend=False, linestyle=":")  # 往后推的时间序列
diff_7.plot(ax=axs[1], grid=True, marker=".")  # 7天 差分的时间序列
axs[0].set_ylim([170_000, 900_000])  # 美化绘图
plt.show()
```

```python
diff_7.abs().mean() # Mean absolute error (MAE), 或者是 mean absolute deviation (MAD)

# targets = df[["bus", "rail"]]["2019-03":"2019-05"]
# (diff_7 / targets).abs().mean() # Mean absolute percentage error (MAPE)
```

RNN能够分析时间序列数据，例如网站上每天活跃用户的数量、城市每小时的温度、家里每日耗电量、附近汽车的轨迹等。一旦RNN学习了数据中过去的模式，它便能够利用这些知识来预测未来，当然，前提是过去的模式仍然在未来成立。

更一般地说，RNN能够针对任意长度的序列进行处理，而不是针对固定大小的输入。例如，它们可以将句子、文档或音频样本作为输入，因此在自然语言处理应用（比如自动翻译或语音识别）中非常有用。

介绍RNN的基本概念以及如何使用时域反向传播来对它们进行训练。

此外，将探讨RNN面临的两个主要问题：

不稳定的梯度，可以通过各种技术（包括循环Dropout和循环层归一化）来缓解。
非常有限的短期记忆，可以使用LSTM和GRU单元进行扩展。
然后，将使用它们来预测时间序列

循环神经元和层
前馈神经网络，其中激活仅在一个方向上流动，从输入层流向输出层。循环神经网络看起来非常像前馈神经网络，只不过它还具有反向的连接。

最简单的RNN：它由一个接收输入、产生输出并将输出反送回自身的神经元组成，如图（左）所示。在每个时间步长t（也称为帧），该循环神经元接收输入x(t)和前一个时间步长的输出ŷ(t-1)。由于在第一个时间步长中不涉及先前的输出，因此通常将其设置为0。可以沿时间轴来展开这个小网络，如图1（右）所示。这被称为时间展开网络（它是同一循环神经元在每个时间步长的表示）。

循环神经元

如下图所示，可以轻松地创建一个循环神经元层。在每个时间步长t，每个神经元接收输入向量x(t)和前一个时间步长的输出向量ŷ(t-1)。请注意，现在输入和输出都是向量（当只有一个神经元时，输出是标量）。

一层循环神经元随时间展开

每个循环神经元都有两组权重：一组用于输入x(t)，另一组用于前一个时间步长的输出ŷ(t-1)。我们称这些权重向量为wx和wŷ。如果考虑整个循环神经元层（简称“循环层”）而不仅仅是一个循环神经元，则可以将所有权重向量放在wx和wŷ这两个权重矩阵中。然后，可以如预期的那样计算整个循环层的输出向量，如公式所示，其中b是偏置向量，φ(·)是激活函数（例如ReLU)

公式：单个实例的循环层输出

就像前馈神经网络一样，可以通过将时间步长处的所有输入放在输入矩阵 
 中， 来一次性计算出整个小批量的循环层输出（见下面公式 ）。

公式：一次传递中所有实例（小批量）的循环神经元层输出

也可以写成：

 

在此等式中：

 ：是一个 
 矩阵，包含小批量中每个实例在时间步长 
 处该层的输出。 （
 是小批量中的实例数量，
 是神经元数量。）

 ：是一个 
 矩阵，包含所有实例的输入。 （
 是输入特征的数量。）

 ：是一个 
 矩阵，包含当前时间步长的输入连接权重。

 ：是一个 
 矩阵，包含前一时间步长的输出连接权重。

 ：是大小为 
 的向量，包含每个神经元的偏置项。

权重矩阵 
 和 
 经竖直重合并形成形状为 
 的单个权重矩阵 
（见公式 15-2 的第二行）。

符号 
 表示矩阵 
 和 
 的水平合并。

请注意：

 是 
 和 
 的函数；
而 
 是 
 和 
 的函数；
 是 
 和 
 的函数； 以此类推。
这使 
 成为自时间 
 以来所有输入（即 
）的函数。在第一个时间步长 
 时，没有先前的输出，因此通常假定它们均为零。

记忆单元
由于在时间步长t时循环神经元的输出是先前时间步长中所有输入的函数，因此可以说它具有记忆的形式。神经网络中跨时间步长保留某些状态的部分称为记忆单元（简称单元）。单个循环神经元或循环神经元层是非常基本的单元，它只能学习短模式（通常约为10个步长）。后面将介绍一些能够学习更长模式（大约要长10倍）的更复杂、功能更强大的单元类型。单元在时间步长t的状态表示为h(t)［“h”代表“隐藏”(hidden)］，是该时间步长的某些输入和其前一个时间步长状态的函数：h(t)=f(h(t-1)，x(t))。它在时间步长t的输出表示为ŷ(t)，也是先前状态和当前输入的函数。就目前为止我们讨论的基本单元而言，输出等于状态。

输入序列和输出序列
RNN可以同时接收一个输入序列并产生一个输出序列。这种序列到序列的网络对于预测时间序列很有用，例如预测家里每日耗电量：将过去N天的数据输入它，然后训练它输出未来一天的耗电量（即从N-1天前到明天）。

或者，可以向网络提供一个输入序列，并忽略除了最后一个输出外的所有输出，这是一个序列到向量的网络。例如，可以向网络提供与电影评论相对应的单词序列，然后网络将输出一个情感得分［例如，从0（代表不喜欢）到1（代表喜欢）］。

相反，可以在每个时间步长中一次又一次地向网络提供相同的输入向量，并让其输出一个序列，这是一个向量到序列的网络。例如，输入可以是图像（或CNN的输出），而输出可以是该图像的描述文字。最后，可能有一个称为编码器的序列到向量的网络，后跟一个称为解码器的向量到序列的网络。例如，这可以用于将句子从一种语言翻译成另一种语言。可以用一种语言向网络输入一个句子，编码器会将其转换为单个向量表示，然后解码器会将此向量解码为另一种语言的句子。这种称为“编码器—解码器”(Encoder-Decoder)的两步模型比使用单个序列到序列的RNN进行即时翻译要好得多：句子的最后一个单词会影响翻译的第一个单词，因此在翻译之前需要等待，直到看完整个句子。

序列到序列，序列到向量， 向量到序列，编码器-解码器

随堂练习：简单RNN的前向传播
见其他文件夹

训练RNN
要训练RNN，诀窍是将其按照时间逐步展开，然后使用常规的反向传播。这种策略称为“时间反向传播”(BackPropagation Through Time，BPTT)。

就像在常规的反向传播中一样，首先通过展开的网络进行第一次前向传递（由虚线箭头表示）。然后，使用损失函数Loss(Y(0)，Y(1)，…，Y(T)；)（其中Y(i)）是第i个目标，Ŷ(i)是第i个预测结果，而T是最大时间步）评估输出序列。注意，此损失函数可能会忽略某些输出。例如，在序列到向量的RNN中，除了最后一个之外，所有输出都被忽略。在图中，仅基于最后三个输出计算损失函数。然后，将该损失函数的梯度通过展开的网络向后传播（由实线箭头表示）。在此示例中，由于输出Ŷ(0)和Ŷ(0)未用于计算损失，因此梯度不会向后流动，它们仅通过Ŷ(2)、Ŷ(3)和Ŷ(4)流动。此外，由于在每个时间步骤中使用相同的参数W和b，因此它们的梯度将在反向传播期间进行多次调整。完成反向阶段并计算出所有梯度后，BPTT可以执行梯度下降步骤来更新参数（这与常规反向传播没有区别）。

RNN反向传播

幸运的是，Keras为我们处理了所有这些复杂度

RNN预测时间序列
在用RNN预测之前，先加载时间序列并使用经典工具开始分析它，以更好地理解要处理的问题，并获得一些基准指标。

2001年1月1日，芝加哥有297192人乘坐公共汽车，126455人乘坐火车。day_type列包含W（表示工作日）、A（表示星期六），以及U（表示星期日或节假日）。

这是一个时间序列：具有不同时间步的值的数据，通常间隔固定。更具体地说，由于每个时间步有多个值，因此这称为多元时间序列。如果只查看bus列，它将是一元时间序列，每个时间步都有一个值。预测未来值（即预测）是处理时间序列时最典型的任务。其他任务包括插补（填补过去的缺失值）、分类、异常检测等。

从图中可以看到每周都有明显类似的模式。这称为每周季节性(seasonality)。

这个模式很强大，以至于仅通过复制一周前的值来预测明天的乘客量便可产生相当不错的结果。这称为朴素预测：通过简单地复制过去的值来做出预测。朴素预测通常是一个很好的基准，在某些情况下甚至很难被击败。

为了可视化这些朴素预测，我们用虚线叠加两个时间序列（bus和rail）以及滞后一周（即向右移动）的相同时间序列。还将绘制两者之间的差异（即时间t处的值减去时间t-7处的值），这称为差分

现在我们有个基准（即朴素预测), 尝试使用到目前为止介绍的机器学习模型来预测这个时间序列，先从一个基本的线性模型开始。目标是根据过去8周（56天）的客运量数据来预测“明天”的客运量。因此，模型的输入将是序列（一旦模型投入生产，通常每天一个序列），每个序列包含从时间步t-55到t的56个值。对于每个输入序列，模型将输出一个值：时间步t+1的预测值。

如何准备训练数据：将使用过去的每个56天（作为窗口）作为训练数据，每个窗口对应的目标值将是紧随其后的值。

Keras实际上有一个很好的实用函数（叫作tf.keras.utils.timeseries_dataset_from_array()），它可以帮助准备训练集。它以时间序列作为输入，并构建一个包含所需长度的所有窗口及其相应目标值的tf.data.Dataset

数据集中的每个样本都是一个长度为3的窗口，以及它对应的目标值（即紧接在窗口之后的值）。窗口是[0，1，2]、[1，2，3]和[2，3，4]，它们的目标值分别是3、4、5。由于一共有3个窗口，它不是批量大小的倍数，因此最后一个批次只包含一个窗口而不是两个。

获得相同结果的另一种方法是使用tf.data的Dataset类的window()方法。它更复杂，但它给了完全的控制权，window()方法返回窗口数据集的数据集

在此示例中，数据集包含6个窗口，每个窗口都比前一个窗口多移动一步，最后3个窗口较小，因为它们已到达序列的末尾

通常，将drop_remainder=True传递给window()方法来摆脱这些较小的窗口。window()方法返回一个嵌套的数据集，类似于列表的列表。当想要通过调用其数据集方法来转换每个窗口（例如，对它们进行乱序处理或对它们进行批处理）时，这很有用。但是，不能直接使用嵌套数据集进行训练，因为模型以张量而不是数据集为输入。因此，我们必须调用flat_map()方法：它将嵌套数据集转换为平面数据集（包含张量而不是数据集的数据集）。例如，假设{1，2，3}表示包含张量1、2和3序列的数据集，如果将嵌套数据集{{1，2}，{3，4，5，6}}展平，将得到平面数据集{1，2，3，4，5，6}。此外，flat_map()方法将一个函数作为参数，它允许在展平之前转换嵌套数据集中的每个数据集。例如，如果将函数lambda ds：ds.batch(2)传递给flat_map()，那么它会将嵌套数据集{{1，2}，{3，4，5，6}}转换为平面数据集{[1，2]，[3，4]，[5，6]}：它是一个包含3个张量的数据集，每个张量的大小为2。

接下来，使用timeseries_dataset_from_array()创建用于训练和验证的数据集。由于梯度下降期望训练集中的实例独立同分布(IID)，因此必须设置参数shuffle=True来打乱训练窗口

使用线性模型进行预测
使用简单的RNN进行预测
Keras中的所有循环层都期望形状为［批量大小、时间步、维度］的三维输入，其中一元时间序列的维度为1，多元时间序列的维度更高。回想一下，input_shape参数忽略了第一个维度（即批量大小），并且由于循环层可以接受任何长度的输入序列，因此可以将第二个维度设置为None，这意味着“任何大小”。

最后，由于我们处理的是一元时间序列，因此最后一个维度的大小应为1。这就是我们指定输入形状[None，1]的原因：它表示“任意长度的一元序列”。请注意，数据集实际上包含形状为［批量大小，时间步］的输入，因此缺少最后一个维度（大小为1），但在这种情况下Keras非常友好地为我们添加了它。

该模型的工作原理与之前看到RNN的完全一样：初始状态h(init)设置为0，并与第一个时间步的值x(0)一起被传递给单个循环神经元。神经元计算这些值的加权和加上偏置项，并将激活函数（默认情况下使用双曲正切函数(tanh）)应用于结果。结果是第一个输出y0。在简单的RNN中，这个输出也是新的状态h0。这个新状态与下一个输入值x(1)一起被传递到同一个循环神经元，重复上述过程直到最后一个时间步。最后，该层只输出最后一个值：在现在的数据示例中，序列步长为56，所以最后一个值是y55。所有这些都是针对批次中的每个序列（在本例中，每个批次有32个序列）同时执行的。

注意：Keras中的循环层仅返回最终输出。要使它们在每个时间步返回一个输出，必须设置return_sequences=True

这就是第一个应用到训练数据的循环模型！这是一个序列到向量的模型。由于只有一个输出神经元，因此输出向量的大小为1。现在，像之前一样编译、训练和评估这个模型：

它的验证MAE大于100000！这是意料之中的，原因有二：

该模型只有一个循环神经元，因此它在每个时间步可以用来进行预测的唯一数据是当前时间步的输入值和前一个时间步的输出值。换句话说，RNN的记忆极其有限：它只记忆一个数字，即它之前的输出。让我们数一数这个模型有多少个参数：因为该循环神经元只有2个输入值，所以整个模型只有3个参数（2个权重加上1个偏置项）。对于这个时间序列来说，这还远远不够。相比之下，我们之前的模型可以一次查看56个先前的值，它总共有57个参数。
时间序列包含从0到大约1.4的值，但由于默认激活函数是tanh，因此循环层只能输出-1到+1之间的值。它无法预测1.0到1.4之间的值。让我们来解决这两个问题：我们将创建一个具有更大循环层（包含32个循环神经元）的模型，并在其顶部添加一个密集输出层（具有一个输出神经元且没有激活函数）。循环层能够将更多信息从一个时间步携带到下一个时间步，密集输出层将最终输出从32维投影到一维，没有任何值范围限制：
会发现它的验证MAE击败了朴素预测和线性回归，是迄今训练过的最好模型

使用深度RNN进行预测
堆叠多层单元可以产生深度RNN

深度RNN随时间展开

使用Keras实现深度RNN非常简单：只需堆叠循环层即可。在下面的示例中，使用3个SimpleRNN层,前两个是序列到序列的层，最后一个是序列到向量的层。最后，Dense层（可以将其视为向量到向量的层）生成模型的预测值。所以，这个模型就像图中表示的模型一样，只不过输出Ŷ(0)到Ŷ(t-1)被忽略，并且在Ŷ(t)之上有一个密集层，它输出实际的预测值：

确保为所有循环层设置return_sequences=True（最后一层除外）。如果你忘记为某个循环层设置此参数，它将输出一个仅包含最后一个时间步的输出的二维数组，而不是包含所有时间步的输出的三维数组。下一个循环层会报错：没有以预期的三维格式为其提供序列。

如果训练并评估这个模型，它并没有打败“较浅”的RNN。看起来这个RNN对于我们的任务来说有点太大了。

多元时间序列预测
神经网络的一大优点是它们非常灵活，特别是，它们可以处理多元时间序列而几乎无须改变它们的架构。例如，尝试使用公共汽车和铁路数据作为输入来预测铁路时间序列。事实上，还可以加入日期类型，由于可以提前知道明天是工作日、周末还是假期，因们可以将日期类型序列偏移一天，这样模型就可以将“明天”的日期类型作为输入了。为了简单起见，将使用Pandas进行此处理：

现在df_mulvar是一个包含5列的DataFrame，这5列分别是公共汽车数据列、铁路数据列，以及包含第二天类型的独热编码的3列（有3种可能的日期类型W、A和U）。接下来可以重复之前的操作。首先，将数据分成训练、验证和测试：

它与univar_model RNN的唯一区别是输入形状不同：在每个时间步，该模型接收5个输入而不是1个。该模型实际的验证MAE达到了23000左右。 取得了很大的进步！

让RNN同时预测公共汽车和铁路客运量并不难，只需要在创建数据集时更改目标，针对训练集将它们设置为mulvar_train[["bus"，"rail"]][seq_length：]，针对验证集设置为mulvar_valid[["bus"，"rail"]][seq_length：]。还必须在输出Dense层中添加一个额外的神经元，因为它现在必须进行两项预测：一项针对明天的公共汽车客运量，另一项针对铁路客运量

对多个相关任务使用单个模型可能会比对每个任务使用单独的模型产生更好的性能，不仅因为针对一个任务学习的特征可能对其他任务也有用，而且还因为必须跨多个任务表现良好可以防止模型过拟合

预测未来多个时间步
到目前为止，只预测了下一个时间步的值，但通过适当地改变目标我们可以预测未来多个时间步的值（例如，要预测未来2周的客运量，只需将目标更改为未来14天的值而不是1天后的值）

训练RNN一次性预测未来的14个值。仍然可以使用序列到向量的模型，但它会输出14个值而不是1个值。但是，首先需要将目标更改为包含接下来的14个值的向量。为此，可以再次使用timeseries_dataset_from_array()，但这次要求它创建没有目标(targets=None)且更长的序列的数据集，长度为seq_length+14。然后，可以使用数据集的map()方法将自定义函数应用于每批序列，将它们拆分为输入和目标值。在这个例子中，使用多元时间序列作为输入（使用所有5列），预测未来14天的铁路客运量

使用序列到序列模型进行预测
与其训练模型仅在最后一个时间步预测接下来的14个值，不如训练它在每个时间步预测接下来的14个值。换句话说，可以将这个序列到向量的循环神经网络转变为序列到序列的循环神经网络。这种技术的优点是损失将包含RNN在每个时间步的输出的损失项，而不仅仅包含最后一个时间步的输出的损失项。

这意味着将有更多的误差梯度流过模型，并且它们不必在时间中流动那么多，因为它们来自每个时间步的输出，而不仅仅是来自最后一个时间步。这既能稳定训练，又能加速训练。

需要明确的是，在时间步0，模型将输出一个向量，其中包含时间步1到14的预测值。在时间步1，模型将预测时间步2到15的值，以此类推。换句话说，目标是连续窗口的序列，在每个时间步移动一个时间步。目标不再是向量，而是与输入长度相同的序列，每一步都包含一个14维向量。

准备数据集并不简单，因为每个实例都有一个作为输入的窗口和一个作为输出的窗口序列。一种方法是使用之前创建的to_windows()实用函数，连续两次，获取连续窗口的窗口。例如，将数字0到6的序列转换为包含4个连续窗口的序列的数据集，每个窗口的长度为3：

现在数据集包含长度为4的序列作为输入，目标是包含每个时间步的接下来两个步骤的序列。例如，第一个输入序列是[0，1，2，3]，它对应的目标是[[1，2]，[2，3]，[3，4]，[4，5]]，即每个时间步的下两个值。

它与之前的模型几乎相同：唯一的区别是我们在SimpleRNN层中设置了return_sequences=True。这样，它将输出一个向量（每个大小为32）序列，而不是在最后一个时间步输出单个向量。Dense层足够智能，可以将序列作为输入处理：它将在每个时间步应用，将32维向量作为输入并输出14维向量。事实上，另一种获得完全相同结果的方法是使用核大小为1的Conv1D层：Conv1D(14，kernel_size=1)。

训练代码和平时一样。在训练过程中，模型的所有输出都会被使用，但训练后只有最后一个时间步的输出很重要，其余的可以忽略。

处理长序列
简单的RNN可以很好地预测时间序列或处理其他类型的序列，但它们在长时间序列上表现不佳。要在长序列上训练RNN，必须在许多时间步上运行它，使展开的RNN成为一个非常深的网络。就像所有深度神经网络一样，它可能会遇到不稳定梯度问题：训练可能需要很长时间，或者训练可能不稳定。此外，当RNN处理一个长序列时，它会逐渐忘记序列中的第一个输入。所以长序列的训练需要解决不稳定梯度和遗忘问题，先看看不稳定梯度问题

解决不稳定的梯度问题
在深度网络中用于缓解不稳定梯度问题的许多技巧也可用于RNN：良好的参数初始化、更快的优化器、dropout等。然而，非饱和激活函数（例如ReLU）在这里可能没有那么大的帮助。事实上，它们实际上可能导致RNN在训练过程中变得更加不稳定。为什么？假设梯度下降以一种在第一个时间步略微增加输出的方式更新权重。因为在每个时间步都使用相同的权重，所以第二个时间步的输出可能略有增加，第三个时间步的输出也可能略有增加，以此类推，直到输出爆炸——非饱和激活函数不会阻止这种情况的发生。

我们可以通过较小的学习率来降低这种风险，或者可以使用像双曲正切函数这样的饱和激活函数（这解释了为什么它是默认值）。

以几乎相同的方式，梯度本身也可能爆炸。如果你注意到训练不稳定，那么可能需要监控梯度的大小（例如使用TensorBoard）并使用梯度裁剪。

此外，批量归一化不能像在深度前馈网络中那样有效地用于RNN。事实上，我们不能在时间步之间使用它，只能在循环层之间使用它。

更准确地说，在技术上可以将BN层添加到记忆单元，以便在每个时间步（在该时间步的输入和上一步的隐藏状态上）应用它。然而，无论输入和隐藏状态的实际尺度和偏移量如何，每个时间步都将使用相同的BN层（具有相同的参数）。实际上，这并没有产生好的结果，2015年的论文发现BN仅在应用于层的输入而不是隐藏状态时才略微有益。换句话说，在循环层之间（垂直方向）应用它比没有应用效果稍微好一点，但不应在循环层内（水平方向）应用。在Keras中，可以通过在每个循环层之前添加BatchNormalization层来简单地在层之间应用BN，但这会减慢训练速度，而且可能没有太大帮助。

另一种形式的归一化通常更适用于RNN：层归一化。它与批量归一化非常相似，但层归一化不是跨批量维度归一化，而是跨特征维度归一化。它的一个优点是可以在每个时间步独立地为每个实例即时计算所需的统计数据。这也意味着它在训练和测试期间的行为方式相同（与BN不同），并且它不需要像BN那样使用指数移动平均来估计训练集中所有实例的特征统计量。与BN一样，层归一化为每个输入学习一个尺度和一个偏移参数。在RNN中，它通常在输入和隐藏状态的线性组合之后立即使用。

请注意，states参数是一个包含一个或多个张量的列表。在简单的RNN单元下，它包含一个等于前一个时间步的输出的张量，但其他单元可能有多个状态张量（例如，LSTMCell有一个长期状态和一个短期状态）。单元还必须具有state_size属性和output_size属性。在简单的RNN中，两者都等于单元数。以下代码实现了一个自定义记忆单元，其行为类似于SimpleRNNCell，只不过它还会在每个时间步应用层归一化：

LNSimpleRNNCell类继承自tf.keras.layers.Layer类，就像其他自定义层一样。
构造函数接受单元数和所需的激活函数并设置state_size和output_size属性，然后创建一个没有激活函数的SimpleRNNCell（因为想在线性操作之后、激活函数之前执行层归一化）。
之后，构造函数创建LayerNormalization层，最后它获取所需的激活函数。
call()方法首先应用SimpleRNNCell，它计算当前输入和先前隐藏状态的线性组合，并返回结果两次（实际上，在SimpleRNNCell中，输出正好等于隐藏状态，换句话说，new_states[0]等于outputs，所以可以忽略call()方法其余部分中的new_states）。接下来，call()方法应用层归一化，然后应用激活函数。
最后，它返回输出两次：一次作为输出，一次作为新的隐藏状态。要使用这个自定义单元，需要做的就是创建一个tf.keras.layers.RNN层，并向它传递一个单元实例：
同样，可以创建一个自定义单元以在每个时间步之间应用dropout。但是，有一个更简单的方法：Keras提供的大多数循环层和单元都有dropout和recurrent_dropout超参数：前者定义应用于输入的dropout率，后者定义时间步之间隐藏状态的dropout率。因此，无须创建自定义单元来在RNN的每个时间步应用dropout。使用这些技术，可以缓解不稳定的梯度问题并有效地训练RNN

处理短期记忆问题
由于数据在遍历RNN时会经历转换，因此在每个时间步都会丢失一些信息。一段时间后，RNN的状态几乎不包含第一个输入的踪迹。为了解决这个问题，人们引入了具有长期记忆的各种类型的单元。它们已被证明非常成功，以至于基本单元不再被大量使用。首先看看这些长期记忆单元中最受欢迎的LSTM单元。

LSTM单元
长短期记忆(Long Short-Term Memory，LSTM)单元于1997年首次提出，多年来由研究人员逐步改进。如果将LSTM单元视为黑盒，它可以像基本单元一样使用，但性能要好得多：训练会收敛得更快，并且能够检测数据中的长期模式。在Keras中，可以简单地使用LSTM层而不是SimpleRNN层：

LSTM单元架构如图所示。如果不看黑盒里面的东西，LSTM单元看起来就像一个普通单元，除了它的状态被分成两个向量：h(t)和c(t)［“c”代表“单元”(cell)］。可以将h(t)视为短期状态，将c(t)视为长期状态。

LSTM单元

关键思想是网络可以学习在长期状态中存储什么，丢弃什么，以及从中读取什么。当长期状态c(t-1)从左到右遍历网络时，它首先通过遗忘门，丢弃一些记忆，然后通过加法运算添加一些新记忆（添加由输入门选择的记忆）。结果c(t)直接被发送出去，没有进行进一步的转换。因此，在每个时间步，都会删除一些记忆并添加一些记忆。而且，在加法运算之后，复制长期状态并使用tanh函数，然后通过输出门对结果进行过滤。这会产生短期状态h(t)（等于此时间步的单元输出）。

现在看看新的记忆是从哪里来的，以及这些门是如何工作的。首先，当前输入向量x(t)和先前的短期状态h(t-1)被馈送到4个不同的全连接层。它们都有不同的用途：

主要的层是输出g(t)的层。它具有分析当前输入x(t)和先前（短期）状态h(t-1)的作用。在基本单元中，除了这一层外别无其他，它的输出直接输出到ŷ(t)和h(t)。但是，在LSTM单元中，该层的输出不会直接输出，相反，它最重要的部分存储在长期状态中（其余部分被丢弃）。
其他三层是门控制器。由于它们使用逻辑(logistic)激活函数，输出范围为0～1。门控制器的输出被馈送到逐元素乘法运算单元：如果它们输出0，则关闭门；如果输出1，则打开门。具体来说：
◆ 遗忘门（由f(t)控制）控制应擦除长期状态的哪些部分。

◆ 输入门（由i(t)控制）控制g(t)的哪些部分应添加到长期状态。

◆ 输出门（由o(t)控制）控制应在该时间步读取和输出长期状态的哪些部分，包括h(t)和y(t)。

简而言之，LSTM单元可以学习识别重要输入（这是输入门的作用），将其存储在长期状态中，在需要时保留它（这是遗忘门的作用），并在需要时提取它。这就解释了为什么这些单元在捕捉时间序列、长文本、录音等的长期模式方面取得了惊人的成功。

下面公式总结了如何计算单元的长期状态，短期状态以及单个实例在每个时间步的输出。

公式：LSTM的计算

 

在这个等式中：

Wxi、Wxf、Wxo和Wxg是4个层连接到输入向量x(t)的权重矩阵。
Whi、Whf、Who和Whg是4个层的权重矩阵，用于连接到先前的短期状态h(t-1)。
bi、bf、bo和bg是4个层的偏置项。请注意，TensorFlow将bf初始化为一个全1而不是全0的向量。这可以防止在训练开始时忘记一切。
GRU单元
GRU单元是LSTM单元的简化版本，它的表现和LSTM差不多，以下是主要的简化：

两个状态向量合并为一个向量h(t)。
单个门控制器z(t)控制遗忘门和输入门。如果门控制器输出1，则遗忘门打开(=1)，输入门关闭(1-1=0)。如果它输出0，则相反。换句话说，每当必须存储记忆时，将首先擦除存储它的位置。这实际上是LSTM单元本身的常见变体。
没有输出门，在每个时间步输出完整的状态向量。但是，有一个新的门控制器r(t)，它控制先前状态的哪一部分将显示给主要的层(g(t))。
GRU单元

公式：GRU计算

 

Keras提供了一个tf.keras.layers.GRU层，使用它只是用GRU替换SimpleRNN或LSTM，非常方便；还提供了一个tf.keras.layers.GRUCell, 可以基于它来创建自定义单元

```python
import tensorflow as tf
my_series = [0,1,2,3,4,5]
my_dataset = tf.keras.utils.timeseries_dataset_from_array(
    my_series,
    targets=my_series[3:],
    sequence_length=3,
    batch_size=2
)
my_dataset

# 0 1 2 -> 3
# 1 2 3 ->  4
# 2 3 4  -> 5
```

```python
list(my_dataset)  #
```

数据集中的每个样本都是一个长度为3的窗口，以及它对应的目标值（即紧接在窗口之后的值）。窗口是[0，1，2]、[1，2，3]和[2，3，4]，它们的目标值分别是3、4、5。由于一共有3个窗口，它不是批量大小的倍数，因此最后一个批次只包含一个窗口而不是两个。

获得相同结果的另一种方法是使用tf.data的Dataset类的window()方法。它更复杂，但它给了完全的控制权，window()方法返回窗口数据集的数据集

```python
for window_dataset in tf.data.Dataset.range(6).window(4, shift=1):
    for element in window_dataset:
        print(f"{element}", end=" ")
    print()
```

在此示例中，数据集包含6个窗口，每个窗口都比前一个窗口多移动一步，最后3个窗口较小，因为它们已到达序列的末尾

通常，将drop_remainder=True传递给window()方法来摆脱这些较小的窗口。window()方法返回一个嵌套的数据集，类似于列表的列表。当想要通过调用其数据集方法来转换每个窗口（例如，对它们进行乱序处理或对它们进行批处理）时，这很有用。但是，不能直接使用嵌套数据集进行训练，因为模型以张量而不是数据集为输入。因此，我们必须调用flat_map()方法：它将嵌套数据集转换为平面数据集（包含张量而不是数据集的数据集）。例如，假设{1，2，3}表示包含张量1、2和3序列的数据集，如果将嵌套数据集{{1，2}，{3，4，5，6}}展平，将得到平面数据集{1，2，3，4，5，6}。此外，flat_map()方法将一个函数作为参数，它允许在展平之前转换嵌套数据集中的每个数据集。例如，如果将函数lambda ds：ds.batch(2)传递给flat_map()，那么它会将嵌套数据集{{1，2}，{3，4，5，6}}转换为平面数据集{[1，2]，[3，4]，[5，6]}：它是一个包含3个张量的数据集，每个张量的大小为2。

```python
dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window_dataset: window_dataset.batch(4))
for window_tensor in dataset:
    print(f"{window_tensor}")
```

```python
# 创建辅助函数，更方便地从数据集提取窗口
def to_windows(dataset, length):
    dataset = dataset.window(length, shift=1, drop_remainder=True)
    return dataset.flat_map(lambda window_ds: window_ds.batch(length))
```

```python
# 使用map方法将每个窗口拆分为输入和目标值，然后将生成的窗口分组为大小为2的批次
dataset = to_windows(tf.data.Dataset.range(6), 4) # 0 1 2 3, 1 2 3 4, 2 3 4 5
dataset = dataset.map(lambda window: (window[:-1], window[-1]))
list(dataset.batch(2))
```

```python
# 拆分训练，验证和测试，并归一化
rail_train = df["rail"]["2016-01":"2018-12"] / 1e6
rail_valid = df["rail"]["2019-01":"2019-05"] / 1e6
rail_test = df["rail"]["2019-06":] / 1e6
```

接下来，使用timeseries_dataset_from_array()创建用于训练和验证的数据集。由于梯度下降期望训练集中的实例独立同分布(IID)，因此必须设置参数shuffle=True来打乱训练窗口

```python
seq_length = 56
train_ds = tf.keras.utils.timeseries_dataset_from_array(
    rail_train.to_numpy(),
    targets=rail_train[seq_length:],
    sequence_length=seq_length,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_ds = tf.keras.utils.timeseries_dataset_from_array(
    rail_valid.to_numpy(),
    targets=rail_valid[seq_length:],
    sequence_length=seq_length,
    batch_size=32,
)

# train_ds : [32个，(x(y-55), ... x(t)), y(t+1)), 32个 (x(y-55), ... x(t)), y(t+1)) ... ]
```

### 使用线性模型进行预测

```python
tf.random.set_seed(42)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(1, input_shape=[seq_length])
])
early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor="val_mae", patience=50, restore_best_weights=True)
opt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])

history = model.fit(train_ds, validation_data=valid_ds, epochs=500, callbacks=[early_stopping_cb])
```

```python
model.evaluate(valid_ds)[-1] * 1e6
```

### 使用简单的RNN进行预测

```python
model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])
])

```

Keras中的所有循环层都期望形状为［批量大小、时间步、维度］的三维输入，其中一元时间序列的维度为1，多元时间序列的维度更高。回想一下，input_shape参数忽略了第一个维度（即批量大小），并且由于循环层可以接受任何长度的输入序列，因此可以将第二个维度设置为None，这意味着“任何大小”。

最后，由于我们处理的是一元时间序列，因此最后一个维度的大小应为1。这就是我们指定输入形状[None，1]的原因：它表示“任意长度的一元序列”。请注意，数据集实际上包含形状为［批量大小，时间步］的输入，因此缺少最后一个维度（大小为1），但在这种情况下Keras非常友好地为我们添加了它。

该模型的工作原理与之前看到RNN的完全一样：初始状态h(init)设置为0，并与第一个时间步的值x(0)一起被传递给单个循环神经元。神经元计算这些值的加权和加上偏置项，并将激活函数（默认情况下使用双曲正切函数(tanh）)应用于结果。结果是第一个输出y0。在简单的RNN中，这个输出也是新的状态h0。这个新状态与下一个输入值x(1)一起被传递到同一个循环神经元，重复上述过程直到最后一个时间步。最后，该层只输出最后一个值：在现在的数据示例中，序列步长为56，所以最后一个值是y55。所有这些都是针对批次中的每个序列（在本例中，每个批次有32个序列）同时执行的。

注意：Keras中的循环层仅返回最终输出。要使它们在每个时间步返回一个输出，必须设置return_sequences=True

这就是第一个应用到训练数据的循环模型！这是一个序列到向量的模型。由于只有一个输出神经元，因此输出向量的大小为1。现在，像之前一样编译、训练和评估这个模型：

```python
# todo： 随堂练习， 编译，训练，并在训练结束后在验证集评估这个模型
def fit_and_evaluate(model, learning_rate, train_ds=train_ds, valid_ds=valid_ds):
    early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor="val_mae", patience=50, restore_best_weights=True)
    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
    model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])
    history = model.fit(train_ds, validation_data=valid_ds, epochs=500, callbacks=[early_stopping_cb])

    print("验证集的误差: " + str(model.evaluate(valid_ds)[-1] * 1e6))

```

```python
fit_and_evaluate(model, 0.02)
```

它的验证MAE大于100000！这是意料之中的，原因有二：
1. 该模型只有一个循环神经元，因此它在每个时间步可以用来进行预测的唯一数据是当前时间步的输入值和前一个时间步的输出值。换句话说，RNN的记忆极其有限：它只记忆一个数字，即它之前的输出。让我们数一数这个模型有多少个参数：因为该循环神经元只有2个输入值，所以整个模型只有3个参数（2个权重加上1个偏置项）。对于这个时间序列来说，这还远远不够。相比之下，我们之前的模型可以一次查看56个先前的值，它总共有57个参数。
2. 时间序列包含从0到大约1.4的值，但由于默认激活函数是tanh，因此循环层只能输出-1到+1之间的值。它无法预测1.0到1.4之间的值。让我们来解决这两个问题：我们将创建一个具有更大循环层（包含32个循环神经元）的模型，并在其顶部添加一个密集输出层（具有一个输出神经元且没有激活函数）。循环层能够将更多信息从一个时间步携带到下一个时间步，密集输出层将最终输出从32维投影到一维，没有任何值范围限制：

```python
# todo: 随堂练习，用这个解决方案搭建模型，并编译，训练，在训练结束后在验证集评估这个模型，
model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),
    tf.keras.layers.Dense(1)
])

fit_and_evaluate(model, 0.02)
```

```python
model.predict(valid_ds.take(1)).shape
```

会发现它的验证MAE击败了朴素预测和线性回归，是迄今训练过的最好模型

### 使用深度RNN进行预测

堆叠多层单元可以产生深度RNN

![深度RNN随时间展开](./images/RNN/p4.png)

使用Keras实现深度RNN非常简单：只需堆叠循环层即可。在下面的示例中，使用3个SimpleRNN层,前两个是序列到序列的层，最后一个是序列到向量的层。最后，Dense层（可以将其视为向量到向量的层）生成模型的预测值。所以，这个模型就像图中表示的模型一样，只不过输出Ŷ(0)到Ŷ(t-1)被忽略，并且在Ŷ(t)之上有一个密集层，它输出实际的预测值：

```python
deep_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),
    tf.keras.layers.SimpleRNN(32, return_sequences=True),
    tf.keras.layers.SimpleRNN(32),
    tf.keras.layers.Dense(1)
])
```

确保为所有循环层设置return_sequences=True（最后一层除外）。如果你忘记为某个循环层设置此参数，它将输出一个仅包含最后一个时间步的输出的二维数组，而不是包含所有时间步的输出的三维数组。下一个循环层会报错：没有以预期的三维格式为其提供序列。

如果训练并评估这个模型，它并没有打败“较浅”的RNN。看起来这个RNN对于我们的任务来说有点太大了。

```python
fit_and_evaluate(deep_model, 0.02)
```

```python
deep_model.evaluate(valid_ds)
```

### 多元时间序列预测

神经网络的一大优点是它们非常灵活，特别是，它们可以处理多元时间序列而几乎无须改变它们的架构。例如，尝试使用公共汽车和铁路数据作为输入来预测铁路时间序列。事实上，还可以加入日期类型，由于可以提前知道明天是工作日、周末还是假期，因们可以将日期类型序列偏移一天，这样模型就可以将“明天”的日期类型作为输入了。为了简单起见，将使用Pandas进行此处理：

```python
df_mulvar = df[["bus", "rail"]] / 1e6
df_mulvar["next_day_type"] = df["day_type"].shift(-1)
df_mulvar

df_mulvar = pd.get_dummies(df_mulvar, dtype=float)
df_mulvar
```

现在df_mulvar是一个包含5列的DataFrame，这5列分别是公共汽车数据列、铁路数据列，以及包含第二天类型的独热编码的3列（有3种可能的日期类型W、A和U）。接下来可以重复之前的操作。首先，将数据分成训练、验证和测试：

```python
mulvar_train = df_mulvar["2016-01":"2018-12"]
mulvar_valid = df_mulvar["2019-01":"2019-05"]
mulvar_test = df_mulvar["2019-06":]
```

```python
tf.random.set_seed(42)
train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),  # 使用5列作为输入
    targets=mulvar_train["rail"][seq_length:],  # 只预测铁路序列
    sequence_length=seq_length,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=mulvar_valid["rail"][seq_length:],
    sequence_length=seq_length,
    batch_size=32
)
```

```python
tf.random.set_seed(42)
mulvar_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(1)
])
```

```python
fit_and_evaluate(mulvar_model, 0.02, train_ds = train_mulvar_ds, valid_ds = valid_mulvar_ds)
```

它与univar_model RNN的唯一区别是输入形状不同：在每个时间步，该模型接收5个输入而不是1个。该模型实际的验证MAE达到了23000左右。 取得了很大的进步！

让RNN同时预测公共汽车和铁路客运量并不难，只需要在创建数据集时更改目标，针对训练集将它们设置为mulvar_train[["bus"，"rail"]][seq_length：]，针对验证集设置为mulvar_valid[["bus"，"rail"]][seq_length：]。还必须在输出Dense层中添加一个额外的神经元，因为它现在必须进行两项预测：一项针对明天的公共汽车客运量，另一项针对铁路客运量

对多个相关任务使用单个模型可能会比对每个任务使用单独的模型产生更好的性能，不仅因为针对一个任务学习的特征可能对其他任务也有用，而且还因为必须跨多个任务表现良好可以防止模型过拟合

```python
tf.random.set_seed(42)

seq_length = 56
train_multask_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),
    targets=mulvar_train[["bus", "rail"]][seq_length:],  # 2个目标
    sequence_length=seq_length,
    batch_size=32,
    shuffle=True,
    seed=42
)
valid_multask_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=mulvar_valid[["bus", "rail"]][seq_length:],
    sequence_length=seq_length,
    batch_size=32
)

tf.random.set_seed(42)
multask_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(2)
])

fit_and_evaluate(multask_model, 0.02, train_multask_ds, valid_multask_ds)
```

```python
# 评估多任务RNN的预测
Y_preds_valid = multask_model.predict(valid_multask_ds)
Y_preds_valid
for idx, name in enumerate(["bus", "rail"]):
    mae = 1e6 * tf.keras.metrics.MeanAbsoluteError()(
        mulvar_valid[name][seq_length:], Y_preds_valid[:, idx])
    print(name, int(mae))
```

### 预测未来多个时间步

到目前为止，只预测了下一个时间步的值，但通过适当地改变目标我们可以预测未来多个时间步的值（例如，要预测未来2周的客运量，只需将目标更改为未来14天的值而不是1天后的值）

训练RNN一次性预测未来的14个值。仍然可以使用序列到向量的模型，但它会输出14个值而不是1个值。但是，首先需要将目标更改为包含接下来的14个值的向量。为此，可以再次使用timeseries_dataset_from_array()，但这次要求它创建没有目标(targets=None)且更长的序列的数据集，长度为seq_length+14。然后，可以使用数据集的map()方法将自定义函数应用于每批序列，将它们拆分为输入和目标值。在这个例子中，使用多元时间序列作为输入（使用所有5列），预测未来14天的铁路客运量

```python
def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):
    # (32, 70, 5)
    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]


ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),
    targets=None,
    sequence_length=seq_length+14,
    batch_size=32,
    shuffle=True,
    seed=42
).map(split_inputs_and_targets)

ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=None,
    sequence_length=seq_length+14,
    batch_size=32).map(split_inputs_and_targets)
```

```python
# 输出用14个单元
ahead_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
])
```

```python
fit_and_evaluate(ahead_model, 0.02, ahead_train_ds, ahead_valid_ds)
```

```python
# 训练好后 一次预测接下来的14个值
import numpy as np
X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]
Y_pred = ahead_model.predict(X)  # shape [1, 14]
```

### 使用序列到序列模型进行预测

与其训练模型仅在最后一个时间步预测接下来的14个值，不如训练它在每个时间步预测接下来的14个值。换句话说，可以将这个序列到向量的循环神经网络转变为序列到序列的循环神经网络。这种技术的优点是损失将包含RNN在每个时间步的输出的损失项，而不仅仅包含最后一个时间步的输出的损失项。

这意味着将有更多的误差梯度流过模型，并且它们不必在时间中流动那么多，因为它们来自每个时间步的输出，而不仅仅是来自最后一个时间步。这既能稳定训练，又能加速训练。

需要明确的是，在时间步0，模型将输出一个向量，其中包含时间步1到14的预测值。在时间步1，模型将预测时间步2到15的值，以此类推。换句话说，目标是连续窗口的序列，在每个时间步移动一个时间步。目标不再是向量，而是与输入长度相同的序列，每一步都包含一个14维向量。

准备数据集并不简单，因为每个实例都有一个作为输入的窗口和一个作为输出的窗口序列。一种方法是使用之前创建的to_windows()实用函数，连续两次，获取连续窗口的窗口。例如，将数字0到6的序列转换为包含4个连续窗口的序列的数据集，每个窗口的长度为3：

```python
my_series = tf.data.Dataset.range(7)
dataset = to_windows(to_windows(my_series, 3), 4)
list(dataset)
```

```python
# 可以使用map()方法将这些窗口的窗口拆分为输入和目标
dataset = dataset.map(lambda S: (S[:, 0], S[:, 1:]))
list(dataset)
```

现在数据集包含长度为4的序列作为输入，目标是包含每个时间步的接下来两个步骤的序列。例如，第一个输入序列是[0，1，2，3]，它对应的目标是[[1，2]，[2，3]，[3，4]，[4，5]]，即每个时间步的下两个值。

```python
# 创建辅助函数为序列到序列模型准备数据集。
def to_seq2seq_dataset(series, seq_length=56, ahead=14, target_col=1, batch_size=32, shuffle=False, seed=None):
    ds = to_windows(tf.data.Dataset.from_tensor_slices(series), ahead+1)
    ds = to_windows(ds, seq_length).map(lambda S: (S[:, 0], S[:, 1:, 1]))
    if shuffle:
        ds = ds.shuffle(8 * batch_size, seed=seed)
    return ds.batch(batch_size)

```

```python
# 创建数据集
seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle=True, seed=42)
seq2seq_valid = to_seq2seq_dataset(mulvar_valid)

# 构建序列模型
seq2seq_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 5]),
    tf.keras.layers.Dense(14)
])
```

它与之前的模型几乎相同：唯一的区别是我们在SimpleRNN层中设置了return_sequences=True。这样，它将输出一个向量（每个大小为32）序列，而不是在最后一个时间步输出单个向量。Dense层足够智能，可以将序列作为输入处理：它将在每个时间步应用，将32维向量作为输入并输出14维向量。事实上，另一种获得完全相同结果的方法是使用核大小为1的Conv1D层：Conv1D(14，kernel_size=1)。

```python
seq2seq_model.summary()
```

训练代码和平时一样。在训练过程中，模型的所有输出都会被使用，但训练后只有最后一个时间步的输出很重要，其余的可以忽略。

```python
opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)
seq2seq_model.compile(loss=tf.keras.losses.Huber(), optimizer=opt, metrics=["mae"])
history = seq2seq_model.fit(seq2seq_train, validation_data=seq2seq_valid, epochs=500, callbacks=[early_stopping_cb])
```

```python
X = mulvar_valid.to_numpy()[np.newaxis, :seq_length]
y_pred_14 = seq2seq_model.predict(X)[0,-1] # 只需要最后一个时间步的输出
```

```python
Y_pred_valid = seq2seq_model.predict(seq2seq_valid)
for ahead in range(14):
    preds = pd.Series(Y_pred_valid[:-1, -1, ahead],
                      index=mulvar_valid.index[56 + ahead : -14 + ahead]) # 因为最后几个窗口没有完整 14 步预测（靠近结尾的数据不足），所以这里裁掉最后 14 步，保证索引与预测数组 Y_pred_valid[:-1] 长度匹配。
    mae = (preds - mulvar_valid["rail"]).abs().mean() * 1e6
    print(f"MAE for +{ahead + 1}: {mae:,.0f}")

```

## 处理长序列

简单的RNN可以很好地预测时间序列或处理其他类型的序列，但它们在长时间序列上表现不佳。要在长序列上训练RNN，必须在许多时间步上运行它，使展开的RNN成为一个非常深的网络。就像所有深度神经网络一样，它可能会遇到不稳定梯度问题：训练可能需要很长时间，或者训练可能不稳定。此外，当RNN处理一个长序列时，它会逐渐忘记序列中的第一个输入。所以长序列的训练需要解决不稳定梯度和遗忘问题，先看看不稳定梯度问题

### 解决不稳定的梯度问题

在深度网络中用于缓解不稳定梯度问题的许多技巧也可用于RNN：良好的参数初始化、更快的优化器、dropout等。然而，非饱和激活函数（例如ReLU）在这里可能没有那么大的帮助。事实上，它们实际上可能导致RNN在训练过程中变得更加不稳定。为什么？假设梯度下降以一种在第一个时间步略微增加输出的方式更新权重。因为在每个时间步都使用相同的权重，所以第二个时间步的输出可能略有增加，第三个时间步的输出也可能略有增加，以此类推，直到输出爆炸——非饱和激活函数不会阻止这种情况的发生。

我们可以通过较小的学习率来降低这种风险，或者可以使用像双曲正切函数这样的饱和激活函数（这解释了为什么它是默认值）。

以几乎相同的方式，梯度本身也可能爆炸。如果你注意到训练不稳定，那么可能需要监控梯度的大小（例如使用TensorBoard）并使用梯度裁剪。

此外，批量归一化不能像在深度前馈网络中那样有效地用于RNN。事实上，我们不能在时间步之间使用它，只能在循环层之间使用它。

更准确地说，在技术上可以将BN层添加到记忆单元，以便在每个时间步（在该时间步的输入和上一步的隐藏状态上）应用它。然而，无论输入和隐藏状态的实际尺度和偏移量如何，每个时间步都将使用相同的BN层（具有相同的参数）。实际上，这并没有产生好的结果，2015年的论文发现BN仅在应用于层的输入而不是隐藏状态时才略微有益。换句话说，在循环层之间（垂直方向）应用它比没有应用效果稍微好一点，但不应在循环层内（水平方向）应用。在Keras中，可以通过在每个循环层之前添加BatchNormalization层来简单地在层之间应用BN，但这会减慢训练速度，而且可能没有太大帮助。

另一种形式的归一化通常更适用于RNN：层归一化。它与批量归一化非常相似，但层归一化不是跨批量维度归一化，而是跨特征维度归一化。它的一个优点是可以在每个时间步独立地为每个实例即时计算所需的统计数据。这也意味着它在训练和测试期间的行为方式相同（与BN不同），并且它不需要像BN那样使用指数移动平均来估计训练集中所有实例的特征统计量。与BN一样，层归一化为每个输入学习一个尺度和一个偏移参数。在RNN中，它通常在输入和隐藏状态的线性组合之后立即使用。



  
