# sklearn做分类项目

截止到现在上面全是回归项目，以下是分类问题

```python
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, GridSearchCV

mnist = fetch_openml('mnist_784', as_frame=False, parser="auto")
```



## 数据集与加载说明：`fetch_openml('mnist_784')`

### 1. 数据集介绍：`MNIST`

`MNIST`（Mixed National Institute of Standards and Technology）是机器学习和计算机视觉领域最经典的图像分类数据集之一：

| 内容     | 描述                             |
| -------- | -------------------------------- |
| 样本数量 | 70,000 张图像                    |
| 图像大小 | 28×28 像素的灰度图               |
| 特征数量 | 784（即 28×28 拉平成一维）       |
| 标签     | 数字 0 到 9（共 10 类）          |
| 类型     | 分类任务（digit classification） |

每个图像被展开成一个 784 维向量（784 个像素），其对应标签是图像中表示的数字。


### 2. 函数介绍：`fetch_openml()`

`fetch_openml()` 是 `scikit-learn` 提供的一个数据获取工具，它允许我们从 [OpenML 数据库](https://www.openml.org/) 下载标准公开数据集，常用于快速实验。

sklearn.datasets包含3中类型的函数：fetch_*用来下载数据集，load_*用来加载和sklearn捆绑的小数据集（不需要互联网下载），make_*用于生成假数据集，可用于测试

基本用法：

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784')
```

| 参数            | 含义                                   |
| --------------- | -------------------------------------- |
| `'mnist_784'`   | `OpenML` 上 `MNIST` 数据集的 ID 或名称 |
| `as_frame`      | 是否返回 `pandas.DataFrame` 格式       |
| `parser='auto'` | 自动选择数据解析方式，推荐保留此默认值 |

### 3. 参数说明：`as_frame=False`

* 默认：`as_frame=True` 会将数据以 `pandas.DataFrame` 和 `pandas.Series` 的形式返回。
* 在本例中我们设置 `as_frame=False`，其效果是：

> 返回的数据将是 `NumPy` 数组格式：

> 为什么使用 `as_frame=False`？

* 图像数据是一堆像素值的矩阵，不适合看成结构化的表格

> 返回值说明

数据集作为``sklearn.utils.Bunch`对象返回，这些对象是字典，键也可以通过属性访问。 "DESCR"数据集的描述，"data"输入数据，"target"标签"

```python
print(mnist.DESCR)
```

```python
# 获取图片特征和分类标签
X, y = mnist.data, mnist.target
X
X.shape
y
```

todo: 介绍图片特征 和 绘图函数

```python
import matplotlib.pyplot as plt

def plot_digit(img_data):
    image = img_data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")

some_digit = X[0]   # 图像和 y[0]对应
plot_digit(some_digit)
plt.show()
```

```python
# 随堂练习：利用plt 画前100张手写数字图片， 提示：plt.subplots
fig ,axes = plt.subplots(10,10,figsize=(9,9))
def plot_digitxy(img_data,ax):
    image = img_data.reshape(28, 28)
    ax.imshow(image, cmap="binary")
    ax.axis("off")
for i in range(10):
    for j in range(10):
        plot_digitxy(X[i*10 + j], axes[i][j])
plt.show()
```

```python
# 简单的训练集和测试集分离
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

plt.figure(figsize=(9, 9))
for idx, image_data in enumerate(X[:100]):
    plt.subplot(10, 10, idx + 1)
    plot_digit(image_data)
plt.subplots_adjust(wspace=0, hspace=0) 		# 去除子图的横纵坐标轴
plt.show()
```

```python
# 随堂练习：根据标签的分布情况，来分层采样测试集; 并验证分离后 维持了原数据的标签分布
from sklearn.model_selection import train_test_split
X_train_stract, X_test_stract, y_train_stract, y_test_stract = train_test_split(X , y ,test_size = 1.0/7,strasity = y)
```


训练集已经打乱了，这可以保证所有交叉验证折叠都是相似的（不会在某个折叠丢失数字）。此外，一些算法对训练集的顺序敏感，如果连续获得相似的实例，它们的性能会很差，打乱数据集可以确保不发生这种情况

## 使用随机梯度下降线性分类器进行训练

```python
y_train_5 = y_train == '5'
y_test_5 = y_test == '5'

from sklearn.linear_model import SGDClassifier          # 其实就是逻辑回归模型

sgd_clf = SGDClassifier(loss="log_loss", random_state=100)   # 使用log损失函数， 随机梯度先将训练
sgd_clf.fit(X_train, y_train_5)
```

```python
# 用来检测是否是数字5
some_digit = X[0]
sgd_clf.predict([some_digit])
```

todo: 性能测量

```python
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
```

有时，需要比`Scikit-Learn`提供的工具更多地控制交叉验证过程。在这些情况下(比如特定的数据划分策略，非标准评估指标），可以自己实现交叉验证

以下代码自己实现交叉验证，与`Scikit-Learn`的`cross_val_score()`函数大致相同，它输出相同的结果

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
import numpy as np

skfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is not
                                       # already shuffled
    								   # 分类型交叉验证，确保每个折叠中的各类样本比例一致，适用于处理类别不平衡的数据
for train_index, test_index in skfolds.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf)  # 复制了新的模型
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_fold = X_train[test_index]
    y_test_fold = y_train_5[test_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = np.sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))
```

`StratifiedKFold`类执行分层采样来生成包含每个类的比率的折叠。在每次迭代中，                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         代码都会创建分类器的克隆，在训练集上训练该克隆，并在测试集上进行预测。然后它计算正确预测的数量并输出正确预测的比率。

```python
# 虚构分类器，把所有分类归为 最频繁出现的类，在这种情况下是阴性类（即非5）
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier()
dummy_clf.fit(X_train, y_train_5)
print(np.any(dummy_clf.predict(X_train)))  # 验证 我的预测是不是全部都是阴性，全是阴性->False
```



>    虚构分类器精度超过90%！这仅仅是因为只有大约10%的图像是5，所以如果总是猜测图像不是5，那么在90%的情况下都是正确的。

所以精度通常不是分类器的首选性能指标，尤其是当你处理不平衡的数据集时（即，当某些类比其他类出现更频繁时）。评估分类器性能的更好方法是查看混淆矩阵(Confusion Matrix，CM)

混淆矩阵的一般思想是计算所有A/B对的A类实例被分类为B类的次数。例如，要了解分类器将8与0的图像混淆的次数，可以查看混淆矩阵的第8行、第0列。要计算混淆矩阵，首先需要有一组预测值，以便可以将它们与实际目标进行比较。可以对测试集进行预测，但最好暂时保持不变（一旦有一个准备好启动的分类器，只在项目的最后才使用测试集）。作为替代，可以使用cross_val_predict()函数：

```python
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
y_train_pred  # 每个测试折叠的预测 True/False
```

```python
# 混淆矩阵的numpy写法
# mat_00 = np.sum((y_train_pred == 0) & (y_train_5 == 0))
# mat_01 = np.sum((y_train_pred == 0) & (y_train_5 == 1))
# mat_10 = np.sum((y_train_pred == 1) & (y_train_5 == 0))
# mat_11 = np.sum((y_train_pred == 1) & (y_train_5 == 1))
# arr = np.array([[mat_00, mat_01],
#                 [mat_10, mat_11]])
# print(arr)
#  真实值   预测值	!=5   		==5
#  != 5				a			b
#  == 5				c			d				二元分类的混淆矩阵，可以拓展到多元分类
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_train_5, y_train_pred)
cm
```

## 分类项目的数学特征、指标

### 混淆矩阵

混淆矩阵中的每一行代表一个实际类，而每一列代表一个预测类。该矩阵的第一行考虑非5图像（阴性类）：其中52242张被正确分类为非5（称为真阴性），而其余2337张被错误分类为5（假阳性，也称为Ⅰ类错误）。第二行考虑5（阳性类）的图像：844张被错误分类为非5（假阴性，也称为Ⅱ类错误），而其余4577张被正确分类为5（真阳性）。一个完美的分类器只会有真阳性和真阴性，所以它的混淆矩阵只会在它的主对角线上（左上到右下）有非零值：

> 列索引是预测标签，行索引是真实标签

```python
# 随堂练习：展示一个完美分类预测的 混淆矩阵
y_pred_perfect = y_train_5
confusion_matrix(y_train_5, y_pred_perfect)
```

### 准确率与召回率

混淆矩阵提供了大量信息，但也需要更简洁的指标。一个指标是阳性预测的精度，这称为分类器的准确率：

$$ 准确率 = TP / (TP+FP) $$

`TP`是真阳性数，`FP`是假阳性数。

获得完美准确率的一种简单方法是创建一个始终做出阴性预测的分类器，只对它最有信心的一个实例做出阳性预测。如果这个预测是正确的，那么分类器就有100%的准确率（准确率=1/1=100%）。显然，这样的分类器不会很有用，因为它忽略了除一个阳性实例之外的所有实例。因此，准确率通常与另一个名为召回率的指标一起使用，召回率也称为灵敏度或真阳性率(True Positive Rate，`TPR`)：这是分类器正确检测到的阳性实例的比率

$$ 召回率 = TP / (TP + FN) $$

`FN`是假阴性的数量。

> 总而言之，准确率 = 真阳性/预测的总阳性
>
> 召回率 = 真阳性/真实总阳性

```python
# 快速问答：现在的准确率和召回率分别是多少？
# array([[52242,  2337],
#        [  844,  4577]], dtype=int64)   混淆矩阵
4577 / (2377+4577) ,4577 / (844+4577)  
```

```python
# Scikit-Learn提供了几个函数来计算分类器指标，包括准确率和召回率
# precision_score(算的是准确率）
# recall_score(算的是召回率）
from sklearn.metrics import precision_score, recall_score

precision_score(y_train_5, y_train_pred) # 4577 / (2377+4577)
recall_score(y_train_5, y_train_pred)   # 4577 / (844+4577)
```

将准确率和召回率组合成一个称为`F1`分数的指标通常很方便，尤其是需要一个指标来比较两个分类器时。F1分数是准确率和召回率的调和均值。常规均值对所有值一视同仁，而调和均值则为低值赋予更多权重。因此，如果召回率和准确率都很高，那么分类器会获得较高的F1分数
$$
F_1 = \frac{2}{\frac{1}{\text{精确率}} + \frac{1}{\text{召回率}}} = 2 \times \frac{\text{精确率} \times \text{召回率}}{\text{精确率} + \text{召回率}} = \frac{TP}{\frac{TP + FN + FP}{2}}
$$

```python
# F1 分数
from sklearn.metrics import f1_score
f1_score(y_train_5, y_train_pred)
```

`F1`分数有利于具有相似准确率和召回率的分类器。这并不总是想要的：在某些情况下主要关心准确率，而在其他情况下真正关心召回率。例如，如果训练了一个分类器来检测对孩子安全的视频，你可能更喜欢一个拒绝许多好的视频（低召回率）但只保留安全视频（高准确率）的分类器，而不是一个有很高的召回率，但会让一些非常糟糕的视频出现在你的产品中的分类器（在这种情况下，甚至可能想要添加一个人工流水线来检查分类器的视频选择）。另一种情况，假设训练了一个分类器来检测监控图像中的小偷：如果你的分类器只有30%的准确率，但是只要它有99%的召回率就可能没问题（当然，保安会收到一些误报，但几乎所有的小偷都会被抓住）。

现实不能两者兼顾：提高准确率会降低召回率，反之亦然。这称为准确率/召回率权衡。


 为了理解这种权衡，先看看`SGDClassifier`是如何做出分类决策的。对于每个实例，它都会根据决策函数计算分数。如果该分数大于阈值，则将实例分配给阳性类，否则分配给阴性类。下图展示了从左侧最低分到右侧最高分的几个数字。假设决策阈值位于中心箭头（两个5之间），你会在该阈值的右侧找到4个真阳性（实际上为5）和1个假阳性（实际上为6）。因此，使用该阈值，准确率为80%(4/5)。但是在6个实际5中，分类器只检测到4个，因此召回率为67%(4/6)。如果你提高阈值（将它移到右边的箭头），假阳性(6)变成真阴性，从而提高了准确率（在这种情况下高达100%），但是一个真阳性变成假阴性，召回率降低到了50%。相反，降低阈值会增加召回率并降低准确率。

![准确率召回率权衡](./images/sklearn_classification/p1.png)

`Scikit-Learn`不允许直接设置阈值，但可以访问用于预测的决策分数。可以调用它的decision_function()方法，而不是调用分类器的predict()方法，该方法返回每个实例的分数，然后根据这些分数使用你想要的任何阈值进行预测：

```python
import numpy as np
y_scores = sgd_clf.decision_function([some_digit])
y_scores

# z = np.dot(w, x) + b; sigmoid(z) -> 0.5  # z >=0  逻辑回归

print(np.isclose(y_scores, np.dot(sgd_clf.coef_, some_digit) + sgd_clf.intercept_))   # np.dot(w,x) + b  就是 decision_function在做的事情
#
threshold = 0  # 决策阈值
y_some_digit_pred = (y_scores > threshold)
```

```python
# 提高阈值，归为5的标准更加严格，这会降低召回率； 3500以下的也是非5
threshold = 3500
y_some_digit_pred = (y_scores > threshold)
y_some_digit_pred
```

```python
# 用cross_val_predict()函数获取训练集中所有实例的分数，指定要返回决策分数而不是预测：
from sklearn.metrics import precision_recall_curve
y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function")
y_scores
```

```python
# 算所有可能的阈值的准确率和召回率（该函数添加最后一个为0的准确率和最后一个为1的召回率，对应于负无穷阈值）
# 所有可能的阈值：训练集的所有分数可以从低到高排序，以每个分数为基准（阈值），低于阈值的是非5，高于的是5，计算准确率和召回率
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
precisions
recalls
thresholds
```

```python
plt.figure(figsize=(8, 4))
plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)  # c="b", linestyle="--"  准确率关于阈值的变化图，应该呈现的是上升趋势
plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)   # c="g", linestyle="-" 召回率关于阈值的变化图，一定呈现的是下降趋势
plt.vlines(threshold, 0, 1.0, "k", "dotted", label="threshold")   # 在threshold处画一个垂线

idx = (thresholds >= threshold).argmax()  # first index ≥ threshold
plt.plot(thresholds[idx], precisions[idx], c="b", marker="o")   # "bo" 可以合并
plt.plot(thresholds[idx], recalls[idx], c="g", marker="o")      # "go" 可以合并
plt.axis([np.min(thresholds), np.max(thresholds), 0, 1])
plt.grid()
plt.xlabel("Threshold")
plt.legend(loc="center right")
plt.show()
```

准确率曲线比召回率曲线更曲折。原因是当你提高阈值时，准确率有时会下降（尽管通常它会上升）。回顾之前的例子：阈值右移，准确率从4/5(80%)下降到3/4(75%)（分母和分子会同时下降）。另外，召回率只有在阈值增加时才会下降，这也解释了为什么它的曲线看起来很平滑。

```python
# 选择一个好的准确率/召回率权衡的另一种方法是直接绘制准确率与召回率的关系图
# 随堂练习：不看下面代码，画召回率-准确率关系图

# 召回率-准确率关系图
plt.plot(recalls, precisions, linewidth=2, label="Precision/Recall curve")
plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], c="black", linestyle=":")  # "k:"可以合并
plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], "k:")
plt.plot([recalls[idx]], [precisions[idx]], "ko",
         label="Point at threshold 3500")

plt.plot((0.66, 0.88), (0.80, 0.55), "k-")  # ^ 上三角形
plt.text(0.72, 0.55, "Higher\nthreshold", color="#333333")
plt.legend(loc="lower left")
plt.xlabel("recall")
plt.ylabel("precision")
plt.grid()
plt.show()
```

```python
# 假设你决定以90%的准确率为目标。可以使用上图来找到需要使用的阈值，
# 但看图不是很准确 -> 你可以搜索为你提供至少90%准确率的最低阈值
# 随堂练习：找到准确率至少90%的最低阈值

idx_for_90_precision = (precisions >= 0.9).argmax() #    >=0.9为True的最小索引
threshold_for_90_precision = thresholds[idx_for_90_precision]

# 进行预测不用predict，因为模型内部有自己的阈值
y_train_pred_90 = (y_scores > threshold_for_90_precision)

# 检查预测的准确率和召回率
precision_score(y_train_5, y_train_pred_90)
recall_score(y_train_5, y_train_pred_90)

# idx_for_90_precision = (precisions >= 0.9).argmax()
# threshold_for_90_precision = thresholds[idx_for_90_precision]
# threshold_for_90_precision
```

创建具有高准确率的分类器相当容易：只需设置足够高的阈值。但是这样会导致召回率太低，高准确率分类器就不是很有用！

如果有人要求你：“让我们达到99%的准确率。”你就应该问：“召回率是多少？”

受试者操作特征(Receiver Operating Characteristic，`ROC`)曲线是二元分类器使用的另一种常用工具。它与准确率/召回率曲线非常相似，但不是绘制准确率与召回率，`ROC`曲线绘制的是真阳性率（召回率的另一个名称）与假阳性率(False Positive Rate，`FPR`)。`FPR`（也称为fall-out）是被错误分类为阳性的阴性实例的比率。它等于1-真阴性率(True Negative Rate，`TNR`)，即被正确分类为阴性的阴性实例的比率。`TNR`也称为特异性。因此，`ROC`曲线绘制了灵敏度（召回率）与1-特异性。
$$
假阳性率(FPR) = \frac {真实值为False但却被认为为True的个数}{真实值为False的总个数}
$$

$$
真阳性率(TPR,召回率) = \frac {真实值为True且预测值也为True的个数}{真实值为True的总个数}
$$

$$
受试者操作特征(roc) =  \frac {TPR}{FPR}
$$




```python
from sklearn.metrics import roc_curve
# precision_recall_curve
fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)   # fpr: 错误分为阳性的阴性比率  1-真阴性率；   tpr：真阳性率，就是召回率

fpr
# tpr
```

```python
# 这里的阈值列表是降序排列
idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()  # 目标：还是锁定准确率90%的点   阈值按降序排列
tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]

# 绘制 FPR与TPR的对比图
plt.plot(fpr, tpr, linewidth=2, label="ROC curve")

# 试想以下纯随机猜测的混淆矩阵
# p     1-p
# p     1-p


plt.plot([0, 1], [0, 1], 'k:', label="Random classifier's ROC curve")  # 为什么随机分类器的ROC curve是一条直线？

plt.xlabel('False Positive Rate (Fall-Out)')
plt.ylabel('True Positive Rate (Recall)')
plt.grid()
plt.axis([0, 1, 0, 1])
plt.legend(loc="lower right", fontsize=13)
plt.show()
```

再来权衡：召回率（`TPR`)越高, 假阳性（`FPR`）越多，上图的虚线是纯随机分类器的`ROC`曲线，一个好的分类器要仅可能地远离这条线-> 贴近左上角（想象完美分类器，召回率是1，假阳性是0）

-> 贴近左上角的程度可以衡量分类器的好坏 -> 直接去测量曲线下面的面积，面积越接近1，性能越接近完美；面积越接近0.5，性能越不行 -> ==曲线下的面积指标名称：`AUC`(Area Under the Curve)==

```python
from sklearn.metrics import roc_auc_score
roc_auc_score(y_train_5, y_scores)		# 0.9614520950203463
```

由于`ROC`曲线与准确率/召回率(Precision/Recall，`PR`)曲线非常相似，如何决定使用哪一个。根据经验，只要阳性类很少见，或者当你更减少假阳性而不是假阴性时，应该更关注`PR`曲线。否则，使用`ROC`曲线。例如，之前的`ROC`曲线（和`ROC`` AUC`分数），可能会认为分类器很好。但这主要是因为与阴性（非5）相比，阳性(5)很少。相比之下，`PR`曲线清楚地表明分类器还有改进的空间：曲线实际上可以更靠近右上角


创建一个`RandomForestClassifier`，我们可以将其`PR`曲线和`F1`分数与`SGDClassifier`的那些进行比较

`precision_recall_curve`()函数需要每个实例的标签和分数，因此需要训练随机森林分类器并让它为每个实例分配一个分数。但是因为工作原理和逻辑回归不一样，`RandomForestClassifier`类没有`decision_function()`方法，但它有一个`predict_proba()`方法可以返回每个实例的类概率，可以只使用阳性类的概率作为分数

```python
# RandomForestClassifier 比较PR曲线和F1分数
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state=100)
y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method="predict_proba")

y_probas_forest[:2]
```

```python
y_scores_forest = y_probas_forest[:, 1] # 第二列包含了阳性的概率，所以可以直接把它当成分数，传给precision_recall_curve
precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(y_train_5, y_scores_forest)
```

```python
# 绘制随机森林和逻辑回归的 准确率召回率曲线（PR曲线）
plt.plot(recalls_forest, precisions_forest, "b-", linewidth=2,
         label="Random Forest")
plt.plot(recalls, precisions, "--", linewidth=2, label="SGD")

plt.xlabel("Recall")
plt.ylabel("Precision")
plt.axis([0, 1, 0, 1])
plt.grid()
plt.legend(loc="lower left")

plt.show()
```

```python
# RandomForest Classifier的PR曲线在更右上角 比SGD Classifer更好，
y_train_pred_forest = y_probas_forest[:,1] >= 0.5   # 高于50%概率的归为阳性

#
f1_score(y_train_5, y_train_pred_forest)             # 计算F1分数
roc_auc_score(y_train_5, y_probas_forest[:,1])       # 计算ROC AUC分数
precision_score(y_train_5, y_train_pred_forest)      # 计算准确率
recall_score(y_train_5, y_train_pred_forest)         # 计算召回率
```


- 总结：
1. 训练二元分类器
2. 合适的指标（类的预测/分数）
3. 交叉验证对验证集预测
4. 选择适合的准确率/召回率权衡，`ROC`曲线，`AUC`分数，`F1`分数等

## 多类分类


- 多类分类

多类分类器（也称为多项分类器）可以区分两个以上的类。

一些`Scikit-Learn`分类器（例如，`LogisticRegression`、`RandomForestClassifier`）能够类。其他的是严格的二元分类器（例如`SGDClassifier`(随机梯度下降)和`SVC`）。但是，可以使用多种策略通过多个二元分类器来执行多类分类。

创建一个可以将数字图像分为10类（从0到9）的系统的一种方法是训练10个二进制分类器，每个数字一个（数字0检测器，数字1检测器，数字2检测器，…，数字9检测器）。然后，当想要对图像进行分类时，可以从该图像的每个分类器中获得分数，然后选择分类器输出最高分数的类。这称为一对其余(One-Versus-the-Rest，`OvR`)策略，有时也称为一对全部(One-Versus-All，`OvA`)。

一种策略是为每对数字训练一个二元分类器：一个区分0和1，另一个区分0和2，另一个区分1和2，等等。这称为一对一(One-Versus-One，`OvO`)策略。如果有N个类，则需要训练N×(N-1)/2个分类器。对于`MNIST`问题，这意味着要训练45个二元分类器！当想要对图像进行分类时，必须通过所有45个分类器运行图像并查看哪个类得到最高分。`OvO`的主要优点是每个分类器只需要在训练集中包含它必须区分的两个类的部分上进行训练。

一些算法（例如支持向量机分类器）无法适应训练集的大小。对于这些算法，`OvO`是首选，因为在小训练集上训练许多分类器比在大训练集上训练几个分类器更快。然而，对于大多数二元分类算法，`OvR`是首选。

Scikit-Learn会检测何时尝试将二元分类算法用于多类分类任务，并根据算法自动运行`OvR`或`OvO`。使用sklearn.svm.SVC类用支持向量机分类器来试试这个。只训练前2000个图像，否则运行很久：

```python
from sklearn.svm import SVC

svm_clf = SVC(random_state=100)
svm_clf.fit(X_train[:2000], y_train[:2000]) # 注意这里 y_train已经是多分类了，不是y_train_5这种二元分类，超过了2个类，使用OVO策略训练了45个二元分类器
```

```python

svm_clf.predict([some_digit])
# svm_clf.decision_function_shape  # 这个只是decision function 返回的shape，默认多分类用OVO处理
```

```python
some_digit_scores = svm_clf.decision_function([some_digit])  # 返回10个类的分数：每个类别的分数等于得到最高分的次数 去加/减小调整
some_digit_scores.round(2)

class_id = some_digit_scores.argmax()
class_id

svm_clf.classes_   # 目标类列表存储在classes_属性中，按值排序。
svm_clf.classes_[class_id]  # 查找类标签
```

```python
# 强制使用一对一 或 一对其余
from sklearn.multiclass import OneVsRestClassifier

ovr_clf = OneVsRestClassifier(SVC(random_state=100)) # 基于SCV 只用一对其余策略创建多类分类器
ovr_clf.fit(X_train[:2000], y_train[:2000])
```

```python
ovr_clf.predict([some_digit])
ovr_clf.estimators_             # 这里有10SVC二元分类器
len(ovr_clf.estimators_)  # 检查分类器的数量
```

```python
#  警告：可能运行1分钟以上
# 训练一个多分类的 SGDClassifier
sgd_clf = SGDClassifier(random_state=100)
sgd_clf.fit(X_train, y_train)       # 1对其余，训练10个二元分类器
```

```python
a_digit = X_train[25]  # '2'
plot_digit(a_digit)
```

```python
sgd_clf.predict([a_digit]) # 一个预测不对的例子
sgd_clf.decision_function([a_digit]).round()  # 注意：这里的decision_function会为每个类打个分数，在2，8上的分数比较高
# cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")  # 每个类别中图像数量大致相同，因此可以用精度指标评估; 警告：代码要运行2-3分钟
```

```python
# 来个缩放预处理
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, FunctionTransformer

# 特征缩放后可以选择更快的梯度下降，找到的最优解也更好
float_64_convert = FunctionTransformer(lambda X: X.astype(np.float64))
sgd_clf_faster = SGDClassifier(random_state=100, learning_rate="invscaling", eta0=0.1)    # 开始的学习率是0.1， 学习率 = 0.1/迭代次数
scaling_sgd_clf = make_pipeline(float_64_convert, StandardScaler(), sgd_clf_faster)

cross_val_score(scaling_sgd_clf, X_train, y_train, cv=3, scoring="accuracy")    # 又快又好   快：特征缩放   好：特征缩放 + 更激进的学习率
```

真实项目流程复习：探索数据，尝试多个模型，列出最佳模型，使用GridSearchCV/随机搜索微调超参数， 假设找到了最佳模型，看如何通过错误分析去改进它

混淆矩阵现在是10*10，把它弄成彩图更容易分析 -> ConfusionMatrixDisplay.from_predictions

```python
from sklearn.metrics import ConfusionMatrixDisplay

y_train_pred = cross_val_predict(scaling_sgd_clf, X_train, y_train, cv=3)
ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)
plt.show()
```

```python
# 数据集上的5更少，还是5可能预测不对？ -> 按行归一化混淆矩阵

#  注意：normalize="true" 是字符串true, 意思是按真实标签来归一化
ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, normalize="true", values_format=".0%")  # values_format=".0%" 显示没有小数的百分比
plt.show()
```

```python
# 把0权重放在正确预测上, 重点强调分类错误,错误更突出
sample_weight = y_train_pred != y_train
ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, sample_weight=sample_weight, normalize="true", values_format=".0%")
plt.show()
```

```python
# 混淆矩阵按列归一化，normalize="pred", 按预测来归一化
ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, sample_weight=sample_weight, normalize="pred", values_format=".0%")
plt.show()

# 可以看出我们的分类器， 4/9, 7/9, 1/8容易分类错误
#                    3/5, 3/2, 5/6容易混淆
```

分析混淆矩阵的目的：深入了解改进分类器的方法，减少假"x", 为x收集更多数据，计算图片特定的特征数量以区分出来，或者考虑是不是要对图像进行预处理，以凸显某些特征


```python
# 深入了解分类器正在做什么，以及失败原因的好办法：分析单个错误

cl_a, cl_b = '4', '9'
X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]
X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]
X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]
X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]
```

分类器出错里的数字中（即在左下角和右上角的块中），有的写得非常糟糕，即使是人类也很难对它们进行分类。

然而，大多数错误分类的图像对我们来说似乎是明显的错误，那是因为人的视觉系统在任何信息到达我们的意识之前都会进行大量复杂的预处理。因此，这个错误人看起来明显，并不意味着对分类器来说是这样

使用的是一个简单的`SGDClassifier`分类，它只是一个线性模型：它所做的只是为每个像素分配每个类的权重，当它看到一个新图像时，它只是将加权像素强度相加以获得每个类的分数。由于数字3和数字5，数字1和数字8仅相差几个像素，因此该模型很容易混淆它们。


```python
# 两个数字图像 转成混淆矩阵的形式呈现
size = 5
pad = 0.2
plt.figure(figsize=(size, size))
for images, (label_col, label_row) in [(X_ba, (0, 0)), (X_bb, (1, 0)),
                                       (X_aa, (0, 1)), (X_ab, (1, 1))]:
    for idx, image_data in enumerate(images[:size*size]):
        x = idx % size + label_col * (size + pad)
        y = idx // size + label_row * (size + pad)
        plt.imshow(image_data.reshape(28, 28), cmap="binary",
                   extent=(x, x + 1, y, y + 1))
plt.xticks([size / 2, size + pad + size / 2], [str(cl_a), str(cl_b)])   # 指定两个刻度，对应两个字符串
plt.yticks([size / 2, size + pad + size / 2], [str(cl_b), str(cl_a)])
plt.plot([size + pad / 2, size + pad / 2], [0, 2 * size + pad], "k:")
plt.plot([0, 2 * size + pad], [size + pad / 2, size + pad / 2], "k:")
plt.axis([0, 2 * size + pad, 0, 2 * size + pad])
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()
```

数字3和数字5之间的主要区别是连接顶线和底弧的小线的位置。如果绘制一个数字3，并且连接点稍微向左移动，则分类器可能会将其分类为数字5，反之亦然。换句话说，这个分类器对图像的移动和旋转非常敏感。减少3/5混淆的一种方法是对图像进行预处理，以确保它们很好地居中并且不会过度旋转。然而，这较难实现，因为它需要预测每个图像的正确旋转。一种更简单的方法是使用训练图像的轻微移动和旋转变体来扩充训练集。这将迫使模型学会更能容忍这种变化。这称为数据增强



## 进阶：多标签分类，每个标签也肯能是多分类问题

### 单分类

- 多标签分类，为每个实例输出多个分类

在某些情况下，可能希望分类器为每个实例输出多个类。考虑一个人脸识别分类器：如果它在同一张照片中识别出多个人，它应该为它识别的每个人附加一个标签。假设分类器经过训练可以识别三张面孔：Alice、Bob和Charlie。然后，当向分类器显示Alice和Charlie的照片时，它应该输出[True，False，True]（意思是“是Alice，不是Bob，是Charlie”）。这种输出多个二进制标签的分类系统称为多标签分类系统。

```python
from sklearn.neighbors import KNeighborsClassifier

y_train_large = y_train >= '7'
y_train_odd = y_train.astype('int8') % 2 == 1
y_multilabel = np.c_[y_train_large, y_train_odd]  # 创建一个y_multilabel数组，数字图像对应 [是否是大数字，是否为奇数]

knn_clf = KNeighborsClassifier()  # K邻近分类器支持多标签分类
knn_clf.fit(X_train, y_multilabel)

knn_clf.predict([some_digit]) # 5
knn_clf.predict([a_digit])    # 2

# 评估多标签分类器的方式：为每个单独标签测量F1分数，然后计算F1分数在所有标签上的平均
y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)
f1_score(y_multilabel, y_train_knn_pred, average='macro')		# average='macro'：表示所有的标签都是同等重要的
```

这种方法假设所有标签都同等重要，但事实可能并非如此。特别是，如果拥有的Alice的照片比Bob或Charlie的照片多得多，可能希望对分类器在Alice照片上的得分赋予更多权重。一个简单的选项是为每个标签赋予与其支持度（即具有该目标标签的实例数）相等的权重。为此，只需在调用`f1_score()`函数时设置average="weighted"

`KNN`（K-Nearest Neighbors）用于多标签分类时，对每个标签单独投票，不会学到标签之间的依赖关系：对于每一个测试样本：

1. 找到最近的 k 个邻居。
2. 对这 k 个邻居的标签进行统计（例如 [1, 0, 1, 0, 0]）。
3. 对每一个标签维度分别做“多数投票”，决定是否给该标签打勾（设为1）或不打勾（设为0）。

这种策略很难捕获标签之间的依赖关系。例如，一个大数字（7、8或9）是奇数的可能性是偶数的两倍，但是“奇数”标签的分类器不知道“大”标签的分类器预测的是什么。为了解决这个问题，可以将模型组织成一个链：当一个模型进行预测时，它使用输入特征加上链中在它之前的模型的所有预测。

`sklearn`有`ClassifierChain`的类，它可以链接多个分类模型，根据每个模型在链中的位置为其提供适当的标签。`ClassifierChain`类设置了`cv`参数后，它将使用交叉验证从每个验证集中的每个实例获得 它们作为验证集时候的 预测，然后这些预测会作为特征 用于训练链中稍后的模型

```python
from sklearn.multioutput import ClassifierChain

chain_clf = ClassifierChain(SVC(), cv=3, random_state=100)

# self.estimators_ = [clone(self.base_estimator) for _ in range(Y.shape[1])]
# 拟合数据时，会根据标签y的数量 自动链接模型
# 模型预测时，它使用输入特征 加上链中在它之前的模型的所有预测
chain_clf.fit(X_train[:2000], y_multilabel[:2000])
chain_clf.predict([some_digit])			# array([[0., 1.]])
```

### 多输出分类

- 多输出分类

多标签分类的推广，其中每个标签可以是多类的（即它可以有两个以上的可能值）

例子：从图像中去除噪声，以嘈杂的数字图像作为输入，并且（希望）输出干净的数字图像，表示为像素强度数组

分类器的输出是多标签（每个像素一个标签），每个标签可以有多个值（像素强度范围从0到255）。因此，这是一个多输出分类系统的示例

注意：分类和回归的界限有时比较模糊，比如这个例子预测像素强度更类似于回归，而不是分类。而且多输出系统不限于分类任务，可以每个实例输出包含分类标签和值标签的多个标签

```python
# 多输出-多类分类 - 图像去噪声
np.random.seed(100)
noise = np.random.randint(0, 100, (len(X_train), 784))
X_train_mod = X_train + noise
noise = np.random.randint(0, 100, (len(X_test), 784))
X_test_mod = X_test + noise

y_train_mod = X_train
y_test_mod = X_test
```

```python
plt.subplot(1,2,1)
plot_digit(X_test_mod[0])
plt.subplot(1,2,2)
plot_digit(y_test_mod[0])
plt.show()
```

```python
# 训练分类器，并清洗图像。 同样可以用KNN，预测标签（预测每个像素点的像素）时，找到 k个最邻近特征 的像素值（像素分类），投票就行

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train_mod, y_train_mod)
clean_digit = knn_clf.predict([X_test_mod[0]])
plot_digit(clean_digit)
plt.show()

cross_val_score(forest_clf, X_train, y_train, cv=3, scoring="accuracy")		#array([0.96515, 0.96315, 0.9654 ])

```

```python
forest_clf.fit(X_train, y_train)
y_test_pred = forest_clf.predict(X_test)
```

```python
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_test_pred)   # 随机森林模型的分类精度是 0.9698
# np.sum(y_test_pred == y_test) / len(y_test_pred)  # numpy的方式计算精度
```

随堂测试

```python
# 随堂练习1： 为MNIST数据集构建分类器，使其能够达到97%以上的精度。 提示：KNeighborsClassifer适合，但要找到好的超参数值（对weights和n_neighbors进行网格搜索）
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
grid_can_shu = [
    {
        'weights':['uniform' , 'distance'],
        'n_neighbors':[3,5,8]
    }
]
knn_clf = KNeighborsClassifier()
grid_search = GridSearchCV(knn_clf, param_grid=grid_can_shu , cv=3, scoring= 'accuracy')
grid_search.fit(X_train, y_train)

grid_search.best_estimator_

grid_search.best_params_

best_line = grid_search.best_estimator_
best_line.fit(X_train, y_train)
y_pred = best_line.predict(X_test)
accuracy_score(y_test, y_pred)

```

```python
# 随堂练习2：编写一个函数，可以将MNIST图像在任何方向（左、右、上或下）移动一个像素。然后，对于训练集中的每个图像，创建四个移动副本（每个方向一个）并将它们添加到训练集。最后，在这个扩展的训练集上训练你最好的模型，并在测试集上测量它的精度。你应该观察到你的模型现在表现得更好了！这种人工增加训练集的技术称为数据增强或训练集扩展。

# TODO
# 测试一下scipy_img_shift的效果
from scipy.ndimage import shift as scipy_img_shift
ashift = scipy_img_shift(X_train[0].reshape((28,28)),shift = (0,1))          # shift = (y轴平移,x轴平移)
plot_digit(X_train[0])
plt.show()

plot_digit(ashift)
plt.show()

import numpy as np
X_train_al = np.repeat(X_train,5,axis=0)
def image_shift(X):
    X[0] = scipy_img_shift(X[0].reshape((28,28)) , shift = ( 1,0)).reshape((1,-1))
    X[1] = scipy_img_shift(X[1].reshape((28,28)) , shift = (-1,0)).reshape((1,-1))
    X[2] = scipy_img_shift(X[2].reshape((28,28)) , shift = (0,-1)).reshape((1,-1))
    X[3] = scipy_img_shift(X[3].reshape((28,28)) , shift = (0, 1)).reshape((1,-1))
    return
# X_train_al = image_shift(X_train[0])
for i in range( len(X_train_al) // 5):
    image_shift(X_train_al[i*5:(i+1)*5])
    pass


X_train_al.shape
y_train_al = np.repeat(y_train,5)
y_train_al = y_train_al.reshape(-1,1).flatten()
y_train_al.shape , X_train_al.shape

indices = np.random.permutation(len(y_train_al))
y_train_al = y_train_al[indices]
X_train_al = X_train_al[indices]
from sklearn.ensemble import RandomForestClassifier
forest_clf = RandomForestClassifier(random_state=100)
forest_clf.fit(X_train_al, y_train_al)

print(accuracy_score(forest_clf.predict(X_test), y_test))		# 0.9799

```



# 调用总结

```python
# 1. fetch_openml是机器学习库里在的数据，
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', as_frame=False, parser="auto")

# 2. 随机梯度下降线性分类器
from sklearn.linear_model import SGDClassifier          # 其实就是逻辑回归模型

# 3. 交叉验证分数
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

# 4. 分类型交叉验证
from sklearn.model_selection import StratifiedKFold
skfolds = StratifiedKFold(n_splits=3) 
for train_index, test_index in skfolds.split(X_train, y_train_5):
    pass;

# 5. 克隆
from sklearn.base import clone
clone_clf = clone(sgd_clf)  # 复制了新的模型

# 6. 交叉验证预测 训练器，数据集，标签，交叉验证次数
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)	# cv：交叉验证次数
y_train_pred.shape

# 7. 混淆矩阵，
from sklearn.metrics import confusion_matrix
confusion_matrix(真实标签,预测标签)

# 8. 准确率，召回率，f1分数
from sklearn.metrics import precision_score, recall_score, f1_score

# 9. 返回准确率，召回率，阈值
from sklearn.metrics import precision_recall_curve
precision_recall_curve(真实标签,预测标签概率)

# 10. 返回真阳性率tpr与假阳性率fpr和阈值threshold
from sklearn.metrics import roc_curve

# 11. 返回auc分数，基于roc
from sklearn.metrics import roc_auc_score

# 12. svc向量机
from sklearn.svm import SVC

# 13. 强制使用ovr进行分类预测
from sklearn.multiclass import OneVsRestClassifier

# 14. 随机森林分类器
from sklearn.ensemble import RandomForestClassifier

# 15. 混淆矩阵的进阶版，对应的是多分类问题
from sklearn.metrics import ConfusionMatrixDisplay
y_train_pred = cross_val_predict(scaling_sgd_clf, X_train, y_train, cv=3)
ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)

# 16. 临近值预测分类问题
from sklearn.neighbors import KNeighborsClassifier

# 17. 链接多个分类模型,他会根据标签先训来拿出一个模型，再将当作新的特征进行训练
from sklearn.multioutput import ClassifierChain

# 18. 会在原矩阵上在左侧增加1列
from sklearn.preprocessing import add_dummy_feature
X_b = add_dummy_feature(X,value = 1.0)
```







# 回归补充

## 回归话题补充

两种不同的训练模型的方式
- 通过闭式方程，直接计算出最拟合训练集的模型参数
- 迭代优化的方式：梯度下降和它的变体：批量梯度下降，小批量梯度下降以及随机梯度下降

多种正则化技巧
- 岭回归，Lasso回归和弹性网络回归

分类任务模型
- 针对多分类的`softmax`回归



### 闭式方程

$$
\hat{\theta} = (X^\top X)^{-1} X^\top y
$$

这个方程中：

- $\hat{\theta}$ 是使代价函数最小的 $\theta$ 值。
- $y$ 是包含 $y^{(1)}$ 到 $y^{(m)}$ 的目标值向量。



```python
# 生成假数据
import numpy as np
from matplotlib.lines import lineStyles
from pyexpat.model import XML_CQUANT_PLUS

np.random.seed(100)

m = 100
X = 2 * np.random.rand(m, 1)
y = 4 + 3 * X + np.random.randn(m, 1)
```

```python
# 用方程拟合
from sklearn.preprocessing import add_dummy_feature

X_b = add_dummy_feature(X,value = 1.0)
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
print(theta_best)
```

```python
# 做出预测
X_new = np.array([[0], [2]])
X_new_b = add_dummy_feature(X_new) # X_new里 增加一列1
y_predict = X_new_b @ theta_best
y_predict
```

```python
# 绘制预测结果
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
plt.plot(X_new, y_predict, "r-", label="Predictions")
# plt.plot(X, y, "b.")

plt.plot(X,y, color = 'blue',linestyle = '' , marker = '.')
# extra code – beautifies and saves Figure 4–2
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([0, 2, 0, 15])
plt.grid()
plt.legend(loc="upper left")

plt.show()
```

```python
# 使用sklearn做更简单了
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)       # 线性回归内部 他用的是方程算最优参数
lin_reg.intercept_, lin_reg.coef_
lin_reg.predict(X_new)
```

```python
# LinearRegression类基于 np.linalg.lstsq()函数，可以直接调用它
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```

```python
# 上面的函数计算的是 X的伪逆(np.linalg.pinv(), Moore-Penrose逆） 和 y的矩阵乘法
np.linalg.pinv(X_b) @ y
```

伪逆本身是使用被称为奇异值分解(Singular Value Decomposition，SVD)的标准矩阵分解技术来计算的，可以分解训练集矩阵X为三个矩阵UΣV⊤的乘积［numpy.linalg.svd()］。伪逆的计算公式为：X+=VΣ+U⊤。为了计算矩阵Σ+，该算法取Σ并将所有小于一个小阈值的值设置为零，然后将所有非零值替换成它们的倒数，最后把结果矩阵转置。这种方法比计算标准方程更有效，而且它可以很好地处理边缘情况：的确，标准方程可能没有解，如果矩阵X⊤X是不可逆的（即奇异的），例如如果m＜n或某些特征是多余的，但伪逆总是有定义的。



#### 计算复杂度

标准方程计算$X^⊤X$的逆，是一个(n+1)×(n+1)的矩阵（n是特征数量）。对这种矩阵求逆的计算复杂度通常为$O(n^{2.4}) $到$O(n^3) $之间，取决于具体实现。换句话说，如果将特征数量翻倍，那么计算时间将乘以大约$2^2.4 =5.3 $倍到$2^3=8$倍。 Scikit-Learn的LinearRegression类使用的SVD方法复杂度约为$O(n^2)$。如果你将特征数量加倍，那计算时间大约是原来的4倍。

特征数量比较大（例如100000）时，标准方程和SVD的计算将极其缓慢。好的一面是，相对于训练集中的实例数量[$O(m)$]来说，两个都是线性的，所以能够有效地处理大量的训练集，只要内存足够。同样，线性回归模型一经训练（不论是标准方程还是其他算法），预测就非常快速：因为计算复杂度相对于想要预测的实例数量和特征数量来说，都是线性的。换句话说，对两倍的实例（或者是两倍的特征）进行预测，大概需要两倍的时间。

### 梯度下降
- 更适合特征数量或者训练实例数量大到内存无法满足要求的场景。

- 要点复习：不同尺度的特征梯度下降速度不一样，应用梯度下降时，需要保证所有特征值的大小比例都差不多（`Sklearn`的`StandardScaler`类），否则收敛的时间会长很多
![有和没有特征缩放的梯度下降](./images/regression/p1.png)

左边的训练集特征1和特征2有相同的数值规模，而右边的训练集上，特征1的值比特征2要小得多。（因为特征1较小，需要比特征2有更大的变化才能影响代价函数，这也是被拉长的特征是更小的特征的原因）

```python
# 批量梯度下降算法快速实现
eta = 0.1 # 学习率
n_epochs = 1000
m = len(X_b)

np.random.seed(100)
theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    gradients = X_b.T @ (X_b @ theta - y) / m
    theta = theta - eta * gradients
    pass
theta
# array([[3.9848596],
#        [2.8623262]])
```

#### 随机梯度下降(Stochastic Gradient Descent)

批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以当训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，随机梯度下降每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。显然，这让算法变得快多了，因为每个迭代都只需要操作少量的数据。它也可以被用来训练海量的数据集，因为每次迭代只需要在内存中运行一个实例（SGD可以作为核外算法实现，大量数据无法一次载入内存，分批训练）。另外，由于算法的随机性质，它比批量梯度下降要不规则得多。代价函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。随着时间推移，代价函数最终会非常接近最小值，但是即使它到达了最小值，依旧会持续反弹，永远不会停止。一旦算法停止，最终的参数值会很好，但不是最优的。

![随机梯度下降的路线图](./images/regression/p2.png)

当代价函数非常不规则时，随机梯度下降其实可以帮助算法跳出局部最小值，所以相比批量梯度下降，它对找到全局最小值更有优势。因此，随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最小值。

要解决这个困境，有一个办法是逐步降低学习率。开始的步长比较大（这有助于快速进展和逃离局部最小值），然后越来越小，让算法尽量靠近全局最小值。这个过程叫作模拟退火，因为它类似于冶金时熔化的金属慢慢冷却的退火过程。确定每个迭代学习率的函数叫作学习率调度。如果学习率降得太快，可能会陷入局部最小值，甚至停留在走向最小值的半途中。如果学习率降得太慢，你需要很长时间才能跳到最小值附近，如果提早结束训练，可能只得到一个次优的解决方案。

学习率调度理解：eta = 1.0 / (alpha * (t + t0))； t0和alpha两个超参数需要提前确定； 或者是 按 eta = eta0 / (1+k*t)理解, eta0是初始学习率，k是衰减的速度

```python
# 有学习率调度的随机梯度下降实现

n_epochs = 50
t0, t1 = 5, 50

def learning_schedule(t):
    return t0 / (t + t1)

np.random.seed(100)
theta = np.random.randn(2,1)

for epoch in range(n_epochs):
    for iteration in range(m):
        random_index = np.random.randint(m)
        xi = X_b[random_index : random_index + 1]
        yi = y[random_index : random_index + 1]
        gradients = xi.T @ (xi @ theta - yi)  # 单个样本的梯度 不需要除以m
        eta = learning_schedule(epoch*m + iteration)
        theta = theta - eta * gradients
theta
# array([[3.98380275],
#        [2.87435455]])
```

用随机梯度下降时，训练实例必须独立且在同一个概率分布下(Independent and Identically Distributed，`IID`)，以确保平均而言将参数拉向全局最优值。确保这一点的一种简单方法是在训练过程中对实例进行随机混洗（例如，随机选择每个实例，或者在每个轮次开始时随机混洗训练集）。如果不对实例进行混洗（例如，如果实例按标签排序），那么`SGD`将首先针对一个标签进行优化，然后针对下一个标签进行优化，以此类推，并且它不会接近全局最小值。

要使用带有`Scikit-Learn`的随机梯度下降执行线性回归，可以使用`SGDRegressor`类，该类默认优化MSE代价函数。以下代码最多可运行1000个轮次(`max_iter`)，或者直到 连续100个轮次训练后，损失下降小于10-5(`tol`)为止(`n_iter_no_change`)。它使用默认的学习调度（与前一个使用的不同）以0.01(`eta0`)的学习率开始（`eta0`/迭代次数)。最后，它不使用任何正则化（`penalty=None`）

```python
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01, n_iter_no_change=100, random_state=100)
# sgd_reg.fit(X, y) # DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
sgd_reg.fit(X, y.ravel())

sgd_reg.intercept_, sgd_reg.coef_
```


#### 小批量梯度下降（Mini-Batch gradient descent)



最后一个梯度下降算法变体称为小批量梯度下降。了解了批量和随机梯度下降后，就很容易理解它：在每一步中，不是根据完整的训练集（如批量GD）或仅基于一个实例（如随机GD）来计算梯度，小批量GD在称为小型批量的随机实例集上计算梯度。小批量GD优于随机GD的主要优点是，可以通过矩阵操作的硬件优化来提高性能，特别是在使用GPU时。

与随机GD相比，该算法在参数空间上的进展更稳定，尤其是在相当大的小批次中。小批量GD最终将比随机GD走得更接近最小值，但它可能很难摆脱局部最小值（在受局部最小值影响的情况下，不像有MSE代价函数的线性回归）。下图展示了训练期间参数空间中三种梯度下降算法所采用的路径。它们最终都接近最小值，但是批量GD的路径实际上是在最小值处停止，而随机GD和小批量GD都继续走动。但是，要记住批量GD每步需要花费很多时间，如果使用了良好的学习率调度，则随机GD和小批量GD也会达到最小值。

![不同梯度下降在参数空间中的路径](./images/regression/p3.png)


- 线性回归算法的比较

![线性回归的算法比较](./images/regression/p4.png)


### 多项式回归

```python
np.random.seed(100)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)  # 0.5*x^2 + x + 2
```

```python
plt.figure(figsize=(6, 4))
plt.plot(X, y, "b.")  # c="b", linstyle=”“， marker=”."   # b+""+"."
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.axis([-3, 3, 0, 10])
plt.grid()
plt.show()
```

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)  # include_bias=False 不包括1

# PolynomialFeatures 特征可以转一下，转成一个带多项式的特征， degree=2表示最高到二次项
X_poly = poly_features.fit_transform(X)
X_poly.shape
X[0]
X_poly[0] # X_poly包含X的原始特征以及该特征的平方from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)  # include_bias=False 不包括1

# PolynomialFeatures 特征可以转一下，转成一个带多项式的特征， degree=2表示最高到二次项
X_poly = poly_features.fit_transform(X)
X_poly.shape
X[0]
X_poly[0] # X_poly包含X的原始特征以及该特征的平方
```

```python
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)  # 内部会默认算偏置的权重
lin_reg.intercept_, lin_reg.coef_
```

```python
# 随堂练习，利用lin_reg的预测能力 和 poly_features的组合特征的能力，生成抛物线
x_test = np.linspace(-3,3,100)
y_pred = lin_reg.predict(poly_features.transform(x_test.reshape(-1, 1)))

plt.plot(x_test, y_pred, "r-", label="Predictions")
plt.plot(X, y, "b.")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.legend()
plt.grid()
plt.show()
```

-注意：`PolynomialFeatures`可以将特征的所有组合添加到给定的多项式阶数。例如特征a和b，degree=3,会有ab，a^2 * b, ab^2. `PolynomialFeatures(degree=d)`可以将一个包含n个特征的数组转换为包含(n+d)!/(d!n!)个特征的数组, 要小心特征组合的数量爆炸



### 学习曲线
所有`sklearn`估计器都可以使用fit()方法进行训练，有一些估计器还有partial_fit()方法，可以调用该方法在一个或多个实例上运行单轮训练。

反复调用partial_fit()会逐渐训练模型。可能同时有一个warm_start=True的超参数，如果warm_start=True,则在训练模型调用fit()方法将不会重置模型，它会在停止的地方继续训练。

sklearn的学习曲线可以得到模型的训练误差和验证误差随训练集大小变化的图（类似于反复调用partial_fit(), 每次评估一次训练和验证误差），如果不支持partial_fit()或warm_start, 则必须在训练集逐渐变大的子集上多次训练它。

sklearn的learning_curve函数返回它评估模型的训练集大小，以及它为每个大小和每个交叉验证测量的训练和验证分数

```python
from sklearn.model_selection import learning_curve  # 导入学习曲线函数
from sklearn.linear_model import LinearRegression
train_sizes, train_scores, valid_scores = learning_curve(LinearRegression(), X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5, scoring="neg_mean_squared_error")  # train_sizes 对应 40个 训练集的比例， 最小0.01， 最大1

# scoring="neg_mean_squared_error" 交叉验证希望分数越高越好，所以是 负的损失
train_sizes.shape
train_scores.shape  # 40 * 5, 因为有5次交叉验证
valid_scores.shape  # 40 * 5, 因为有5次交叉验证

train_errors = -train_scores.mean(axis=1)  # 40个不同训练集的评估结果， 去取平均数（平均5次验证）
valid_errors = -valid_scores.mean(axis=1)  #  # 40个不同训练集的评估结果， 去取平均数（平均5次验证）

plt.figure(figsize=(6, 4))
plt.plot(train_sizes, train_errors, c="r", linewidth=2, label="train", linestyle="-", marker="+")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")

plt.xlabel("Training set size")
plt.ylabel("RMSE")
plt.grid()
plt.legend(loc="upper right")
plt.axis([0, 80, 0, 4])
plt.show()
```

这是个典型的欠拟合learning curve：训练集只有1-2个实例，模型完全贴合，红色曲线从0开始，但是随着将新实例添加到训练集中，模型不能完美拟合数据，训练误差一致上升，直到达到平稳状态

验证误差：很多训练集时，无法正确泛化，一开始验证误差很大，随着样例变多，验证误差逐渐降低，但误差没降低多少已经趋于平稳，接近训练的误差曲线。

-> 典型的欠拟合状态：两条曲线趋于平稳，很接近，很高。

```python
# 随堂练习：生成相同数据上10阶多项式模型的学习曲线（如下图） 提示：可以把Polynomial feature和LinearRegression组成一个流水线
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
# X(只有一列/只有一个特征） -> [X, X^2, X^3, .... X^10]

a_pipeline = make_pipeline(PolynomialFeatures(10, include_bias=False), LinearRegression())

train_sizes, train_scores, valid_scores = learning_curve(a_pipeline, X, y, train_sizes=np.linspace(0.01, 1.0, 40), cv=5, scoring="neg_mean_squared_error")  # train_sizes 对应 40个 训练集的比例， 最小0.01， 最大1

train_errors = -train_scores.mean(axis=1)  # 40个不同训练集的评估结果， 去取平均数（平均5次验证）
valid_errors = -valid_scores.mean(axis=1)  #  # 40个不同训练集的评估结果， 去取平均数（平均5次验证）

plt.figure(figsize=(6, 4))
plt.plot(train_sizes, train_errors, c="r", linewidth=2, label="train", linestyle="-", marker="+")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="valid")

plt.xlabel("Training set size")
plt.ylabel("RMSE")
plt.grid()
plt.legend(loc="upper right")
plt.axis([0, 80, 0, 4])
plt.show()
```

这个学习曲线和之前的区别：

- 和之前比，训练数据上的误差要低得多
- 曲线之间存在间隙。这意味着该模型在训练数据上的性能要比在验证数据上的性能好得多，这是过拟合的标志。但是，如果使用更大的训练集，则两条曲线会继续接近




**理论：偏差/方差权衡**

- 偏差/方差权衡统计学和机器学习的重要理论成果是以下一个事实：模型的泛化误差可以表示为三个非常不同的误差之和： 偏差 + 方差 + 不可避免的误差 == 模型的预测误差
- 偏差：这部分泛化误差的原因在于错误的假设，比如假设数据是线性的，而实际上是二次的。高偏差模型最有可能对训练数据欠拟合。 （高偏差 是 欠拟合的另一个表达）
- 方差：这部分是由于模型对训练数据的细微变化过于敏感。具有许多自由度的模型（例如高阶多项式模型）可能具有较高的方差，因此可能过拟合训练数据。 （高方差 是 过拟合的另一个表达）
- 不可避免的误差：这部分误差是因为数据本身的噪声所致。减少这部分误差的唯一方法就是清洗数据（例如修复数据源（如损坏的传感器），或者检测并移除异常值）。

增加模型的复杂度通常会显著提升模型的方差，减少偏差。反过来，降低模型的复杂度则会提升模型的偏差，降低方差。这就是为什么称其为 方差-偏差权衡。


### 正则化线性模型

讨论的是通过约束模型的权重实现的正则化，岭回归（Ridge，L2正则化）， 套索回归（Lasso，L1正则化），弹性网络回归（Elastic Net，L1+L2正则化）



#### 岭回归

岭回归（L2正则化）是线性回归的正则化版本：将等于$\frac{\alpha}{m} \sum_{i=1}^{n} \theta_i^2$的正则化项添加到MSE。这迫使学习算法不仅拟合数据，而且还使模型权重尽可能地小。请注意，仅在训练期间将正则化项添加到代价函数中。训练模型后，你希望使用非正则化MSE（或RMSE）来评估模型的性能。

超参数α控制要对模型进行正则化的程度。如果α=0，则岭回归仅是线性回归。如果α非常大，则所有权重最终都非常接近于零，结果是一条经过数据均值的平线。
$$
J(\boldsymbol{\theta}) = \text{MSE}(\boldsymbol{\theta}) + \frac{\alpha}{m} \sum_{i=1}^{n} \theta_i^2
$$

请注意，偏置项 $\theta_0$ 没有进行正则化（总和从 $i=1$ 开始，而不是 0）。
如果我们将 $\mathbf{w}$ 定义为特征权重的向量（$\theta_1$ 至 $\theta_n$），则正则项等于 $\alpha \|\mathbf{w}\|_2^2$，其中 $\|\mathbf{w}\|_2$ 表示权重向量的 $\ell_2$ 范数:   向量的l2范数算出来是个数字： 向量的分量的平方和开根号

对于梯度下降，只需将 $\frac{2\alpha}{m} \mathbf{w}$ 添加到 MSE 梯度向量中对应于特征权重的部分，而不添加任何偏差项的梯度。

在执行岭回归之前缩放数据（例如使用StandardScaler）很重要，因为它对输入特征的缩放敏感。大多数正则化模型都需要如此。

```python
# alpha的增加会导致更平坦的预测，从而减少了模型的方差，但增加了其偏差
import numpy as np
np.random.seed(100)
m = 20
X = 3 * np.random.rand(m, 1)
y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5
X_new = np.linspace(0, 3, 100).reshape(100, 1)
```

```python
plt.figure(figsize=(6, 4))
plt.plot(X, y, ".")
plt.xlabel("$x_1$")
plt.ylabel("$y$  ", rotation=0)
plt.axis([0, 3, 0, 3.5])
plt.grid()
plt.show()
```

```python
def foo(aaa , bbb):
    return aaa * bbb

foo(aaa=3, bbb=5)
foo(**{"aaa":3, "bbb":5})  # 这一行的代码 和 上一行的效果 foo(aaa=3, bbb=5)
```

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# 线性回归要正则化的话

from sklearn.linear_model import Ridge

X_new = np.linspace(0, 3, 100).reshape(100, 1)
def plot_model(model_class, polynomial, alphas, **model_kwargs):
    # model_class: 某一个模型类（有fit方法的估计器）
    # polynomial：布尔值，True用多项式模型，False直接线性模型
    # alphas：小数列表，里面的每个数，都代表一个正则化的强度
    # **model_kwargs: model_kwargs本身字典， **字典放到函数里调用，等于会把 字典的键和值摊开，当成关键字参数
    #                 模型本身的超参数

    plt.plot(X, y, "b.", linewidth=3)
    for alpha, style in zip(alphas, ("b:", "g--", "r-")):
        if alpha > 0:
            # 如果alpha > 0, 这里的model_class就是 Ridge， 然后就是  实例化 不同 正则化强度alpha的 岭回归
            model = model_class(alpha=alpha, **model_kwargs)
        else:
            model = LinearRegression()

        if polynomial:
            model = make_pipeline(
                PolynomialFeatures(degree=10, include_bias=False),
                StandardScaler(),
                model)
        model.fit(X, y.ravel())
        y_new_regul = model.predict(X_new)
        plt.plot(X_new, y_new_regul, style, linewidth=2,
                 label=fr"$\alpha = {alpha}$")
    plt.legend(loc="upper left")
    plt.xlabel("$x_1$")
    plt.axis([0, 3, 0, 3.5])
    plt.grid()

plt.figure(figsize=(9, 3.5))
plt.subplot(121)   # plt.subplot(121), 1行2列的第一张图
plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=100)
plt.ylabel("$y$  ", rotation=0)


plt.subplot(122)   # plt.subplot(122), 1行2列的第二张图
plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=100)
plt.show()
```

alpha增加会导致更平坦的预测，从而减少了模型的方差，但增加了其偏差。

岭回归可以通过计算闭合形式的方程 或执行梯度下降执行， 闭式解公式：$
\hat{\boldsymbol{\theta}} = (X^\top X + \alpha A)^{-1} X^\top \mathbf{y}
$

```python
# 岭回归可以通过计算闭合形式的方程 或执行梯度下降执行
from sklearn.linear_model import SGDRegressor

# 通过Ridge类 （Ridege估计器）去训练 正则化的回归模型
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=0.1, solver="cholesky")  # solver="cholesky" 闭式解公式的变体
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])


# 通过随机梯度下降，去训练正则化的回归模型
# penalty="l2", alpha=0.1/m -> 添加正则项 alpha * l2范数的平方，这里 0.1/m，/m要手动添加，获得和Ridge(alpha=0.1)一样的效果
#  tol=None表示不会提前停止，跑到max_iter规定的次数
sgd_reg = SGDRegressor(penalty="l2", alpha=0.1/m, tol=None, max_iter=1000, eta0=0.1, random_state=100)
sgd_reg.fit(X, y.ravel())
sgd_reg.predict([[1.5]])
```

`RidgeCV`类也执行岭回归，但它使用交叉验证自动调整超参数。它大致相当于使用`GridSearchCV`，但它针对岭回归进行了优化并且运行速度更快。其他几个估计器（主要是线性的）也有高效的`CV`变体，例如`LassoCV`和`ElasticNetCV`。

#### Lasso回归
它和岭回归类似，在代价函数上增加了一个正则化项，但它使用向量的`l1`范数而不是`l2`范数的平方，注意`l1`范数乘以`2a`，而`l2`范数在岭回归中乘以`a/m`。不同的范数选择不同系数的目的是确保选择最佳的`a`可以独立于训练集大小

为什么`l2`正则化的 正则化强度`/m`，但是`l1`正则化的 正则化强度 不用除以`m`？

为了一个目的：我在选择`alpha`的时候，可以不考虑`m` （训练集的大小）

向量的`l1`范数：数字 == 向量分量的绝对值的（1次方）的和


$$
J(\theta)=\text{MSE}(\theta)+2\alpha\sum_{i=1}^{n}\lvert\theta_i\rvert
$$




- Lasso回归和岭回归的相似性：都可以限制权重大小，进而降低过拟合 （直接记）

$$
w^{(t+1)} \;=\; w^{(t)} \;-\; \eta\,\alpha\,\operatorname{sign}\!\bigl(w^{(t)}\bigr)
$$

其中
$
\operatorname{sign}(w_t)=
\begin{cases}
-1, & w_t < 0\\
[-1,1], & w_t = 0\\
+1, & w_t > 0
\end{cases}
$

上述公式省掉了 均方误差函数的梯度，重点体现权重的衰减： W是正数，微小幅度往下降；W是负数，微小幅度往上升；W是0，可以保持不变

Lasso代价函数在权重为0处是不可微(不可导）的，但是用任何围绕该点的梯度值之间的中间值去替代，梯度下降仍然可以正常工作


- Lasso回归和岭回归的不同：岭回归倾向把权重压得接近0，Lasso回归倾向于完全消除掉不重要特征得权重。换句话说，Lasso回归自动执行特征选择并输出具有很少非零特征权重的系数模型 （直接记）

解释：L1正则化的损失函数的梯度 = 0 在权重是0的时候更容易被满足

数学细节（了解）：


1. 设定

记

* 训练样本 $X\in\mathbb R^{n\times p}$，目标 $y\in\mathbb R^{n}$
* 参数向量 $w\in\mathbb R^{p}$

目标函数（把 $1/2n$ 写进平方误差，只为符号简洁）

$$
J(w)
= \frac1{2n}\,\|\,y-Xw\|_2^{2} \;+\; \lambda\,\|w\|_1
\tag{1}
$$

其中 $\lambda>0$ 控制正则强度。



2. 可微部分的梯度

平方损失是可微的：

$$
\nabla_{w}\frac1{2n}\,\|\,y-Xw\|_2^{2}
= -\frac1n X^{\!\top}(y-Xw)
=: g(w)\in\mathbb R^{p}.
\tag{2}
$$

（也常写成 $g(w)=\frac1n X^{\!\top}Xw-\frac1n X^{\!\top}y$。）



3. 不可微部分的**次梯度**

L1 范数在 $w_j=0$ 处不可导，用**次梯度 (sub-gradient)**：

$$
\partial|w_j|=
\begin{cases}
\{\;+\!1\;\}, & w_j>0 \\[2pt]
[-1,+1], & w_j=0 \\[2pt]
\{\;-1\;\}, & w_j<0
\end{cases}
\tag{3}
$$

整体写成向量形式：$\partial\|w\|_1 = \{\,s\in\mathbb R^{p} \mid s_j\in\partial|w_j|\,\}$.



4. 最优条件：**0 = 梯度 + λ·次梯度**

$$
\mathbf 0 \;\in\;
g(w) \;+\; \lambda\,\partial\|w\|_1
\tag{4}
$$

把它拆成每一维 $j$：

$$
0\;\in\;g_j(w) + \lambda\cdot \mathrm{sign}(w)_j,
\qquad
\text{其中 } \mathrm{sign}(0)\equiv[-1,1].
\tag{5}
$$



5. 逐坐标解析：产生“0”与“软阈值”

设残差 $r = y - Xw$。由 (2):

$$
g_j(w) \;=\; -\frac1n \boldsymbol x_j^{\!\top} r
$$

其中 $\boldsymbol x_j$ 是第 $j$ 列特征向量。



5.1 若 $w_j\neq0$

则 $\mathrm{sign}(w_j)$ 只可能是 ±1，(5) 变为

$$
-\frac1n\boldsymbol x_j^{\!\top}r + \lambda\,\mathrm{sign}(w_j)
 = 0
\;\;\Longrightarrow\;\;
\bigl|\boldsymbol x_j^{\!\top}r\bigr|/n = \lambda .
\tag{6}
$$

也就是说 **非零权重必须让该特征与残差的相关度** 精准等于 $\lambda$。



5.2 若 $w_j=0$

此时 $\mathrm{sign}(0)=[-1,1]$，条件 (5) 允许

$$
\left|\,\frac1n \boldsymbol x_j^{\!\top} r \right|
\;\le\;\lambda .
\tag{7}
$$

> ↳ **宽松得多**：只要相关度落进 $[-λ,λ]$，就满足最优条件，
> 所以“停在 0”最容易。

```python
from sklearn.linear_model import Lasso  # Lasso: L1正则化的线性回归类

lasso_reg = Lasso(alpha=0.1)  # L1正则化的线性回归实例，正则化强度是0.1
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
```

```python
# 随堂练习：注意上述可以改用 SGDRegressor(penalty="l1", alpha=0.1)
sgd_l1_reg = SGDRegressor(penalty="l1", alpha=0.2, max_iter=10000, tol=None, random_state=100, eta0=0.2)
sgd_l1_reg.fit(X,y.ravel())
sgd_l1_reg.predict([[1.5]])

# plt.figure(figsize=(9, 3.5))
# plt.subplot(1,2,1)
# plot_model(SGDRegressor, polynomial=False, alphas=(0, 0.2, 2), random_state=100, penalty="l1")
# plt.ylabel("$y$  ", rotation=0)
# plt.subplot(1,2,2)
# plot_model(SGDRegressor, polynomial=True, alphas=(0, 2e-2, 2), random_state=100, penalty="l1")   # a=0.01时 已经看起来像个二次曲线了，体现Lasso回归自动特征选择，并输出很少非0特征权重
# plt.show()

```

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(9, 3.5))
plt.subplot(1,2,1)
plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=100)
plt.ylabel("$y$  ", rotation=0)
plt.subplot(1,2,2)
plot_model(Lasso, polynomial=True, alphas=(0, 1e-2, 1), random_state=100)   # a=0.01时 已经看起来像个二次曲线了，体现Lasso回归自动特征选择，并输出很少非0特征权重
plt.show()
```

为了防止在使用Lasso时梯度下降最终在最优解附近反弹，需要降低训练期间的学习率，虽然降低仍会反弹，但反弹幅度不大，学习率 乘以 梯度会越来越小，最终收敛

#### 弹性网络回归
弹性网络回归是岭回归和Lasso回归之间的中间地带。它的正则化项是岭和Lasso正则化项的加权和。可以控制混合比r，当r=0时，弹性网络相当于岭回归，当r=1时，相当于Lasso回归
$$J(\boldsymbol{\theta}) \;=\; \mathrm{MSE}(\boldsymbol{\theta}) \;+\; r\!\bigl(2\alpha \sum_{i=1}^{n} \lvert \theta_i \rvert \bigr) \;+\; (1-r)\!\left(\frac{\alpha}{m}\sum_{i=1}^{n} \theta_i^{2}\right)$$



什么时候应该使用弹性网络、岭、Lasso，或者普通的线性回归（即不进行任何正则化）呢？(记忆)

通常来说，有正则化（哪怕是很小），总是比没有更可取一些。所以大多数情况下，应该避免使用纯线性回归。岭回归是个不错的默认选择，但是如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso回归或是弹性网络，因为它们会将无用特征的权重降为零，如前所述。一般而言，弹性网络优于Lasso回归，因为当特征数量超过训练实例数量，又或者是几个特征强相关时，Lasso回归的表现可能非常不稳定。

```python
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV   # 这些正则化回归，它们有自己的 交叉验证，找最优超参数的方式

elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.1)  # alpha=0.1对应公式里的 α, l1_ratio对应公式里的r
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
```


### 早停
对于梯度下降这一类迭代学习的算法，还有一个与众不同的正则化方法，就是在验证误差达到最小值时停止训练，该方法叫作早停法。下图展示了一个用批量梯度下降训练的复杂模型（高阶多项式回归模型），该模型正在我们之前使用的二次数据集上使用批量梯度下降进行训练。经过一轮一轮的训练，算法不断地学习，训练集上的预测误差(RMSE)自然不断下降，其在验证集上的预测误差也随之下降。但是，一段时间之后，验证误差停止下降反而开始回升。这说明模型开始过拟合训练数据。通过早停法，一旦验证误差达到最小值就立刻停止训练。这是一个非常简单而有效的正则化技巧

下面首先添加多项式特征并缩放所有输入特征，包括训练集和验证集（代码假定你已将原始训练集拆分为较小的训练集和验证集）。然后它创建一个没有正则化和小学习率的SGDRegressor模型。在训练循环中，它调用partial_fit()而不是fit()来执行增量学习。在每个轮次中，它测量验证集上的RMSE。如果它低于目前看到的最低RMSE，它会在best_model变量中保存模型的副本。此实现实际上并没有停止训练，但它可以在训练后恢复到最佳模型。注意，模型是使用copy.deepcopy()复制的，因为它同时复制了模型的超参数和学习到的参数（模型内部的属性）。相反，sklearn.base.clone()只复制模型的超参数。

#### 从numpy上实现早停

```python
import numpy as np
from copy import deepcopy
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDRegressor
import matplotlib.pyplot as plt

np.random.seed(42)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m, 1)
X_train, y_train = X[: m // 2], y[: m // 2, 0]
X_valid, y_valid = X[m // 2 :], y[m // 2 :, 0]

preprocessing = make_pipeline(PolynomialFeatures(degree=90, include_bias=False),
                              StandardScaler())
X_train_prep = preprocessing.fit_transform(X_train)
X_valid_prep = preprocessing.transform(X_valid)


sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42)
n_epochs = 500


best_valid_rmse = float('inf')  # 现在开始训练之前，全局最优的 验证集误差，应该设置成 正无穷大，后面比较才可能替换掉这个值

train_errors, val_errors = [], []  # 训练误差 和 验证误差

for epoch in range(n_epochs):

    sgd_reg.partial_fit(X_train_prep, y_train)

    y_valid_predict = sgd_reg.predict(X_valid_prep)  # 得到在验证集上的预测值
    val_error = mean_squared_error(y_valid, y_valid_predict, squared=False)  # 评估在验证集上的误差

    # 早停的核心逻辑
    if val_error < best_valid_rmse:
        best_valid_rmse = val_error
        best_model = deepcopy(sgd_reg)  # 通过深拷贝，当前的 sgd_reg,保存最优模型

    # 每个轮次计算训练误差 和 验证误差
    y_train_predict = sgd_reg.predict(X_train_prep)
    train_error = mean_squared_error(y_train, y_train_predict, squared=False)
    val_errors.append(val_error)
    train_errors.append(train_error)


best_epoch = np.argmin(val_errors)
plt.figure(figsize=(6, 4))

# 在图里 画注释
plt.annotate('Best model',
             xy=(best_epoch, best_valid_rmse),
             xytext=(best_epoch, best_valid_rmse + 0.5),
             ha="center",   # 水平方向对齐方式
             arrowprops=dict(facecolor='black', shrink=0.05))   # arrowprops 字典中的一个键，用来设置箭头两端的“收缩比例”，即让箭头稍微不完全连接起点 xy 和终点 xytext

plt.plot([0, n_epochs], [best_valid_rmse, best_valid_rmse], "k:", linewidth=2)  # 横线
plt.plot(val_errors, "b-", linewidth=3, label="Validation set")                 # 验证误差折线
plt.plot(best_epoch, best_valid_rmse, "bo")                                     # 最佳验证误差的点
plt.plot(train_errors, "r--", linewidth=2, label="Training set")                # 训练误差折线
plt.legend(loc="upper right")
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.axis([0, n_epochs, 0, 3.5])
plt.grid()
plt.show()
```

```python
# validation_fraction=0.1：会自动将训练数据中划出 10% 作为验证集。
# 在每一次迭代（或每一轮 boosting）之后，模型会用验证集评估指标（比如 均方误差）。
# 如果连续 n_iter_no_change 次迭代，验证集上指标没有显著提升（由 tol 决定），就会触发提前停止。

# 随堂练习：早停策略用sklearn实现， 关注超参数SGDRegressor的超参数：
# early_stopping（布尔值，是否要早停）,validation_fraction（数字，验证集的比例）, n_iter_no_change(正数）,

# sgd_reg = SGDRegressor(penalty=None, eta0=0.002, random_state=42, validation_fraction=0.5, early_stopping=True, tol=1e-5, n_iter_no_change=10, max_iter=10000)

sgd_reg = SGDRegressor(penalty=None, eta0=0.002, max_iter=100000, tol=None)
sgd_reg.fit(X, y.ravel())
sgd_reg.n_iter_   #   通过 n_iter_属性 ，知道实际进行了多个轮次的训练
```




### 逻辑回归

逻辑回归的前提：假设实例的特征 围绕其所属类的均值 服从高斯分布；最小化损失函数是在最大化 实例特征在自己类的出现概率

应用在鸢尾花数据集：一个非常著名的数据集，共有150朵鸢尾花，分别来自三个不同品种：`Setosa`鸢尾花、`Versicolor`鸢尾花和`Virginica`鸢尾花，数据里包含花的萼片以及花瓣的长度和宽度

下面示例仅根据花瓣宽度特征来检测`Virginica`鸢尾花类型

```python
from sklearn.datasets import load_iris

iris = load_iris(as_frame=True)
type(iris) # sklearn.utils._bunch.Bunch, 可以看成字典，（变量名.键， 去读取键对应的值）
list(iris)
iris.data.head(3)
iris.target
iris.target_names
```

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = iris.data[["petal width (cm)"]].values
y = iris.target_names[iris.target] == 'virginica'  # 0/1二元分类

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
log_reg = LogisticRegression(random_state=42)  # 也可以用l1/l2正则化， C控制正则化强度，值越高，正则化越少
log_reg.fit(X_train, y_train)

print(log_reg.coef_)
```

```python
#  exp(w*x+b)

X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
y_proba = log_reg.predict_proba(X_new)  # 逻辑回归最后预测出来是概率，所以逻辑回归内部 predict_proba
decision_boundary = X_new[y_proba[:,1] >= 0.5][0,0]  # 决策边界是一个维度的， 就是一个数字  （因为只有一个特征0

plt.figure(figsize=(8, 3))  # extra code – not needed, just formatting
plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2,
         label="Not Iris virginica proba")
plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica proba")
plt.plot([decision_boundary, decision_boundary], [0, 1], "k:", linewidth=2,
         label="Decision boundary")
#
plt.plot(X_train[y_train == 0], y_train[y_train == 0], "bs")   # marker="s"  方形
plt.plot(X_train[y_train == 1], y_train[y_train == 1], "g^")   # marker="^"  三角形
plt.xlabel("Petal width (cm)")
plt.ylabel("Probability")

plt.legend(loc="center left")
plt.show()
```

`Virginica`鸢尾花（三角形所示）的花瓣宽度范围为1.4～2.5 cm，而其他两种鸢尾花（正方形所示）花瓣通常较窄，花瓣宽度范围为0.1～1.8 cm。注意，这里有一部分重叠。对花瓣宽度超过2 cm的花，分类器可以很有信心地说它是一朵`Virginica`鸢尾花（对该类别输出一个高概率值），对花瓣宽度低于1 cm以下的，也很有信心地说其不是（对“非`Virginica`鸢尾花”类别输出一个高概率值）。在这两个极端之间，分类器则不太有把握。但是，如果你要求它预测出类［使用`predict()`方法而不是`predict_proba()`方法］，它将返回一个可能性最大的类别。也就是说，在大约1.6 cm处存在一个决策边界，这里“是”和“不是”的可能性都是50%，如果花瓣宽度大于1.6 cm，分类器就预测它是`Virginica`鸢尾花

```python
decision_boundary
log_reg.predict([[1.7], [1.5]])
```

- 与其他线性模型一样，逻辑回归模型可以用l1或l2惩罚函数来正则化，sklearn默认添加的是l2函数
- 逻辑回归模型的正则化强度超参数不是alpha，而是C， C可以看成alpha的倒数：C值越高，对模型的正则化越少


### softmax回归
逻辑回归模型经过推广，可以直接支持多个类，而不需要训练并组合多个二元分类器。这就是softmax回归，或者叫多元逻辑回归。

原理很简单：给定一个实例x，softmax回归模型首先计算出每个类k的分数$s_k(x)$，然后对这些分数应用softmax函数（也叫归一化指数），估算出每个类的概率，分数本身看起来跟线性回归预测的方程类似。


$$ S_k(x) = (\theta^{(k)})^\top x \$$

请注意，每个类都有自己的特定参数向量 $ \theta^{(k)}\ $。所有这些向量通常都作为行存储在参数矩阵 $\Theta\$ 中。

一旦为实例 $x$ 计算了每个类的分数，就可以通过 softmax 函数来估计实例属于类的概率 $\hat{p}_k\$。该函数计算每个分数的指数，然后对其进行归一化（除以所有指数的总和）。

**公式 ：softmax 函数**

$$
\hat{p}_k=\sigma\!\big(s(x)\big)_k
= \frac{\exp\!\big(s_k(x)\big)}
{\sum_{j=1}^{K}\exp\!\big(s_j(x)\big)}
$$

在此等式中：

- $k$ 是类别数目。
- s(x) 是一个向量，其中包含实例 x 的每个类的分数。
- $\sigma\\big(s(x)\big)_k\$ 是给定该实例每个类的分数，实例 x 属于第 k 类的估计概率，。

就像逻辑回归分类器一样，softmax回归分类器预测具有最高估计概率的类（简单来说就是分数最高的类):

$$
\hat{y}
= \arg\max_{k}\ \sigma\!\big(s(x)\big)_k
= \arg\max_{k}\ s_k(x)
= \arg\max_{k}\ \big(\theta^{(k)}\big)^{\top} x
$$

argmax运算符返回使函数最大化的变量值，在此等式中，它返回使得估计概率最大化的分类值（k值）

注意：softmax回归分类器一次只能预测一个类（它是多类，而不是多输出），因此它只能与互斥的类（例如不同种类的植物）一起使用。无法使用它做多输出（比如在一张照片中识别多个人）


- softmax如何训练：

训练目标是得到一个能对目标类做出高概率估算的模型（也就是其他类的概率相应要很低）。通过将代价函数（也叫作交叉熵）最小化来实现这个目标，因为当模型对目标类做出较低概率的估算时，会受到惩罚。交叉熵经常被用于衡量一组估算出的类概率与目标类的匹配程度。

$$
J(\Theta)= -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K} y_{k}^{(i)} \,\log\!\left(\hat{p}_{k}^{(i)}\right)
$$

在此等式中：$y_k^{(i)}$ 是属于第 $k$ 类的第 $i$ 个实例的目标概率，等于 $1$ 或 $0$，具体取决于该实例是否属于该类。

请注意，当只有两个类（$K=2$）时，此代价函数等效于逻辑回归的代价函数。

类k的交叉熵梯度向量：

$$
\nabla_{\theta^{(k)}} J(\Theta) = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{p}_k^{(i)} - y_k^{(i)} \right) \mathbf{x}^{(i)}
$$

通过上面计算的梯度，找到最小化函数的参数矩阵

```python
# 应用LogisticRegression分类器对两个以上的类进行训练，会自动使用softmax回归，
# 默认使用l2正则化，可以用超参数C控制正则化强度 （C=1/系数)

X = iris.data[["petal width (cm)", "petal length (cm)"]].values
y = iris["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

softmax_reg = LogisticRegression(C=30, random_state=42)
softmax_reg.fit(X_train, y_train)
```

```python
# softmax_reg.predict([[2,2]])

# softmax_reg.predict_proba([[2,2]]).round(2)  # 三个值 （代表3个类的概率）
softmax_reg.predict([[2,2]])  # 一个值 （代表最后的分类结果）

# 随堂softmax_reg最后预测正确 （accuracy，精度）的概率是多少

# 1. softmax_reg.predict_proba() 算三个概率  算的 测试集的概率
test_probs = softmax_reg.predict_proba(X_test)  # 形状： 测试的数量 * 3
test_probs.shape

# 2. 取概率最大对应的分类: 38*3， 看每一行最大值 出现的索引是什么
test_pred = np.argmax(test_probs, axis=1)


# 3. 看预测的分类 和真实分类对比，得出 在测试集 预测的精度
from sklearn.metrics import accuracy_score
accuracy_score(y_test, test_pred)  # 100%的准确率
np.all(y_test == test_pred)
```















# 支持向量机

支持向量机最基本的是要理解什么是支持向量，即实例中的样本并不是每个在超平面上的样本点都参与模型的构建，参与模型构建的称为支持向量，不参与的称为非支持向量

## 支持向量机

支持向量机(Support Vector Machine，`SVM`)是一种功能强大且用途广泛的机器学习模型，能够执行线性或非线性分类、回归，甚至异常检测。

`SVM`在中小型非线性数据集（即成百上千个实例）中大放异彩，尤其适用于分类任务。然而，它们不能很好地扩展到非常大的数据集（会讲一下它训练的所需时间 和 样本大小m/特征大小n的关系。）

教学顺序：`SVM`核心概念（目的是什么），如何使用，以及工作原理（了解即可，要涉及数学专业研究生学的数学， 凸优化）



### 线性SVM分类

`SVM`的基本思想可以用图来说明。下图所示的数据集来自鸢尾花数据集的一部分。两个类别可以轻松地被一条直线（它们是线性可分离的）分开。左图显示了三种可能的线性分类器的决策边界。决策边界用虚线表示的模型表现非常糟糕，它甚至都无法正确实现分类。其余两个模型在这个训练集上表现堪称完美，但是它们的决策边界与实例过于接近，从而导致在面对新实例时，表现可能不会太好。相比之下，右图中的实线代表`SVM`分类器的决策边界，这条线不仅分离了两个类，并且尽可能远离了最近的训练实例。

![大间隔分类](./images/svm/p1.png)

可以将SVM分类器视为在类之间拟合可能的最宽的街道（由平行的虚线表示）。因此这也叫作大间隔分类(large margin classification)。

注意：
1. SVM训练后，在“街道以外”的地方增加更多的训练实例根本不会对决策边界产生影响，也就是说，它完全由位于街道边缘的实例所决定（或者“支持”）。这些实例被称为支持向量

2. 支持向量对特征缩放敏感，如下图所示，在左图中，垂直刻度比水平刻度大得多，因此可能的最宽街道接近于水平，其中一个特征看起来毫无作用，在特征缩放（例如使用Scikit-Learn的StandardScaler）后，决策边界看起来好很多（见右图）

![决策边界对特征缩放敏感](./images/svm/p2.png)


#### 软间隔分类

如果严格地让所有实例都在街道以外，并且位于正确的一边，这就是硬间隔分类。硬间隔分类有两个主要问题：首先，它只在数据是线性可分离的时候才有效；其次，它对异常值非常敏感。下图显示了有一个额外异常值的鸢尾花数据：左图的数据根本找不出硬间隔，而右图最终显示的决策边界与之前的图所看到的无异常值时的决策边界也大不相同，这个模型可能无法很好地泛化。

![硬间隔对异常值的敏感度](./images/svm/p3.png)

要避免这些问题，需要使用更灵活的模型。目标是在保持街道尽可能宽阔和限制间隔违规（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是软间隔分类

软间隔：目标是在保持街道尽可能宽阔和限制间隔违规（即位于街道之上，甚至在错误的一边的实例）之间找到良好的平衡，这就是软间隔分类。

在使用Scikit-Learn创建SVM模型时，可以指定多个超参数，包括正则化超参数C。

如果将其设置为较小的值，则最终会得到下图左侧的模型。如果将其设置为较大的值，则将获得下图右侧的模型。

![大间隔和较少的间隔违例](./images/svm/p4.png)


减小C（提高正则化强度）会使街道变宽，但也会导致更多的间隔违例(margin violation)。换句话说，减小C会导致更多实例支持街道，因此过拟合的风险较小, 如果减小太多，会导致欠拟合

图中的实现：wx+b=0确定的边界， 街道宽度怎么确定？：wx+b=1 （图中的虚线）（平面1）， wx+b=-1 （图里的虚线）（平面2）； 2*宽度 = 平面1和平面2之间的距离

```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline

iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = iris.target == 2  # 二元分类问题
svm_clf = make_pipeline(StandardScaler(), LinearSVC(C=1, random_state=42, dual=True)) # dual=True为了消除警告用的， dual决定内部是怎么训练的 SVM
svm_clf.fit(X, y)
```

```python
X_new = [[5.5,1.7], [5.0,1.5]]
svm_clf.predict(X_new)

#  支持向量机的决策函数
svm_clf.decision_function(X_new)

# svm_clf[-1]  # 得到了流水线的最后一个组件，现在是LinearSVC
# svm_clf[-1].predict_proba() # LinearSVC没有预测概率 这里会报错


```

第一种植物被归类为Virginica鸢尾花(正类，1），而第二种（负类）则不是。 SVM用来做出这些预测的分数测量每个实例和决策边界之间的有符号距离

与LogisticRegression不同，LinearSVC没有predict_proba()方法来估计类概率

如果使用SVC类而不是LinearSVC，并且将其probability超参数设置为True，则该模型将在训练结束时拟合一个额外的模型以把SVM决策函数分数映射到估计的概率。在底层，这需要使用5折交叉验证为训练集中的每个实例生成样本外预测，然后训练LogisticRegression模型，因此它会大大减慢训练速度。之后，predict_proba()和predict_log_proba()方法就可用了。

```python
# 1. 画数据集，确定可以SVM用 线性分类（y = iris.target == 2  # 二元分类问题）
import matplotlib.pyplot as plt
plt.plot(X[y==True, 0], X[y==True,1], "rs", label="==V")
plt.plot(X[y==False, 0], X[y==False,1], "g^", label="!=V")

plt.legend()
plt.grid(True)
plt.show()

```

```python
# 2. 调用时候SVC（） 设置probability为True，
from sklearn.svm import SVC
svm_clf2 = make_pipeline(StandardScaler(), SVC(C=1, random_state=42, probability=True))
# 3. 训练
svm_clf2.fit(X, y)
# 4. 训练结束，看看能不能预测 特征 属于 0/1类的概率
# X_new = [[5.5,1.7], [5.0,1.5]]
svm_clf2.predict_proba(X_new)
```


### 非线性SVM分类
在许多情况下，线性SVM分类器是有效的，并且效果还可以，但是，有很多数据集远不是线性可分离的。处理非线性数据集的方法之一是添加更多特征，比如多项式特征

```python
# 一个简单的数据集，只有一个特征x1。可以看出，数据集不是线性可分离的。但是如果添加第二个特征x2=(x1)2，生成的2D数据集则是完全线性可分离的。
import numpy as np
import matplotlib.pyplot as plt

X1D = np.linspace(-4, 4, 9).reshape(-1, 1)
X2D = np.c_[X1D, X1D**2]
y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])

plt.figure(figsize=(10, 3))

plt.subplot(121)  # plt.subplot(1,2,1)
plt.grid(True)
plt.axhline(y=0, color='k')  # 纵坐标y=0画了一条水平线
plt.plot(X1D[:, 0][y==0], np.zeros(4), "bs")
plt.plot(X1D[:, 0][y==1], np.zeros(5), "g^")


# plt.gca(): 获取当前Axes（获取当前的子图）
plt.gca().get_yaxis().set_ticks([])  # 去掉当前子图的y轴刻度
plt.xlabel("$x_1$")
plt.axis([-4.5, 4.5, -0.2, 0.2])

plt.subplot(122)
plt.grid(True)
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], "bs")
plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], "g^")
plt.xlabel("$x_1$")
plt.ylabel("$x_2$  ", rotation=0)
plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])
plt.plot([-4.5, 4.5], [6.5, 6.5], "r--", linewidth=3)  # 红线的代码，加了特征后的X变得线性可分
plt.axis([-4.5, 4.5, -1, 17])

plt.subplots_adjust(right=1)
plt.show()
```

```python
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics.pairwise import rbf_kernel

X, y = make_moons(n_samples=100, noise=0.15, random_state=42)   # make_*生成假数据，make_moons会生成二元分类的小数据集，其中数据点的形状像两个交错的新月

polynomial_svm_clf = make_pipeline(PolynomialFeatures(degree=3),
                                   StandardScaler(),
                                   LinearSVC(C=10, max_iter=10000,random_state=42, dual=True))
polynomial_svm_clf.fit(X, y)
```

```python
import numpy as np
import matplotlib.pyplot as plt

def plot_dataset(X, y, axes):
    # axes： 坐标轴的范围，列表
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "bs")
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "g^")
    plt.axis(axes)
    plt.grid(True)
    plt.xlabel("$x_1$")
    plt.ylabel("$x_2$", rotation=0)

def plot_predictions(clf, axes):
    # axes: 坐标轴的范围，列表, [x上界，x下界, y轴上界,y轴下界]
    x0s = np.linspace(axes[0], axes[1], 100)
    x1s = np.linspace(axes[2], axes[3], 100)
    x0, x1 = np.meshgrid(x0s, x1s)
    X = np.c_[x0.ravel(), x1.ravel()]
    y_pred = clf.predict(X).reshape(x0.shape)                     # y_pred.shape: (100,100)  里面全都是预测的分类
    y_decision = clf.decision_function(X).reshape(x0.shape)       # y_decision.shape:(100,100) 里面全都是预测的分数（相对于决策线的距离）
    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)  # 根据每个网格点的预测分类（+1 或 -1），用不同颜色填充区域。颜色分界线就是 SVM 的分类边界。
    # plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=1)
    plt.contour(x0, x1, y_decision, levels=[-1, 1], colors='k', linestyles=['--',  '--']) # 画间隔，间隔 根据决策分数=+1/-1决定

# wx+b=0
# wx+b=1
# wx+b=-1

plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])
plt.show()
```


- 多项式核

添加多项式特征实现起来非常简单，但问题是，如果多项式太低阶，则处理不了非常复杂的数据集，而高阶则会创造出大量的特征，导致模型训练变得太慢。

在使用SVM时，有一种神奇的数学技术可以应用， 叫核技巧。  核技巧产生的效果：就跟添加了许多多项式特征（甚至非常高阶的多项式特征）一样，但实际上并不需要真的添加。这意味着特征数量不会出现组合爆炸式增长

这个技巧是由SVC类实现的

```python
from sklearn.svm import SVC

#  核技巧：决定对特征做什么函数变换（不会添加任何特征）； 参数 kernel=
#   多项式核： kernel="poly:
# 使用三阶多项式核函数。coef0 是核函数中的常数项（偏置项），影响高阶与低阶项的权重。C 是正则化参数，控制间隔大小 与间隔违例的权衡：C 越大越强调训练准确率，C 越小越强调间隔更大。

# 用核技巧要解决的问题： 不添加特征，能达到特征之后去训练 一样的效果
poly_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=3, coef0=1, C=5))
poly_kernel_svm_clf.fit(X, y)
```

```python
poly10_kernel_svm_clf = make_pipeline(
    StandardScaler(),
    SVC(kernel="poly", degree=10, coef0=100, C=5)
)
poly10_kernel_svm_clf.fit(X, y)

fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)

# plt.sca:  plt当前的子图（axes) 设置成指定的子图
plt.sca(axes[0])
plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title("degree=3, coef0=1, C=5")

plt.sca(axes[1])
plot_predictions(poly10_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])
plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])
plt.title("degree=10, coef0=100, C=5")
plt.ylabel("")
plt.show()
```


- 相似性特征

解决非线性问题的另一种技术是添加相似性特征。这些特征由相似函数计算得出，相似函数可以测量每个实例与一个特定地标之间的相似程度，以一维数据集为例，在x1=-2和x1=1处添加两个地标。

接下来，我们采用高斯径向基函数(RBF)作为相似函数，γ=0.3。这是一个从0（与地标距离非常远）到1（在地标位置）变化的钟形函数。

计算新特征。例如，实例x1=-1：它与第一个地标的距离为1，与第二个地标的距离为2。因此它的新特征为x2=exp(-0.3×1^2)≈0.74，x3=exp(-0.3×2^2)≈0.30。 右图显示了转换后的数据集（去除了原始特征），现在可以看出，数据变成了线性可分离的。

![使用高斯RBF的相似性特征](./images/svm/p5.png)

选择“地标”最简单的方法是在数据集里每一个实例的位置上创建一个地标。这会创造出许多维度，因而也增加了转换后的训练集线性可分离的机会。缺点是，一个有m个实例n个特征的训练集会被转换成一个有m个实例m个特征的训练集（假设抛弃了原始特征）。如果训练集非常大，那就会得到同样大数量的特征。


- 高斯RBF核

与多项式特征类似，相似性特征方法也可用于许多机器学习算法，但是要计算出所有附加特征，其计算成本可能会非常大。但使用核技巧能够产生的结果就跟添加了许多相似性特征一样，但实际上并不需要添加

以下代码是用了高斯RBF核的SVC类。



```python
# 高斯RBF核函数怎么使用： kernel="rbf",对应的其他超参数 就是 gamma

rbf_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel="rbf",gamma=5, C=0.001))
rbf_kernel_svm_clf.fit(X, y)
```

增加gamma值会使钟形曲线变得更窄，因此每个实例的影响范围随之变小：决策边界变得更不规则，开始围着单个实例绕弯。

反过来，减小gamma值会使钟形曲线变得更宽，因而每个实例的影响范围增大，决策边界变得更平坦。

所以gamma就像是一个正则化的超参数：如果模型过拟合，则减小它的值；如果欠拟合，则增大它的值（类似超参数C，对过拟合/欠拟合的调整方向一致）
怎么决定使用哪一个核函数？

1. 先尝试线性核。LinearSVC类比SVC(kernel="linear")快得多，尤其是在训练集非常大的情况下
2. 如果训练集不是太大，可以尝试高斯RBF核，它通常效果可以
3. 最后再考虑使用超参数搜索来实验其他核函数



```python
gamma1, gamma2 = 0.1, 5
C1, C2 = 0.001, 1000
hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)

svm_clfs = []

for gamma, C in hyperparams:
    rbf_kernel_svm_clf = make_pipeline(
        StandardScaler(),
        SVC(kernel="rbf", gamma=gamma, C=C)
    )
    rbf_kernel_svm_clf.fit(X, y)
    svm_clfs.append(rbf_kernel_svm_clf)

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)

for i, svm_clf in enumerate(svm_clfs):
    plt.sca(axes[i // 2, i % 2])  # 把plt的当前子图，  设置为 某行某列的子图
    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])
    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])
    gamma, C = hyperparams[i]
    plt.title(f"gamma={gamma}, C={C}")
    if i in (0, 1):
        plt.xlabel("")
    if i in (1, 3):
        plt.ylabel("")


plt.show()

from sklearn.linear_model import SGDClassifier
```


#### SVM类和计算复杂度


1. LinearSVC算法不支持核技巧，但它与训练实例的数量和特征呈线性相关，其训练时间复杂度大致为O(m×n)
2. SVC类支持核技巧，训练时间复杂度通常在O(m^2×n)和O(m^3×n)之间。这意味着如果训练实例的数量变大（例如数十万个实例），它会非常慢，所以这个算法完美适用于中小型的非线性训练集。但是，它还是可以良好地适应特征数量的增加，特别是应对稀疏特征（即每个实例仅有少量的非零特征）。在这种情况下，算法复杂度大致与实例的平均非零特征数成比例。
3. SGDClassifier类也默认执行大间距分类(SVM)，其超参数——尤其是正则化超参数（alpha和penalty）和learning_rate -- 调整这个参数可以产生与线性SVM相似的结果。 它的计算复杂度也是O(m*n), 因为其训练方式，它可以在大型数据集上训练模型

![SVM分类比较](./images/svm/p6.png)




### SVM回归
使用SVM做回归的技巧是调整目标：SVM回归不是试图在两个类之间拟合最大可能的街道同时限制间隔违例，而是尝试在街道上拟合尽可能多的实例，同时限制间隔违例（即街道以外的实例），街道的宽度由超参数epsilon控制

注意：SVM做回归的支持向量是街道外的训练集

```python
# 用于计算出支持向量（SVR中是落在ε之外的点）的位置索引。
def find_support_vectors(svm_reg, X, y):
    y_pred = svm_reg.predict(X)
    epsilon = svm_reg[-1].epsilon
    off_margin = np.abs(y - y_pred) >= epsilon  # 这里 布尔值的numpy数组，True： 在街道外； False:在街道内
    return np.argwhere(off_margin)  #   在街道外的数量 * 1，  每一行都是索引（off_margin为True的索引)

# 给定一个训练好的 svm_reg 模型和数据 X, y，绘制：
# SVR 的拟合曲线（预测值）
# ε 不敏感区间（ε-tube 上下边界）
# 支持向量（ε边界之外的点）

def plot_svm_regression(svm_reg, X, y, axes):
    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)
    y_pred = svm_reg.predict(x1s)
    epsilon = svm_reg[-1].epsilon
    plt.plot(x1s, y_pred, "k-", linewidth=2, label=r"$\hat{y}$")

    plt.plot(x1s, y_pred + epsilon, "k--")
    plt.plot(x1s, y_pred - epsilon, "k--")

    plt.scatter(X[svm_reg._support], y[svm_reg._support], s=180,facecolors='#AAA')
    plt.plot(X, y, "bo")
    plt.xlabel("$x_1$")
    plt.legend(loc="upper left")
    plt.axis(axes)

svm_reg2 = make_pipeline(StandardScaler(),
                         LinearSVR(epsilon=1.2, dual=True, random_state=42))
svm_reg2.fit(X, y)

svm_reg._support = find_support_vectors(svm_reg, X, y)
svm_reg2._support = find_support_vectors(svm_reg2, X, y)

eps_x1 = 1
eps_y_pred = svm_reg2.predict([[eps_x1]])

fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_reg, X, y, [0, 2, 3, 11])
plt.title(f"epsilon={svm_reg[-1].epsilon}")
plt.ylabel("$y$", rotation=0)
plt.grid()

plt.sca(axes[1])
plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])
plt.title(f"epsilon={svm_reg2[-1].epsilon}")

# 指出边距， textcoords=data表示数据坐标系； xy箭头位置 和 xytext文字位置
plt.annotate(
        '', xy=(eps_x1, eps_y_pred), xycoords='data',
        xytext=(eps_x1, eps_y_pred - svm_reg2[-1].epsilon),
        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}
    )  # 画的箭头
plt.text(0.90, 5.4, r"$\epsilon$", fontsize=16)   # 画的epsilon符号


plt.grid()
plt.show()
```

减小epsilon会增加支持向量的数量，从而使模型得到正则化。

此外，如果在间隔区域内添加更多的训练实例，它不会影响模型的预测。因此，该模型被称为$\epsilon$不敏感。

```python
# 要解决非线性回归任务，可以使用核化的SVM模型
from sklearn.svm import SVR

np.random.seed(42)
X = 2 * np.random.rand(50, 1) - 1
y = 0.2 + 0.1 * X[:, 0] + 0.5 * X[:, 0] ** 2 + np.random.randn(50) / 10

svm_poly_reg = make_pipeline(StandardScaler(),
                             SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1))
svm_poly_reg.fit(X, y)
```

```python
svm_poly_reg2 = make_pipeline(StandardScaler(),
                             SVR(kernel="poly", degree=2, C=100, epsilon=0.25))
svm_poly_reg2.fit(X, y)

svm_poly_reg._support = find_support_vectors(svm_poly_reg, X, y)
svm_poly_reg2._support = find_support_vectors(svm_poly_reg2, X, y)


# 比较不同C值的预测曲线
fig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)
plt.sca(axes[0])
plot_svm_regression(svm_poly_reg, X, y, [-1, 1, 0, 1])
plt.title(f"degree={svm_poly_reg[-1].degree}, "
          f"C={svm_poly_reg[-1].C}, "
          f"epsilon={svm_poly_reg[-1].epsilon}")
plt.ylabel("$y$", rotation=0)
plt.grid()

plt.sca(axes[1])
plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])
plt.title(f"degree={svm_poly_reg2[-1].degree}, "
          f"C={svm_poly_reg2[-1].C}, "
          f"epsilon={svm_poly_reg2[-1].epsilon}")
plt.grid()
plt.show()
```

`SVR`类是`SVC`类的回归等价物，`LinearSVR`类也是`LinearSVC`类的回归等价物。`LinearSVR`与训练集的大小线性相关（跟`LinearSVC`一样），而`SVR`类则在训练集变大时，变得很慢（`SVC`类也是一样）

### 线性`SVM`分类器的工作原理

1. 预测
线性 `SVM`分类器通过首先计算决策函数
$$
\boldsymbol{\theta}^T \boldsymbol{x} = \theta_0 x_0 + \theta_1 x_1 + \cdots + \theta_n x_n
$$
来预测新实例 $\boldsymbol{x}$ 的类别，其中 $x_0$ 是偏置特征（始终等于 1）。如果结果是正数，那么预测的类是阳性类（1）；否则就是阴性类（0）。这与 Logistic Regression完全一样。

2. 训练
- 第一个目标：需要找到权重w和偏置b，使街道间隔尽可能宽。
- 第二个目标：限制落在街道内的实例的数量，以及分类错误的数量（街道内 + 街道外）

- 第一个目标的数学表达：首先把街道的边界定义为决策函数（wx+b）等于1或者-1的地方。要想让边界之间的范围变宽（街道变宽），需要让w的l2范数变小。 直观的理解方式：

![较小的权重可以导致较大的间隔](./images/svm/p7.png)

在左图中，权重w1为1，因此w1x1=-1或1处的点为x1=-1和1，因此间隔的大小为2。在右图中，权重为0.5，因此w1x1=-1或1的点是x1=-2和2，间隔的大小为4。因此，需要使w尽可能小。请注意，偏置项b对间隔的大小没有影响，调整它只会移动间隔，而不会影响其大小。

- 第二个目标的数学表达：对于硬间隔来说，要想避免间隔违例，需要没有实例在街道内部，且正确分类 -> 决策函数对于所有阳性训练实例都大于等于1，对于所有阴性训练实例都小于-1。

如果定义当实例为阴性类（$\hat{y}^{(i)} = 0$）时，$t^{(i)} = -1$，当实例为阳性类（$\hat{y}^{(i)} = 1$）时，$t^{(i)} = 1$，那么我们就可以将这个约束条件写为：对所有实例来说，
$t^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b) \geq 1$

因此，我们可以将硬间隔线性 SVM 分类器的目标看作一个约束优化问题，如下面公式所示。

**公式 ：硬间隔线性 `SVM `分类器的目标**
$$
\begin{aligned}
\min_{\mathbf{w}, b} \quad & \frac{1}{2} \mathbf{w}^\top \mathbf{w} \\\\
\text{满足} \quad & t^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b) \geq 1, \quad i = 1, 2, \cdots, m
\end{aligned}
$$

这里最小化w的l2范数，而不是w的l1范数是因为w的l2范数可以求导，l1范数在w=0时不可求导，优化算法在可求导函数上效果更好

要达到软间隔的目标，我们需要为每个实例引入一个松弛变量 $\zeta^{(i)} \geq 0$，
$\zeta^{(i)}$ 衡量的是第 $i$ 个实例多大程度上允许间隔违例。

那么现在我们有了两个互相冲突的目标：
- 使松弛变量越小越好从而减少间隔违例；
- 同时还要最小化 $\frac{1}{2} \mathbf{w}^\top \mathbf{w}$ 以增大间隔。

这里是超参数 $C$ 的用武之地：允许我们在两个目标之间权衡。下面公式给出了这个约束优化问题。

**公式 5-2：软间隔线性 `SVM `分类器目标**
$$
\begin{aligned}
\min_{\mathbf{w}, b, \boldsymbol{\zeta}} \quad & \frac{1}{2} \mathbf{w}^\top \mathbf{w} + C \sum_{i=1}^m \zeta^{(i)} \\\\
\text{使得} \quad & t^{(i)} (\mathbf{w}^\top \mathbf{x}^{(i)} + b) \geq 1 - \zeta^{(i)}, \quad \zeta^{(i)} \geq 0,\quad i = 1, 2, \cdots, m
\end{aligned}
$$

硬间隔和软间隔问题都属于线性约束的凸二次优化问题。这类问题被称为二次规划（Quadratic Programming，`QP`）问题。
如何求解这种优化问题，属于凸优化的内容，感兴趣的可以参考这本书（不建议感兴趣，只是为了内容的完整写在这里）：

<img alt="凸优化" height="500" src="./images/svm/p8.jpg" width="500"/>





另一种训练`SVM`的方式（达到软间隔线性`SVM`分类器目标）是使用梯度下降来最小化`hinge`损失或平方`hinge`损失。`hinge`损失最终在优化： （不带任何约束条件->跟凸优化没有关系）
$$
\min_{\mathbf{w}, b} \quad \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i(\mathbf{w}^\top \mathbf{x}_i + b))
$$

这其中：

- $\max(0, 1 - y_i f(x_i))$：就是 `hinge `损失函数；
- 实际上它和上面的约束 $\xi_i \geq 1 - y_i f(x_i),\ \xi_i \geq 0$ 是完全等价的。

给定阳性类的实例 $x$（即 $t = 1$），如果决策函数的输出 $s = \mathbf{w}^T \mathbf{x} + b$ 大于或等于 1，则损失为 0。这发生在该实例不在街道上，并且在正侧。

给定阴性类的实例（即 $t = -1$），如果 $s \leq -1$，则损失为 0。当实例不在街道上并且处于负侧时，就会发生这种情况。

一个实例间隔的正确侧越远，损失就越大：它对于 hinge 损失呈线性增长，对于平方 hinge 损失呈二次方增长。这使得平方 hinge 损失对异常值更加敏感。

然而，如果数据集是干净的，它往往收敛得更快。

默认情况下，`LinearSVC` 使用平方 hinge 损失，而 `SGDClassifier` 使用 hinge 损失。这两个类都允许通过超参数设置为"hinge"或"squared_hinge"来选择损失。

SVC类的优化算法找的是 和 最小化hinge损失 相似的解决方案

![hinge损失和平方hinge损失](./images/svm/p8.png)




#### 对偶问题
针对一个给定的约束优化问题（称为原始问题），常常可以用另一个不同的，但是与之密切相关的问题来表达，这个问题叫作对偶问题。

通常来说，对偶问题的解只能算是原始问题的解的下限，但是在某些情况下，它也可能跟原始问题的解完全相同。

SVM的优化问题刚好属于某些情况：对偶问题的解和原始解完全相同，所以可以选择是解决原始问题还是对偶问题。

下面公式给出了线性SVM优化的对偶形式 （不用在意推导过程，感兴趣的去读前面提到的书）

$$
\begin{aligned}
\min_{\boldsymbol{\alpha}} \quad & \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha^{(i)} \alpha^{(j)} t^{(i)} t^{(j)} \mathbf{x}^{(i)\top} \mathbf{x}^{(j)} - \sum_{i=1}^{m} \alpha^{(i)} \\
\text{使} \quad & \alpha^{(i)} \geq 0,\quad i = 1, 2, \cdots, m \\
\text{且} \quad & \sum_{i=1}^{m} \alpha^{(i)} t^{(i)} = 0
\end{aligned}
$$

一旦得到最小化该等式（使用二次规划求解器）的向量 $\hat{\boldsymbol{\alpha}}$，就可以使用下面公式  来计算最小化原始问题的 $\hat{\mathbf{w}}$ 和 $\hat{b}$。在这个公式中，$n_s$ 表示支持向量的数量，也就是非0 a(i)的数量。

$$
\hat{\mathbf{w}} = \sum_{i=1}^{m} \hat{\alpha}^{(i)} t^{(i)} \mathbf{x}^{(i)}
$$

$$
\hat{b} = \frac{1}{n_s} \sum_{\substack{i=1 \\ \hat{\alpha}^{(i)} > 0}}^{m} \left( t^{(i)} - \hat{\mathbf{w}}^\top \mathbf{x}^{(i)} \right)
$$

当训练实例的数量小于特征数量时，解决对偶问题比原始问题更快速。更重要的是，对偶问题能够实现核技巧，而原始问题不可能实现。




 ##### 核化SVM

假设想要将一个二阶多项式转换为一个二维训练集，然后在转换训练集上训练线性 SVM 分类器。这个二阶多项式的映射函数如公式以下公式所示。

**公式 二阶多项式映射**

$$
\phi(\mathbf{x}) = \phi\left( \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \right) =
\begin{pmatrix}
x_1^2 \\
\sqrt{2}x_1x_2 \\
x_2^2
\end{pmatrix}
$$

注意转换后的向量是三维的而不是二维的。现在来看，如果应用这个二阶多项式映射，两个二维向量 $\mathbf{a}$ 和 $\mathbf{b}$ 会发生什么变化，然后计算转换后两个向量的点积（参见以下公式）。


$$
\phi(\mathbf{a})^\top \phi(\mathbf{b}) =
\begin{pmatrix}
a_1^2 \\
\sqrt{2}a_1a_2 \\
a_2^2
\end{pmatrix}^\top
\begin{pmatrix}
b_1^2 \\
\sqrt{2}b_1b_2 \\
b_2^2
\end{pmatrix}
= a_1^2b_1^2 + 2a_1a_2b_1b_2 + a_2^2b_2^2
= (a_1b_1 + a_2b_2)^2
= \left( \begin{pmatrix} a_1 \\ a_2 \end{pmatrix}^\top \begin{pmatrix} b_1 \\ b_2 \end{pmatrix} \right)^2
= (\mathbf{a}^\top \mathbf{b})^2
$$

转换后向量的点积等于原始向量的点积的平方：$\phi(\mathbf{a})^\top \phi(\mathbf{b}) = (\mathbf{a}^\top \mathbf{b})^2$。



**关键点**：如果将转换映射 $\phi$ 应用于所有训练实例，那么对偶问题（参考对偶问题的公式）将包含点积 $\phi(\mathbf{x}^{(i)})^\top \phi(\mathbf{x}^{(j)})$ 的计算。

如果是公式：二阶多项式映射 所定义的二阶多项式转换，那么可以直接用 $(\mathbf{x}^{(i)\top} \mathbf{x}^{(j)})^2$ 来替代这个转换后的点积。

所以根本不需要转换训练实例，只需将 对偶公式  里的点积换成点积的平方即可。

如果不嫌麻烦，可以手动将训练集进行转换，然后拟合线性 SVM 算法，会发现，结果一模一样。

但是这个技巧大大提高了整个过程的计算效率。这就是**核技巧**的本质。



函数 $K(\mathbf{a}, \mathbf{b}) = (\mathbf{a}^\top \mathbf{b})^2$ 被称为**二阶多项式核**。

在机器学习里，**核**是能够仅基于原始向量 $\mathbf{a}$ 和 $\mathbf{b}$ 来计算点积的函数，它**不需要计算（甚至不需要知道）转换函数**。以下公式  列出了一些最常用的核函数。


**公式：常用核函数**

- 线性：$K(\mathbf{a}, \mathbf{b}) = \mathbf{a}^\top \mathbf{b}$
- 多项式：$K(\mathbf{a}, \mathbf{b}) = (\gamma \mathbf{a}^\top \mathbf{b} + r)^d$
- 高斯 RBF：$K(\mathbf{a}, \mathbf{b}) = \exp(-\gamma \|\mathbf{a} - \mathbf{b}\|^2)$
- sigmoid：$K(\mathbf{a}, \mathbf{b}) = \tanh(\gamma \mathbf{a}^\top \mathbf{b} + r)$




还要说明一个问题：线性SVM优化的对偶形式的求解公式 显示了用线性 SVM 分类器如何从对偶解走到原始解，但是如果应用了核技巧，最终得到的是包含 $\phi(\mathbf{x}^{(i)})$ 的方程。而 w权重 的维度数量必须与 $\phi(\mathbf{x}^{(i)})$ 相同，后者很可能是巨大甚至是无法实体计算的，所以根本没法计算。

可是不知道权重该如何做出预测呢？你可以将对偶形式的求解公式 中的 $\hat{\mathbf{w}}$ 公式插入新实例 $\mathbf{x}^{(n)}$ 的决策函数中，这样就得到了一个只包含输入向量之间点积的公式。这样就可以再次运用核技巧了（见以下公式）。


**公式：使用核化 SVM 做出预测**

$$
\begin{aligned}
h_{\hat{\mathbf{w}}, \hat{b}}(\phi(\mathbf{x}^{(n)}))
&= \hat{\mathbf{w}}^\top \phi(\mathbf{x}^{(n)}) + \hat{b} \\
&= \left( \sum_{i=1}^{m} \hat{\alpha}^{(i)} t^{(i)} \phi(\mathbf{x}^{(i)}) \right)^\top \phi(\mathbf{x}^{(n)}) + \hat{b} \\
&= \sum_{i=1}^{m} \hat{\alpha}^{(i)} t^{(i)} \left( \phi(\mathbf{x}^{(i)})^\top \phi(\mathbf{x}^{(n)}) \right) + \hat{b} \\
&= \sum_{i=1}^{m} \hat{\alpha}^{(i)} t^{(i)} K(\mathbf{x}^{(i)}, \mathbf{x}^{(n)}) + \hat{b}
\end{aligned}
$$

注意，因为仅对于支持向量才有 $\hat{\alpha}^{(i)} \ne 0$，所以预测时，计算新输入向量 $\mathbf{x}^{(n)}$ 的点积，使用的仅仅是支持向量而不是全部训练实例。

当然，还需要使用同样的技巧来计算偏置项 $\hat{b}$（见以下公式）。

==$x^{(i)}$是第i个支持向量==

**公式：使用核技巧来计算偏置项**
$$
\begin{aligned}
\hat{b} &= \frac{1}{n_s} \sum_{i=1}^{m}
\begin{cases}
t^{(i)} - \hat{\mathbf{w}}^\top \phi(\mathbf{x}^{(i)}) & \text{if } \hat{\alpha}^{(i)} > 0
\end{cases} \\
&= \frac{1}{n_s} \sum_{i=1}^{m}
\begin{cases}
t^{(i)} - \left( \sum_{j=1}^{m} \hat{\alpha}^{(j)} t^{(j)} \phi(\mathbf{x}^{(j)}) \right)^\top \phi(\mathbf{x}^{(i)}) & \text{if } \hat{\alpha}^{(i)} > 0
\end{cases} \\
&= \frac{1}{n_s} \sum_{i=1}^{m}
\begin{cases}
t^{(i)} - \sum_{j=1}^{m} \hat{\alpha}^{(j)} t^{(j)} K(\mathbf{x}^{(j)}, \mathbf{x}^{(i)}) & \text{if } \hat{\alpha}^{(i)} > 0
\end{cases}
\end{aligned}
$$

```python
# 随堂练习： 使用作业1的第一个数据集，训练LinearSVC，然后在同一数据集上训练SVC 和 SGDClassifier，看看能否生成大致相同的模型

path = "./homework/HW1/data/hw1data1.txt"

data = np.loadtxt(path, delimiter=",")
data
X, y = data[:, :-1], data[:, -1]
X.shape, y.shape
axes = [np.min(X[:,0]), np.max(X[:, 0]), np.min(X[:, 1]),np.max(X[:,1])]
plot_dataset(X, y, axes)

from sklearn.svm import LinearSVC, SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
linear_svc = make_pipeline(StandardScaler(), LinearSVC(loss="hinge", C=1, dual=True, random_state=42))
linear_svc.fit(X, y)
m = X.shape[0]
sgd_clf = make_pipeline(StandardScaler(), SGDClassifier(alpha=0.5/m, tol=None, eta0=0.1, learning_rate="invscaling", max_iter=10000))
sgd_clf.fit(X, y)

plt.figure(figsize=(15, 5))
plt.subplot(1,3,1)
plot_dataset(X,y, axes)
plot_predictions(linear_svc, axes=axes)
plt.title("Linear SVC")

plt.subplot(1,3,2)
plot_dataset(X,y, axes)
plot_predictions(svc, axes=axes)
plt.title("SVC")

plt.subplot(1,3,3)
plot_dataset(X,y, axes)
plot_predictions(sgd_clf, axes=axes)
plt.title("SGDClassifier")
plt.show()
```


#### 把SVM看成梯度下降来实现

这样不能支持核化技巧，需要手动添加特征

但能从 另一个角度理解，SVM在做什么：
- 对 w.T@w的最小化是在增加街道间距 ->  间距的增加意味着泛化能力提高（降低过拟合，降低方差）
- 对C*(分类误差）的最小化是在提高模型的拟合能力, 降低偏差，C越大，意味着对分类误差的惩罚越高，增强拟合能力；降低C意味着更关注$ w.T@w$的最小化 -> C是之前学过的正则化强度的倒数

在之前学的有正则化的损失函数里，常见的形式是：
$$
J(\mathbf{w}, b) \;=\; \frac{1}{m} \sum_{i=1}^m L\big(y^{(i)}, \hat{y}^{(i)}\big) \;+\; \frac{\lambda}{m} \|\mathbf{w}\|^2
$$

* 第一项：平均样本误差
* 第二项：正则化项（控制模型复杂度）
* $\lambda$ 越大，正则化越强，抑制过拟合


对线性 SVM（hinge loss）来说，损失函数可以写成：

$$
J(\mathbf{w}, b) \;=\; \frac{1}{m} \sum_{i=1}^m \max\big(0, \, 1 - y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b) \big) \;+\; \frac{\lambda}{m} \|\mathbf{w}\|^2
$$

其中：

* 第一项：**hinge loss**，只在“没被正确分类/分到了间隔内”时才有损失
* 第二项：L2 正则化项（最小化 $\mathbf{w}^\top \mathbf{w}$ 来增大间距）



传统 SVM 常见写法是：

$$
\min_{\mathbf{w},b,\quad\xi_i} \quad \frac{1}{2} \|\mathbf{w}\|^2 \;+\; C \sum_{i=1}^m \xi_i
$$

$$
\text{s.t.} \quad y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$

把它改写成梯度下降版本（平均化 + hinge loss），可以得到：

$$
J(\mathbf{w}, b) \;=\; \frac{1}{m} \sum_{i=1}^m \max\big(0, \, 1 - y^{(i)}(\mathbf{w}^\top \mathbf{x}^{(i)} + b) \big) \;+\; \frac{1}{2 C m} \|\mathbf{w}\|^2
$$

这样可以看到：

* **$C$ 大** → 正则化系数 $1/(2Cm)$ 小 → 更重视分类误差（拟合能力 ↑，偏差 ↓，方差 ↑）
* **$C$ 小** → 正则化系数大 → 更重视间距（泛化能力 ↑，偏差 ↑，方差 ↓）
* **$C = 1/2\lambda$**，对应之前学的“正则化强度的倒数”理解。

`SVM `就变成了一个可以直接用梯度下降迭代的模型（不支持核化，需要手动添加特征）。


- 定义违规（有损失）掩码向量 $ \mathbf{m} \in \{0,1\}^m $：

$$
m_i =
\begin{cases}
1 & \text{if } y^{(i)} f^{(i)} < 1, \\
0 & \text{otherwise}.
\end{cases}
$$


- 则批量梯度为：

$$
\nabla_{\mathbf{w}} J
= \frac{2\alpha}{m} \mathbf{w}
- \frac{1}{m} X^\top \big( \mathbf{y} \odot \mathbf{m} \big),
$$

$$
\nabla_b J
= -\frac{1}{m} \sum_{i=1}^m y^{(i)} m_i
= -\frac{1}{m} \mathbf{y}^\top \mathbf{m},
$$

其中 $ \odot $ 表示元素乘。

```python
from sklearn.base import BaseEstimator

class MyLinearSVC(BaseEstimator):
    def __init__(self, C, eta0, n_epochs=1000, random_state=None):
        self.C = C
        self._alpha = 1 / (2*C)
        self.eta0 = eta0  # 学习率
        self.n_epochs = n_epochs  # 训练的轮次
        self.random_state = random_state # 随机数种子

    @property
    def eta(self):
        return self.eta0

    def fit(self, X, y):
        # 随机初始化，可以联系到为什么很多类 会有 random_state这个参数
        if self.random_state:
            np.random.seed(self.random_state)
        w = np.random.randn(X.shape[1], 1)  # n 个 特征权重
        b = 0

        t = np.array(y, dtype=np.float64).reshape(-1, 1) * 2 - 1  # 把 分类 0 和 分类 1 转成  分类-1 和分类 1
        self.Js = []   # 损失值列表

        # 训练
        for epoch in range(self.n_epochs):
            support_vectors_idx = ((X@w + b)*t < 1).ravel()   # 找出支持向量 （损失值不为0的训练集实例， 当前街道内部/分类错误的训练集实例）
            X_sv = X[support_vectors_idx]
            t_sv = t[support_vectors_idx]

            J = (np.sum(w * w) * self._alpha +  np.sum(1- t_sv * (X_sv@w + b))) / m   # 根据公式计算的损失值
            self.Js.append(J)

            w_gradient_vector = (2*self._alpha*w  - X_sv.T @ t_sv) / m   # dj/dw  (损失值关于权重的梯度）
            b_derivative = - np.sum(t_sv) / m                            #  损失值关于偏置的梯度

            w = w - self.eta * w_gradient_vector
            b = b - self.eta * b_derivative


        self.intercept_ = b
        self.coef_ = w
        support_vectors_idx = ((X@w + b)*t < 1).ravel()
        self.support_vectors_ = X[support_vectors_idx]   # 训练结束后，过滤出最终的支持向量
        return self

    def decision_function(self, X):
        return np.dot(X, self.coef_) + self.intercept_  # wx+b

    def predict(self, X):
        return self.decision_function(X) >= 0
```

```python
C = 1
my_linearSVC = make_pipeline(StandardScaler(), MyLinearSVC(C=C, eta0 =0.1, n_epochs=500, random_state=42))
my_linearSVC.fit(X, y)
my_linearSVC.predict(np.array([[50, 50], [90, 60]]))

```

```python
# 画 迭代轮次 -- 损失值  曲线
plt.plot(range(my_linearSVC[-1].n_epochs), my_linearSVC[-1].Js)
plt.axis([0, my_linearSVC[-1].n_epochs, 0, 1])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.grid()
plt.show()
```

```python
plt.figure(figsize=(12, 5))
plt.subplot(1,2,1)
plot_dataset(X,y, axes)
plot_predictions(linear_svc, axes=axes)
plt.title("Linear SVC")

plt.subplot(1,2,2)
plot_dataset(X,y, axes)
plot_predictions(my_linearSVC, axes=axes)
plt.title("My Linear SVC")

plt.show()
```

```python
# 验证核化的预测公式

from sklearn.datasets import make_moons
from sklearn.metrics.pairwise import rbf_kernel

X, y = make_moons(n_samples=100, noise=0.15, random_state=42)   # make_*生成假数据，make_moons会生成二元分类的小数据集，其中数据点的形状像两个交错的新月

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC

svm_clf = make_pipeline(StandardScaler(), SVC(kernel="rbf", C=1, gamma=5))
svm_clf.fit(X,y)
```

```python
_clf[-1].dual_coef_              # 50个对偶系数
svm_clf[-1].support_vectors_.shape  # 50个支持向量， (50,2)
```

```python
print(svm_clf.decision_function(X[0:1]))   # 取第一个数据，但是 用切片操作 维持2维形状
print(svm_clf.predict(X[0:1]))
```

```python
# 2. x0的预测的分数， 会不会是1.17084101
# b: svm_clf.intercept_

#   K(x(i), x(0))怎么算

#  50*2 -  1*2  -> 50 * 2
import numpy as np
gamma = 5

from sklearn.metrics.pairwise import rbf_kernel


# k_ab = np.exp(-gamma * np.sum((svm_clf.support_vectors_ - X[0:1])**2, axis=1))
x0_transform = svm_clf[0].transform(X[0:1])
#  svm_clf.support_vectors_ 不需要标准化
k_ab = rbf_kernel(svm_clf[-1].support_vectors_, x0_transform, gamma=gamma)  # 和上面的代码是等价的

x0_decision = np.dot(svm_clf[-1].dual_coef_, k_ab) + svm_clf[-1].intercept_

x0_decision
```



==**总结：向量机中的`gamma`是分类的边界，对应的是核化技巧里的参数，`gamma`增加边界会绕着实例绕圈，`degree`影响`poly`核化技巧的最高次方项，`C`影响正则化程度，`C`影响对于超出街道范围的样本的惩罚程度，`kernel`是核化技巧，`linear`、`poly`、`sigmoid`、`rbf`，`epsilon`是在回归上特有的参数，用来定义接到宽度**==





#  决策树



## 决策树
决策树是多功能的机器学习算法，可以执行分类和回归任务，甚至可以执行多输出任务。它们是功能强大的算法，能够拟合复杂的数据集。例如，在加州房屋数据集中训练了`DecisionTreeRegressor`模型，使其完全拟合（实际上是过拟合）。

决策树也是随机森林的基本组成部分，它们是当今最强大的机器学习算法之一。

将讨论如何使用决策树进行训练，可视化和做出预测开始。然后，将了解`Scikit-Learn`使用的CART训练算法，并将探索如何对树进行正则化并将其用于回归任务。最后，将讨论决策树的一些局限性。

```python
import os

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris(as_frame=True)
X_iris = iris.data[["petal length (cm)", "petal width (cm)"]].values
y_iris = iris.target

tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)  # max_depth = 3， 最大深度
tree_clf.fit(X_iris, y_iris)
```

```python
# 可视化决策树的模型

# 用export_graphviz() 函数输出一个图形定义文件 （.dot)
from sklearn.tree import export_graphviz

export_graphviz(tree_clf,
                out_file="./images/decision_tree/iris_tree.dot",  # 输出的dot文件的路径
                feature_names=["petal length (cm)", "petal width (cm)"],
                class_names=iris.target_names,  # 分类的名字
                rounded=True,
                filled=True)

```

```python
# 使用graphviz.Source.from_file() 加载并显示该文件
from graphviz import Source

Source.from_file("./images/decision_tree/iris_tree.dot")


# petal length : 2.5,   petal width: 3 在这个树 预测的分类是什么  （0/1/2）
```

```python
os.environ["PATH"]

1 - (0 + (47/48) ** 2 + (1/48)**2)
1 - ((1/3)**2 + (1/3)**2 + (1/3)**2)
```

找到一朵鸢尾花，并希望根据它的花瓣对其进行分类。你从根节点开始（深度0，顶部）：该节点询问花的花瓣长度是否小于等于2.45 cm。如果是，则向下移动到根的左子节点（深度1，左）。在这种情况下，它是一个叶节点（即它没有任何子节点），因此它不会提出任何问题：只需查看该节点的预测类，然后决策树就可以预测你的花朵是`Setosa`鸢尾花(`class=setosa`)。

假设找到另一朵花，这次花瓣的长度大于2.45 cm。再次从根开始，但现在向下移动到它的右子节点（深度1，右）。这不是叶节点，是拆分节点，所以又问了一个问题：花瓣宽度是否小于等于1.75 cm？如果是，那么花很可能是`Versicolor`鸢尾花（深度2，左）。如果不是，则可能是`Virginica`鸢尾花（深度2，右)

决策树的特质之一就是它们几乎不需要数据准备。实际上，它们根本不需要特征缩放或居中。

节点的samples属性统计它应用的训练实例数量。例如，有100个训练实例的花瓣长度大于2.45 cm（深度1，右），其中54个花瓣宽度小于1.75 cm（深度2，左）。

节点的value属性说明了该节点上每个类的训练实例数量：例如，右下节点应用在0个`Setosa`鸢尾花、1个`Versicolor`鸢尾花和45个`Virginica`鸢尾花实例上。

最后，节点的`gini`属性衡量其基尼杂质：如果应用的所有训练实例都属于同一个类，那么节点就是“纯”的(`gini=0`)。例如，深度1左侧节点仅应用于`Setosa`鸢尾花训练实例，因此它是纯的并且其基尼杂质为0。以下公式说明了第i个节点的基尼杂质Gi的计算方式。例如，深度2左侧节点，基尼杂质等于
1-(0/54)^2-(49/54)^2-(5/54)^2≈0.168。
$$
G_i = 1 - \sum_{k=1}^{n} p_{i,k}^2
$$

在此公式中：

- $ G_i $ 是第 i 个节点的基尼杂质。
- $ p_{i,k} $ 是第  i  个节点中训练实例之间的第 k 类实例的比率。



决策树的决策边界。加粗垂直实线表示根节点（深度0）的决策边界：花瓣长度=2.45cm。因为左侧区域是纯的（只有Setosa鸢尾花），所以它不可再分。

但是右侧区域是不纯的，所以深度1右侧的节点在花瓣宽度=1.75cm处（虚线所示）再次分裂。

因为这里最大深度max_depth设置为3，所以决策树在此停止。但是如果将max_depth设置为4，那么两个深度为2的节点将各自再产生一条决策边界

![决策树的决策边界](./images/decision_tree/p1.png)




### 估计类概率
决策树同样可以估算某个实例属于特定类k的概率：首先，跟随决策树找到该实例的叶节点，然后返回该节点中类k的训练实例占比。

例如，假设发现一朵花，其花瓣长5 cm，宽1.5 cm。相应的叶节点为深度2左侧节点，因此决策树输出如下概率：`Setosa`鸢尾花，0%(0/54)；`Versicolor`鸢尾花，90.7%(49/54)；`Virginica`鸢尾花，9.3%(5/54)。

要求它预测类，那么它会输出`Versicolor`鸢尾花（类1），因为它的概率最大

```python
tree_clf.predict_proba([[5,1.5]]).round(3)  # array([[0.   , 0.907, 0.093]])

tree_clf.predict([[5, 1.5]])  #  [1]
```

```python
tree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_clf_deeper.fit(X_iris, y_iris)
tree_clf_deeper.predict_proba([[6,1.5]]).round(3)
```


### 对树的属性做读取

```python
tree = tree_clf_deeper.tree_  # 获取模型内训练的树形结构 ：拟合后的决策树模型.tree_
tree

tree.node_count   # 节点的数量
tree.max_depth   # 最大深度
tree.max_n_classes  # 类的数量
tree.n_features    # 特征数量
tree.n_outputs   # 预测值的数量
tree.n_leaves    # 叶子节点的数量
tree.impurity   # 混乱度 （各个节点的 Gini系数）  按节点的前序遍历  返回
dict(tree_clf.tree_)	# 可以查看里面有什么属性
```

```python
import numpy as np
tree.children_left  # 每个节点 左孩子的索引， 索引也是按前序遍历的顺序来的

# 左右节点是同一个，说明是个叶节点（叶子节点的  左右节点的索引可能没有规律，但是相等的）
tree.children_left[4], tree.children_right[4]

# 索引去找到对应的节点

#
# # 判断是不是叶子节点
is_leaf = (tree.children_left == tree.children_right)
is_leaf
np.arange(tree.node_count)[is_leaf]
```

```python
# 非叶子节点 被叫做 split nodes(分裂节点）， 分裂基于的特征可以通过feature属性查看
#  叶子节点上的值 要被忽略
tree.feature 	 			# 注意这里的特征 是特征的索引 （0 代表第一个特征， 1代表第二个特征 ...)

tree.threshold 				# 特征对应的阈值
#
tree.value  				# 每个节点，每个分类的实例数量

#
tree.n_node_samples 		# 每个节点的总实例数量
#
np.all(tree.value.sum(axis=(1,2)) == tree.n_node_samples) # value属性可以加成 样本数量属性
```

```python
# 计算 每个节点的深度  按前序遍历计算
def compute_depth(tree_clf):
    tree = tree_clf.tree_
    depth = np.zeros(tree.node_count)
    stack = [(0,0)]  # 存的 (节点索引， 对应的深度）

    while stack:
        node, node_depth = stack.pop()
        depth[node] = node_depth      # 填写节点索引 对应的深度
        if tree.children_left[node] != tree.children_right[node]:  # 判断 是不是 分裂节点，如果是分裂节点，才执行下面逻辑
            stack.append((tree.children_right[node], node_depth + 1))  # tree.children_right[node]  node的右孩子的节点索引
            stack.append((tree.children_left[node], node_depth + 1))
    return depth

depth = compute_depth(tree_clf)
depth
```

```python
# 获取深度1 所有split nodes的特征 和 阈值

tree_clf.tree_.feature[(depth == 1) & (~is_leaf)]
tree_clf.tree_.threshold[(depth == 1) & (~is_leaf)]
```

```python
# 随堂练习：遍历树结构：从根节点  层序遍历整棵树，打印：
# 如果是叶子节点，显示“叶子”而不是特征名和阈值。
# 节点ID | 特征 | 阈值 | 样本数 | 分类结果

# 对齐方式随意，自己看得过去就行
node = 0
tree_clf.tree_.feature[node]
```

```python
print("节点ID | 特征 | 阈值   | 样本数 | 分类结果")

def print_tree_info(tree_clf, i):
    tree = tree_clf.tree_
    if tree.children_left[i] != tree.children_right[i]:
        print(f"{i:^6}| {tree.feature[i]:^4}| {tree.threshold[i]:.2f}  | {tree.n_node_samples[i]:^4}  | {tree.value[i]}")
    else:
        print(f"{i:^6}|    叶子       | {tree.n_node_samples[i]:^4}  | {tree.value[i]} ")


tree = tree_clf.tree_
queue = [0]  # 存的 节点索引

while queue:
    node = queue.pop(0) #  取队列的第一个
    print_tree_info(tree_clf, node)
    if tree.children_left[node] != tree.children_right[node]:  # 判断 是不是 分裂节点，如果是分裂节点，才执行下面逻辑
        queue.append(tree.children_left[node])
        queue.append(tree.children_right[node])

```


### CART训练算法
Scikit-Learn使用分类和回归树(`Classification and Regression Tree`，`CART`)算法来训练决策树（也称为“增长树”）。

该算法的工作原理是，首先使用单个特征k和阈值` tk`（例如，“花瓣长度≤ 2.45 cm”）将训练集分为两个子集。

如何选择k和tk？它搜索产生最纯子集的一对(k，`tk`)，按其大小加权。以下公式给出了算法试图最小化的代价函数。

$$
J(k, t_k) = \frac{m_{\text{左}}}{m} G_{\text{左}} + \frac{m_{\text{右}}}{m} G_{\text{右}}
$$

其中：

- $ G_{\text{左/右}} $：测量左/右子集的不纯度, 可以用基尼指数衡量
- $ m_{\text{左/右}} $：测量左/右子集的实例数

一旦CART算法成功地将训练集分为两部分，它就会使用相同的逻辑将子集进行拆分，然后再拆分子子集，以此类推。一旦达到最大深度（由max_depth超参数定义），或者找不到可减少不纯度的分割，它将停止递归。其他一些超参数可以控制其他一些停止条件：min_samples_split、min_samples_leaf、min_weight_fraction_leaf和max_leaf_nodes。

CART是一种贪心算法：从顶层开始搜索最优拆分，然后每层重复这个过程。几层拆分之后，它并不会检视这个拆分的不纯度是否为可能的最小值。贪心算法通常会产生一个相当不错的解，但是不能保证是最优解。

寻找全局最优的树需要访问所有的可能性，需要O(exp(m))时间，即使小训练集也需要花费很多时间，所以训练决策树满足于“不错”的解决方案。

```python
# 随堂练习：根节点找最纯的一对拆分，看看是不是和 sklearn的相同 （花瓣长度 (特征0） <=2.45cm)
#                        如果不相同，看看拆分结果是否相同
# 提示：        sklearn 用的候选阈值是 相邻样本值的中点   排序后的特征 (X_sort[i] + X_sort[i+1]) / 2
# 在这里，可以将阈值区间进行去重处理
m, n = X_iris.shape  # 同时获取实例数量 和 特征数量 m,n
min_J = 1
best_threshold =  None
best_feature = None


for j in range(n-1, -1, -1):
    sort_index = np.argsort(X_iris[:, j])
    X_iris_sorted = X_iris[sort_index]
    y_iris_sorted = y_iris.iloc[sort_index]

    for i in range(m-1):
        yuzhi = (X_iris_sorted[i, j] +  X_iris_sorted[i+1, j]) / 2  # (X_sort[i] + X_sort[i+1]) / 2 作为 yuzhi
        m_left = np.argmax(X_iris_sorted[:, j] > yuzhi)  # 第一次大于阈值的索引 就是    <=阈值 的数量
        m_right = m - m_left


        y_left = y_iris_sorted.iloc[:m_left]
        y_right = y_iris_sorted.iloc[m_left:]

        gini_left = 1- ((y_left.value_counts() / m_left) ** 2).sum()
        gini_right = 1- ((y_right.value_counts() / m_right) ** 2).sum()
        J = (m_left * gini_left + m_right * gini_right) / m

        if J <= min_J:
            min_J = J
            best_feature = j
            best_threshold = yuzhi


```

```python
best_feature, best_threshold

# 探索numpy pandas的一些用法
# sort_index = np.argsort(X_iris[:, 0])
# X_iris_sorted = X_iris[sort_index]
# X_iris_sorted
# y_iris.loc[X_iris[:, best_feature] <= best_threshold].value_counts()
# y_iris.loc[X_iris[:, best_feature] > best_threshold].value_counts()
#
#
# a_arr = np.array([3,2,6,1,0])
# a_arr[np.argsort(a_arr)]
#
# sort_index = np.argsort(X_iris[:, 0])
# X_iris_sorted = X_iris[sort_index]
#
# X_iris_sorted
# y_iris_sorted = y_iris.iloc[sort_index]
#
# y_iris_sorted
```


### 计算复杂度
进行预测需要从根到叶遍历决策树。决策树通常是近似平衡的，因此遍历决策树需要经过大约 O($ log_2(m)$)  个节点，其中 $ log_2(m) $ 是 m 的二进制对数，等于 log(m)/log(2)。由于每个节点仅需检查一个特征值，因此总体预测复杂度为 O($log_2(m)$)，与特征数量无关。因此，即使处理大训练集，预测也非常快。

训练算法比较每个节点上所有样本上的所有特征值（如果设置了 `max_features`，则更少）。比较每个节点上所有样本的所有特征会导致训练复杂度为 $ O(n \times m \log_2(m)) $。


### 基尼杂质或熵

默认情况下，DecisionTreeClassifier类使用基尼杂质来度量，但是，可以将超参数criterion设置为"entropy"来选择信息熵作为杂质的测量方式。

熵是衡量一个节点中数据混杂程度的指标。如果一个节点中的样本全部属于同一类，那么熵为 0，表示纯度最高；如果样本被多个类均匀分布，则熵较高，表示更混杂。

例如，某个节点中共有 54 个样本，其中 49 个是正类，5 个是负类，则该节点的熵为：

$$
H_i = -\frac{49}{54} \log_2\left(\frac{49}{54}\right) - \frac{5}{54} \log_2\left(\frac{5}{54}\right) \approx 0.445
$$

这个值表示该节点的样本并不是完全纯净的，存在一定的混杂程度。


熵越大，表示一个节点中的样本分布越均匀、越混杂。比如下面两个节点的对比：

- **节点 A** 中有 50 个样本，其中 49 个是正类、1 个是负类：
  $$
  H_A = -\frac{49}{50} \log_2\left(\frac{49}{50}\right) - \frac{1}{50} \log_2\left(\frac{1}{50}\right) \approx 0.081
  $$
  
  熵很小，说明这个节点很“纯”。
  
- **节点 B** 中有 50 个样本，25 个是正类，25 个是负类：

  $$
  H_B = -\frac{25}{50} \log_2\left(\frac{25}{50}\right) - \frac{25}{50} \log_2\left(\frac{25}{50}\right) = -0.5 \log_2(0.5) - 0.5 \log_2(0.5) = 1
  $$

  熵为 1，是两类最均匀的情况，表示完全混乱。

因此，在决策树中我们更倾向于选择那些**可以把数据划分为低熵子集的特征**，以提升纯度。

公式：
$$
H_i = -\sum_{\substack{k=1 \\ p_{i,k} \ne 0}}^{n} p_{i,k} \log_2(p_{i,k})
$$



那么应该使用基尼杂质还是信息熵呢？其实，大多数情况下，它们并没有什么大的不同，产生的树都很相似。基尼杂质的计算速度略微快一些，所以它是个不错的默认选择。它们的不同在于，基尼杂质倾向于从树枝中分裂出最常见的类，而信息熵则倾向于生成更平衡的树




### 正则化超参数

决策树极少对训练数据做出假设（比如线性模型就正好相反，它显然假设数据是线性的）。如果不加以限制，树的结构将跟随训练集变化，严密拟合，并且很可能过拟合。

这种模型通常被称为非参数模型，这不是说它不包含任何参数（事实上它通常有很多参数），而是指在训练之前没有确定参数的数量，导致模型结构自由而紧密地贴近数据。

相应的参数模型，比如线性模型，则有预先设定好的一部分参数，因此其自由度受限，从而降低了过拟合的风险（但是增加了欠拟合的风险）。

为避免过拟合，需要在训练过程中降低决策树的自由度（正则化）。正则化超参数的选择取决于你所使用的模型，但是通常来说，至少可以限制决策树的最大深度。在Scikit-Learn中，这由超参数max_depth控制，默认值为None，意味着无限制。减小max_depth可使模型正则化，从而降低过拟合的风险。


DecisionTreeClassifier类有一些其他参数类似地限制决策树的形状：

- max_features: 为在每个节点处进行拆分而评估的最大特征数
- max_leaf_nodes: 最大叶节点数
- min_samples_split: 一个节点在拆分之前必须拥有的最小样本数
- min_samples_leaf: 必须创建叶节点的最小样本数
- min_weight_fraction_leaf: 与min_samples_leaf相同效果，但表示为加权实例总数的一小部分 （这个参数主要用于加权决策树，也就是说，每个样本不仅仅是“1票”，而是可能有不同的权重（weight））

增加min_*,或者减少max_*超参数将使模型正则化

```python
from sklearn.datasets import make_moons

X_moons, y_moons = make_moons(n_samples=150, noise=0.2, random_state=42)

tree_clf1 = DecisionTreeClassifier(random_state=42)
tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)

tree_clf1.fit(X_moons, y_moons)
tree_clf2.fit(X_moons, y_moons)

```

```python
tree_clf1.score(X_moons, y_moons)
tree_clf2.score(X_moons, y_moons)
```

```python
# 非正则化的模型明显过拟合，正则化后的树泛化得更好
X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2, random_state=43)
tree_clf1.score(X_moons_test, y_moons_test)
tree_clf2.score(X_moons_test, y_moons_test)
```

```python
# 随堂练习：graphviz，画一下 非正则化，正则化 训练出来的决策树
# export_graphviz(tree_clf1,
#                 out_file="./images/decision_tree/moon_tree1.dot", rounded=True, filled=True)
export_graphviz(tree_clf2,
                out_file="./images/decision_tree/moon_tree2.dot", rounded=True, filled=True)
```

```python
Source.from_file("./images/decision_tree/moon_tree2.dot")
```



![非正则化树和正则化树的决策边界](./images/decision_tree/p2.png)




### 决策树回归

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor

np.random.seed(42)
X_quad = np.random.rand(200, 1) - 0.5
y_quad = X_quad ** 2 + 0.025 * np.random.randn(200, 1)

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42, min_samples_leaf=5)
tree_reg.fit(X_quad, y_quad)
```

```python
export_graphviz(tree_reg,
                out_file="./images/decision_tree/tree_reg_1.dot",
                feature_names=["x1"],
                rounded=True,
                filled=True)
Source.from_file("./images/decision_tree/tree_reg_1.dot")
```

这棵树看起来与之前构建的分类树非常相似。

主要区别在于它不是预测每个节点中的类，而是预测一个值。例如，假设你要对x1=0.2的新实例进行预测。根节点询问是否x1≤-0.303。既然不是，则算法转到右子节点，它询问是否x1≤0.272。既然是，算法继续转到左子节点。这是一个叶节点，它预测value=0.028。

此预测是与此叶节点关联的110个训练实例的平均目标值，它在这110个实例上产生等于0.001的均方误差。

```python
tree_reg2 = DecisionTreeRegressor(max_depth=3, random_state=42, min_samples_leaf=5)
tree_reg2.fit(X_quad, y_quad)
```

```python
tree_reg.tree_.threshold
tree_reg2.tree_.threshold
```

```python
# 每个区域的预测值永远等于该区域内实例的目标平均值。算法拆分每个区域的方法，使最多的训练实例尽可能接近这个预测值。
import matplotlib.pyplot as plt

def plot_regression_predictions(tree_reg, X, y, axes=[-0.5, 0.5, -0.05, 0.25]):
    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)
    y_pred = tree_reg.predict(x1)
    plt.axis(axes)
    plt.xlabel("$x_1$")
    plt.plot(X, y, "b.")  # 画数据的散点
    plt.plot(x1, y_pred, "r.-", linewidth=2, label=r"$\hat{y}$")  # 预测的折线图

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_regression_predictions(tree_reg, X_quad, y_quad)



th0, th1a, th1b = tree_reg.tree_.threshold[[0, 1, 4]]  # 阈值的编号 按照深度优先搜索（先序遍历）的遍历顺序 （根左右）
for split, style in ((th0, "k-"), (th1a, "k--"), (th1b, "k--")):
    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)

plt.text(th0, 0.16, "Depth=0", fontsize=15)
plt.text(th1a + 0.01, -0.01, "Depth=1", horizontalalignment="center", fontsize=13)
plt.text(th1b + 0.01, -0.01, "Depth=1", fontsize=13)
plt.ylabel("$y$", rotation=0)
plt.legend(loc="upper center", fontsize=16)
plt.title("max_depth=2")

# -------------------------------------
plt.sca(axes[1])
th2s = tree_reg2.tree_.threshold[[2, 5, 9, 12]]   # 在深度2 的阈值全部画一下
plot_regression_predictions(tree_reg2, X_quad, y_quad)
for split, style in ((th0, "k-"), (th1a, "k--"), (th1b, "k--")):
    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)
for split in th2s:
    plt.plot([split, split], [-0.05, 0.25], "k:", linewidth=1)
plt.text(th2s[2] + 0.01, 0.15, "Depth=2", fontsize=13)
plt.title("max_depth=3")

plt.show()

```

CART算法的工作原理如前所述，只是它不再尝试以最小化杂质的方式拆分训练集，而是尝试以最小化 MSE 的方式拆分训练集。下面公式 给出了算法试图最小化的代价函数。

**公式：CART 回归代价函数**
$$
J(k, t_k) = \frac{m_{\text{左}}}{m} \text{MSE}_{\text{左}} + \frac{m_{\text{右}}}{m} \text{MSE}_{\text{右}}
$$

其中

$$
\text{MSE}_{\text{node}} = \frac{\sum_{i \in \text{node}} (\hat{y}_{\text{node}} - y^{(i)})^2}{m_{\text{node}}}
$$

$$
\hat{y}_{\text{node}} = \frac{\sum_{i \in \text{node}} y^{(i)}}{m_{\text{node}}}
$$



就像分类任务一样，决策树在处理回归任务时容易过拟合。

如果不进行任何正则化（如使用默认的超参数）, 很容易过拟合，得到下面代码生成的预测，只需设置min_samples_leaf=10可以得到一个更合理的模型

```python
tree_reg1 = DecisionTreeRegressor(random_state=42)
tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)
tree_reg1.fit(X_quad, y_quad)
tree_reg2.fit(X_quad, y_quad)

x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)
y_pred1 = tree_reg1.predict(x1)
y_pred2 = tree_reg2.predict(x1)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)

plt.sca(axes[0])
plt.plot(X_quad, y_quad, "b.")
plt.plot(x1, y_pred1, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([-0.5, 0.5, -0.05, 0.25])
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)
plt.legend(loc="upper center")
plt.title("No restrictions")

plt.sca(axes[1])
plt.plot(X_quad, y_quad, "b.")
plt.plot(x1, y_pred2, "r.-", linewidth=2, label=r"$\hat{y}$")
plt.axis([-0.5, 0.5, -0.05, 0.25])
plt.xlabel("$x_1$")
plt.title(f"min_samples_leaf={tree_reg2.min_samples_leaf}")

plt.show()
```

```python
from sklearn.metrics import mean_squared_error
mean_squared_error(y_quad, tree_reg1.predict(X_quad))  # 0.0
```

```python
# 随堂练习：
# 1. 验证不加限制的决策树，是不是在叶子节点，样本数量拆分得只剩一个   tree_
reg_tree = tree_reg1.tree_
np.all(reg_tree.n_node_samples[reg_tree.children_left == reg_tree.children_right] == 1)  # True

# 2. 自己创一个sin曲线（加点噪声）， 用决策树回归模型拟合，
X_train_sin = np.random.rand(500, 1) * 2 * np.pi
y_train_sin = np.sin(X_train_sin) + 0.1 * np.random.randn(500, 1)

#   参数选择：第一组：调 max_depth = 2, 4, 8, None

           # 第二组：固定 max_depth=None/比较大的数，调 min_samples_split = 2, 10, 30

            #第三组：固定 max_depth=4，调 min_samples_leaf = 1, 5, 15

            # 第四组：固定 max_depth=4，调max_leaf_nodes = 10， 20， 40

           # 按组出图对比（参考课件代码）： 这些参数对模型形状和泛化能力有什么影响


fig, axes = plt.subplots(ncols=4, figsize=(16, 4), sharey=True)

first_params = {"max_depth":  [2,4,8,None]}
second_params = {"min_samples_split": [2,10,30]}
third_params =  {"min_samples_leaf": [1, 5, 15]}
fourth_params = {"max_leaf_nodes": [10, 20, 40]}

for i, max_leaf in enumerate(fourth_params["max_leaf_nodes"]):
        tree_reg = DecisionTreeRegressor(random_state=42, max_depth=32, max_leaf_nodes=max_leaf)
        tree_reg.fit(X_train_sin, y_train_sin)
        x1 = np.linspace(0, 6.3, 1000).reshape(-1, 1)
        y_pred_sin = tree_reg.predict(x1)
        plt.sca(axes[i])
        plt.plot(X_train_sin, y_train_sin, "b.")
        plt.plot(x1, y_pred_sin, "r.-", linewidth=2, label=r"$\hat{y}$")


        plt.axis([0, 6.3, -1.1, 1.1])
        plt.xlabel("$x_1$")
        plt.ylabel("$y$", rotation=0)
        plt.legend(loc="upper center")
        plt.title(f"max_leaf_nodes={max_leaf}")

plt.show()
```





### 决策树对轴方向的敏感性

决策树相对容易理解和解释、易于使用、用途广泛且功能强大。但是，它们确实有一些限制。首先，决策树喜欢正交决策边界（所有拆分都垂直于轴），这使得它们对数据的方向很敏感。例如，下图显示了一个简单的线性可分数据集：在左侧，决策树可以轻松地将其拆分，而在右侧，将数据集旋转45°后，决策边界看起来不必要地复杂。虽然两棵决策树都完美地拟合训练集，但右边的模型很可能无法很好地泛化。

```python
def plot_decision_boundary(clf, X, y, axes, cmap):
    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),
                         np.linspace(axes[2], axes[3], 100))
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)

    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)  # 根据预测值画颜色渐变，自动形成边界
    plt.contour(x1, x2, y_pred, cmap="Greys", alpha=0.8)  # 等高线图 只画边界线

    colors = {"Wistia": ["#78785c", "#c47b27"], "Pastel1": ["red", "blue"]}
    markers = ("o", "^")

    # 画数据点
    for idx in (0, 1):
        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],
                 color=colors[cmap][idx], marker=markers[idx], linestyle="")

    plt.axis(axes)
    plt.xlabel(r"$x_1$")
    plt.ylabel(r"$x_2$", rotation=0)

np.random.seed(6)
X_square = np.random.rand(100, 2) - 0.5
y_square = (X_square[:, 0] > 0).astype(np.int64)

angle = np.pi / 4  # 45 degrees
rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],
                            [np.sin(angle), np.cos(angle)]])
X_rotated_square = np.dot(X_square,rotation_matrix)

tree_clf_square = DecisionTreeClassifier(random_state=42)
tree_clf_square.fit(X_square, y_square)
tree_clf_rotated_square = DecisionTreeClassifier(random_state=42)
tree_clf_rotated_square.fit(X_rotated_square, y_square)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(tree_clf_square, X_square, y_square,
                       axes=[-0.7, 0.7, -0.7, 0.7], cmap="Pastel1")
plt.sca(axes[1])
plot_decision_boundary(tree_clf_rotated_square, X_rotated_square, y_square,
                       axes=[-0.7, 0.7, -0.7, 0.7], cmap="Pastel1")
plt.ylabel("")

plt.show()

```

限制此问题的一种方法是缩放数据，然后应用主成分分析转换(PCA)。

将在无监督学习里的降维介绍PCA，但现在只需要知道它以降低特征之间相关性的方式旋转数据，这通常（并非总是）使树的处理变得更容易。

创建一个小型流水线来缩放数据并使用PCA旋转它，然后在该数据上训练DecisionTreeClassifier。旋转使得仅使用一个特征z1就可以很好地拟合数据集，它是原始花瓣长度和宽度的线性函数。

```python
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

pca_pipeline = make_pipeline(StandardScaler(), PCA())
X_iris_rotated = pca_pipeline.fit_transform(X_iris)
tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf_pca.fit(X_iris_rotated, y_iris)
```

```python
plt.figure(figsize=(8, 4))

axes = [-2.2, 2.4, -0.6, 0.7]
z0s, z1s = np.meshgrid(np.linspace(axes[0], axes[1], 100),
                       np.linspace(axes[2], axes[3], 100))
X_iris_pca_all = np.c_[z0s.ravel(), z1s.ravel()]
y_pred = tree_clf_pca.predict(X_iris_pca_all).reshape(z0s.shape)

plt.contourf(z0s, z1s, y_pred, alpha=0.3, cmap=plt.cm.brg)
for idx, (name, style) in enumerate(zip(iris.target_names, ("ko", "bs", "g^"))):
    plt.plot(X_iris_rotated[:, 0][y_iris == idx],
             X_iris_rotated[:, 1][y_iris == idx],
             style, label=f"Iris {name}")

plt.xlabel("$z_1$")
plt.ylabel("$z_2$", rotation=0)
th1, th2 = tree_clf_pca.tree_.threshold[[0, 2]]
plt.plot([th1, th1], axes[2:], "k-", linewidth=2)   # 画的第一个边界
plt.plot([th2, th2], axes[2:], "k--", linewidth=2)  # 画的第二个边界
plt.text(th1 - 0.01, axes[2] + 0.05, "Depth=0",
         horizontalalignment="right", fontsize=15)
plt.text(th2 - 0.01, axes[2] + 0.05, "Depth=1",
         horizontalalignment="right", fontsize=13)
plt.axis(axes)
plt.legend(loc=(0.32, 0.67))

plt.show()
```

```python
# 随堂练习：
pca_pipeline = make_pipeline(StandardScaler(), PCA())
X_pca_square = pca_pipeline.fit_transform(X_rotated_square)
tree_clf_pca2 = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf_pca2.fit(X_pca_square, y_square)

pca_tree_pipleline = make_pipeline(StandardScaler(), PCA(), DecisionTreeClassifier(max_depth=2, random_state=42))
pca_tree_pipleline.fit(X_rotated_square, y_square)
```

```python
plt.figure(figsize=(12, 4))
plt.subplot(121)
plot_decision_boundary(tree_clf_pca2, X_pca_square, y_square,
                       axes=[np.min(X_pca_square[:, 0])-0.1, np.max(X_pca_square[:, 0])+0.1, np.min(X_pca_square[:, 1])-0.1, np.max(X_pca_square[:, 1])+0.1], cmap="Pastel1")

plt.subplot(122)
plot_decision_boundary(pca_tree_pipleline, X_rotated_square, y_square,
                       axes=[np.min(X_rotated_square[:, 0])-0.1, np.max(X_rotated_square[:, 0])+0.1, np.min(X_rotated_square[:, 1])-0.1, np.max(X_rotated_square[:, 1])+0.1], cmap="Wistia")
```


### 决策树有高方差



决策树还有个主要问题是具有相当高的方差：超参数或数据的微小变化可能会产生非常不同的模型。

事实上，由于Scikit-Learn使用的CART训练算法是随机的——它随机选择一组特征在每个节点上进行评估——即使在完全相同的数据上重新训练相同的决策树也可能会产生非常不同的模型

通过对许多树的预测进行平均，可以显著减少方差。这样的树集合称为随机森林

```python

plt.figure(figsize=(8, 4))
lengths, widths = np.meshgrid(np.linspace(0, 7.5, 100), np.linspace(0, 3, 100))

X_iris_all = np.c_[lengths.ravel(), widths.ravel()]
y_pred = tree_clf_tweaked.predict(X_iris_all).reshape(lengths.shape)
plt.contourf(lengths, widths, y_pred, alpha=0.3, cmap=plt.cm.brg)

for idx, (name, style) in enumerate(zip(iris.target_names, ("ko", "bs", "g^"))):
    plt.plot(X_iris[:, 0][y_iris == idx], X_iris[:, 1][y_iris == idx],
             style, label=f"Iris {name}")

th0, th1 = tree_clf_tweaked.tree_.threshold[[0, 2]]
plt.plot([0, 7.2], [th0, th0], "k-", linewidth=2)
plt.plot([0, 7.2], [th1, th1], "k--", linewidth=2)
plt.text(1.8, th0 + 0.05, "Depth=0", verticalalignment="bottom", fontsize=15)
plt.text(2.3, th1 + 0.05, "Depth=1", verticalalignment="bottom", fontsize=13)
plt.xlabel("Petal length (cm)")
plt.ylabel("Petal width (cm)")
plt.axis([0, 7.2, 0, 3])
plt.legend()
plt.show()
```

```python
#  简单补充
# 特征存的是分类相关数值 ： 1. 0 / 1，    2. 0/1/2/.../k-1, 决策树 节点里的左右  选了分类特征是怎么分的？

#   1. 0/1： 直接根据特征 0/1分左右 （0去左边，1去右边）
#   2. 0/1/2/.../k-1  独热编码后，每一列又是 0/1， 变成了上一步的解法


# 信息增益 （information gain）  等价于   CART算法里的， 加权平均的混乱度
#  信息增益 = 当前节点的混乱度（Gini/熵）  -  分成左右的加权平均的混乱度（Gini/熵）， 同一个标准
#  最小化 左右加权平均的混乱度  其实是在 最大化信息增益
#  当前节点的混乱度（Gini/熵） >= 分成左右的加权平均的混乱度   Jensen不等式
```



# 随机森林

如果聚合一组预测器（比如分类器或回归器）的预测，得到的预测结果也比最好的单个预测器要好。这样的一组预测器，称为集成，这种技术也被称为集成学习，而一个集成学习的算法则被称为集成方法。

例如，可以训练一组决策树分类器，每一棵树都基于训练集不同的随机子集进行训练。然后可以获得所有个体树的预测，获得最多选票的类是集成的预测

这样一组决策树的集成被称为随机森林，尽管很简单，但它是迄今可用的最强大的机器学习算法之一。

此外，在项目快要结束时，可能已经构建好了一些不错的预测器，这时就可以通过集成方法，将它们组合成一个更强的预测器。


## 投票分类器

假设已经训练好了一些分类器，每个分类器的精度约为80%。大概包括：一个逻辑回归分类器、一个SVM分类器、一个随机森林分类器、一个k近邻分类器，或许还有更多

创建更好分类器的一种非常简单的方法是聚合每个分类器的预测：获得最多选票的类是集成的预测。这种多数投票分类器称为硬投票分类器

这个投票分类器的精度通常比集成中最好的分类器还要高，这个结果可能有点不符合直觉

事实上，即使每个分类器都是弱学习器（意味着它仅比随机猜测好一点），通过集成依然可以实现一个强学习器（高精度），只要有足够大数量并且足够多种类的弱学习器就可以。



假设有一个略微偏倚的硬币，它有51%的可能正面朝上，49%的可能背面花朝上。如果你掷1000次，大致会得到差不多510次正面和490次背面，所以正面是大多数。

而如果做数学题，会发现在1000次投掷后，大多数为正面朝上”的概率接近75%。投掷硬币的次数越多，这个概率越高（例如，投掷10000次后，这个概率攀升至97%）。

这是因为大数定理导致的：随着你不断投掷硬币，正面朝上的比率越来越接近于正面的概率(51%)。

下面代码生成的图显示了10枚偏倚硬币的投掷结果。可以看出随着投掷次数的增加，正面的比率逐渐接近51%，最终所有10条线全都接近51%，并且始终位于50%以上

同样，假设创建了一个包含1000个分类器的集成，每个分类器都只有51%的概率是正确的（几乎没比随机猜测强多少）。如果以大多数投票的类作为预测结果，可以期待的准确率高达75%。

但是，这基于的前提是：所有的分类器都是完全独立的，彼此的错误毫不相关。显然这是不可能的，因为它们都是在相同的数据上训练的。它们很可能会犯相同的错误，所以也会有很多次大多数投给了错误的类，导致集成的精度有所降低。

```python
import matplotlib.pyplot as plt
import numpy as np
from nltk import extract

heads_proba = 0.51
np.random.seed(42)
coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)
cumulative_heads = coin_tosses.cumsum(axis=0)
cumulative_heads_ratio = cumulative_heads / np.arange(1, 10001).reshape(-1, 1)

plt.figure(figsize=(8, 3.5))
plt.plot(cumulative_heads_ratio)
plt.plot([0, 10000], [0.51, 0.51], "k--", linewidth=2, label="51%")
plt.plot([0, 10000], [0.5, 0.5], "k-", label="50%")
plt.xlabel("Number of coin tosses")
plt.ylabel("Heads ratio")
plt.legend(loc="lower right")
plt.axis([0, 10000, 0.42, 0.58])
plt.grid()
plt.show()
```

当预测器尽可能互相独立时，集成方法的效果最优。获得多种分类器的方法之一就是使用不同的算法进行训练。这会增加它们犯不同类型错误的机会，从而提升集成的精度。

Scikit-Learn提供了一个非常易于使用的VotingClassifier类：只需给它一个名称/预测器对列表，然后像普通分类器一样使用它。

加载 月牙形数据集并将其拆分为训练集和测试集，然后创建并训练由三个不同分类器组成的投票分类器：

```python
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

voting_clf = VotingClassifier(
    estimators=[('lr', LogisticRegression(random_state=42)),
                ('rf', RandomForestClassifier(random_state=42)),
                ('svc', SVC(random_state=42))]
)
voting_clf.fit(X_train, y_train)
```



当拟合VotingClassifier时，它会克隆每个估计器并拟合这个克隆。

原始估计器可通过estimators属性获得，而拟合克隆可通过 estimators_ 属性获得。

如果更喜欢字典而不是列表，则可以改用named_estimators或named_estimators_。

首先，看看每个拟合分类器在测试集上的精度：

```python
voting_clf.named_estimators_
for name, clf in voting_clf.named_estimators_.items():
    print(name, "=", clf.score(X_test, y_test))
```

```python
voting_clf.predict(X_test[:1])                                # 直接预测是1 类
# [clf.predict(X_test[:1]) for clf in voting_clf.estimators_]  # 三个里 有两个预测了是 1类
```

```python
# 投票分类器在测试集上的表现
voting_clf.score(X_test, y_test)  # 投票分类器优于单个分类器
```

```python
# 随堂练习：  不用VotingClassifier，自己模拟一下创建多个估计器，然后保留所有估计器最频繁的预测（投票过程），
#     看看能否得到和 voting_clf相似的预测结果
lr = LogisticRegression(random_state=42)
rf = RandomForestClassifier(random_state=42)
svc = SVC(random_state=42)


lr.fit(X_train, y_train)
rf.fit(X_train, y_train)
svc.fit(X_train, y_train)


```

```python
from scipy.stats import mode

lr_pred = lr.predict(X_test)
rf_pred = rf.predict(X_test)
svc_pred = svc.predict(X_test)

all_preds = np.c_[lr_pred, rf_pred, svc_pred]
all_preds


# final_pred = (np.sum(all_preds, axis=1) >= 2).astype(int)
final_pred = mode(all_preds, axis=1).mode  # 按行求众数
#
from sklearn.metrics import accuracy_score
accuracy_score(y_test,final_pred)
```

```python
mode(np.array([1,1,2,3,3,3,3,3])).mode
```

如果所有分类器都能够估计类概率［即如果它们都有一个predict_proba()方法］，

那么可以告诉Scikit-Learn预测具有最高类概率的类，对所有单独的分类器进行平均。这称为软投票。

它通常比硬投票能获得更高的性能，因为它给高度自信的投票更多的权重。需要做的就是将投票分类器的投票超参数设置为"soft"，并确保所有分类器都可以估计类概率。

默认情况下SVC类不是这种情况，因此需要将其probability超参数设置为True［这会使SVC类使用交叉验证来估计类概率，减慢训练速度，并且它会添加一个predict_proba()方法］。

```python
voting_clf.voting
voting_clf.voting = "soft"
voting_clf.named_estimators["svc"].probability = True # 原始估计器修改属性
voting_clf.fit(X_train, y_train)
voting_clf.score(X_test, y_test)  # 软投票又提高了性能
```


## bagging和pasting


前面提到，获得不同种类分类器的方法之一是使用不同的训练算法。

还有另一种方法是每个预测器使用的算法相同，但是在不同的训练集随机子集上进行训练。

采样时如果将样本放回，这种方法叫作bagging(bootstrap aggregating的缩写)

采样时样本不放回，这种方法则叫pasting。

换句话说，bagging和pasting都允许训练实例在多个预测器中被多次采样，但是只有bagging允许训练实例被同一个预测器多次采样。采样过程和训练过程如图所示：

![在训练集的不同随机样本上训练几个预测其](./images/random_forest/p1.png)



```python
# sklearn提供了一个简单的API——BaggingClassifier类——进行bagging和pasting（或BaggingRegressor用于回归

# 以下代码训练了500棵决策树分类器的集成，每次随机从训练集中采样100个训练实例进行训练，然后放回

# 如果想用pasting，只需要设置bootstrap=False即可

# n_jobs用来指示sklearn用多少CPU内核进行训练和预测，-1表示让sklearn使用所有可用内核

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
```

如果基本分类器可以估计类概率［如果它具有predict_proba()方法］，则BaggingClassifier自动执行软投票而不是硬投票

从下图可以看出，集成预测的泛化效果很可能会比单独的决策树要好一些：二者偏差相近，但是集成的方差更小（两边训练集上的错误数量差不多，但是集成的决策边界更规则）。

```python
def plot_decision_boundary(clf, X, y, alpha=1.0):
    axes=[-1.5, 2.4, -1, 1.5]
    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),
                         np.linspace(axes[2], axes[3], 100))
    X_new = np.c_[x1.ravel(), x2.ravel()]
    y_pred = clf.predict(X_new).reshape(x1.shape)

    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')
    plt.contour(x1, x2, y_pred, cmap="Greys", alpha=0.8 * alpha)
    colors = ["#78785c", "#c47b27"]
    markers = ("o", "^")
    for idx in (0, 1):
        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],
                 color=colors[idx], marker=markers[idx], linestyle="none")
    plt.axis(axes)
    plt.xlabel(r"$x_1$")
    plt.ylabel(r"$x_2$", rotation=0)

tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(tree_clf, X_train, y_train)
plt.title("Decision Tree")
plt.sca(axes[1])
plot_decision_boundary(bag_clf, X_train, y_train)
plt.title("Decision Trees with Bagging")
plt.ylabel("")
plt.show()
```



 由于自举法给每个预测器的训练子集引入了更高的多样性，因此最后bagging比pasting的偏差略高，但这也意味着预测器之间的关联度更低，所以集成的方差降低。

总之，bagging生成的模型通常更好，所以bagging比pasting更受欢迎。

但是，如果有充足的时间和CPU资源，可以使用交叉验证来对bagging和pasting的结果进行评估，再做出最合适的选择。

```python
# 随堂练习：X, y = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)  生成圆的分类数据
# 练习1： 对这个数据用BaggingClassifer分类一下试试，对比单个决策树，和 放回采样训练的 多个决策树预测的决策边界的区别
from sklearn.datasets import make_circles
X_circle, y_circle = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)

bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, n_jobs=-1, random_state=42)
bag_clf.fit(X_circle, y_circle)

tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_circle, y_circle)

fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)
plt.sca(axes[0])
plot_decision_boundary(tree_clf, X_circle, y_circle)
plt.title("Decision Tree")
plt.sca(axes[1])
plot_decision_boundary(bag_clf, X_circle, y_circle)
plt.title("Decision Trees with Bagging")
plt.ylabel("")
plt.show()


```

```python
# 练习2： 复习：这个数据看起来很适合 用SVM。写一下SVM拟合这个数据。 画出决策边界
svc =SVC(kernel="poly", degree=2)
svc.fit(X_circle, y_circle)
plot_decision_boundary(svc, X_circle, y_circle)
```



### 包外评估

对于任意给定的预测器，使用bagging，有些训练实例可能会被采样多次，而有些实例则可能根本不被采样。

BaggingClassifier默认采样m个训练实例，然后放回样本(bootstrap=True)，m是训练集的大小。通过这个过程，可以从数学上表明，对每个预测器来说，平均只对63%的训练实例进行采样[。剩余37%未被采样的训练实例称为包外(Out Of Bag)实例。

注意，对所有预测器来说，这是不一样的37%。



- 展示m次有放回的采样，平均只会对63%的训练实例采样 （大一/高中的数学）

m个训练集，每个样本被选中的概率是1/m，不被选中的概率是 1-1/m;  -> m次都不被选中的概率是 (1-1/m)^m, m越大，这个概率越接近 1/e，约等于37%左右

 `bagging`集成可以使用`OOB`（Out Of Bag）实例进行评估，而不需要单独的验证集：实际上，如果有足够的估计器，那么训练集中的每个实例都可能是多个估计器的`OOB`实例，

因此可以使用这些估计器为该实例做出公平的集成预测。一旦对每个实例都有预测，就可以计算集成的预测精度（或任何其他指标）。

在`sklearn`中，创建`BaggingClassifier`时，设置`oob_score=True`，就可以请求在训练结束后自动进行包外评估。

下面的代码演示了这一点。通过变量`oob_score_`可以得到最终的评估分数：

```python
bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,  oob_score=True, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
# bag_clf.oob_score_
```

```python
bag_clf.oob_score_
```

```python
# 正式测在测试集上的精度
from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict(X_test)
accuracy_score(y_test, y_pred)
```

```python
# 每个训练实例的OOB决策函数也可通过oob_decision_function_属性获得。
# 由于基础估计器有一个predict_proba()方法，决策函数返回每个训练实例的类概率
bag_clf.oob_decision_function_[:3]
```


### 随机补丁和随机子空间

`BaggingClassifier`类也支持对特征进行采样。采样由两个超参数控制：`max_features`和`bootstrap_features`。

它们的工作方式与`max_samples`和`bootstrap`相同，但用于特征采样而不是实例采样。因此，每个预测器将用输入特征的随机子集进行训练。

当你处理高维输入（例如图像）时，此技术特别有用，因为它可以减少训练特征大大加快训练速度。

对训练实例和特征进行采样称为随机补丁方法。保留所有训练实例（通过设置`bootstrap=False`和`max_samples=1.0`）但对特征进行采样（通过将`bootstrap_features`设置为`True`或`max_features`为小于1.0的值）称为随机子空间方法。

对特征进行采样导致更多的预测多样性，用略高一点的偏差换取更低的方差。


## 随机森林

随机森林是决策树的集合，通常通过bagging方法（有时候pasting方法）进行训练，通常将max_samples设置为训练集的大小。

可以使用RandomForestClassifier类，而不是构建BaggingClassifier并将其传递给DecisionTreeClassifier

它更方便并且针对决策树进行了优化（类似地，有一个用于回归任务的RandomForestRegressor类）。

以下代码使用所有可用的CPU内核训练一个包含500棵树的随机森林分类器，每棵树最多有16个叶节点：

```python
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
y_pred_rf
```

`RandomForestClassifier`具有`DecisionTreeClassifier`的所有超参数（以控制树的生长方式），以及`BaggingClassifier`的所有超参数来控制集成本身。

随机森林算法在树的生长上引入了更多的随机性：拆分节点时不再搜索最好的特征，而是在一个随机生成的特征子集里搜索最好的特征。

默认情况下，才采样sqrt(n)个特征（n是特征的总个数）。

导致决策树具有更大的多样性，用更高的偏差换取更低的方差，总之，还是产生了一个整体性能更优的模型。

下面的`BaggingClassifier`与前面的`RandomForestClassifier`相同：

```python
# 随堂练习： 根据刚才对原理的描述，通过BaggingClassifier 和 决策树， 生成和前面相同的RandomForestClassifier
bag_clf2 = BaggingClassifier(DecisionTreeClassifier(max_leaf_nodes=16, max_features="sqrt"), n_estimators=500,  n_jobs=-1, random_state=42)
bag_clf2.fit(X_train, y_train)
y_pred_rf2 = bag_clf2.predict(X_test)

print(np.all(y_pred_rf2 == y_pred_rf))
```

### 极端随机树

随机森林里单棵树的生长过程中，每个节点在拆分时仅考虑了一个随机子集所包含的特征。

如果我们对每个特征使用随机阈值，而不是搜索得出的最佳阈值（如常规决策树所做的），则可能让决策树生长得更加随机

只需在创建`DecisionTreeClassifier`时设置`splitter="random"`。

这种极端随机的决策树组成的森林，被称为极端随机树集成。同样，它也是以更高的偏差换取了更低的方差。

极端随机树训练起来比常规随机森林要快很多，因为在每个节点上找到每个特征的最佳阈值是决策树生长中最耗时的任务之一。

可以使用`Scikit-Learn`的`ExtraTreesClassifier`类。它的`API`与`RandomForest-Classifier`类相同，除了`bootstrap`默认为False。同样，`ExtraTreesRegressor`类具有与`RandomForestRegressor`类相同的`API`，除了`bootstrap`默认为`False`。

通常来说，不一定预先知道一个`RandomForestClassifier`是否会比一个`ExtraTreesClassifier`更好或是更差。唯一的方法是两种都尝试一遍，然后使用交叉验证进行比较。

```python
# 随堂练习：尝试使用ExtraTreesClassifier, 训练后在测试集上评估性能
from sklearn.ensemble import ExtraTreesClassifier

etc = ExtraTreesClassifier(n_estimators=500, n_jobs=-1, random_state=42, max_leaf_nodes=16)
etc.fit(X_train, y_train)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, etc.predict(X_test))

# accuracy_score(y_test, rnd_clf.predict(X_test))
```



### 特征重要性

随机森林的另一个好特性是，它们使测量每个特征的相对重要性变得容易。

`sklearn`通过查看使用该特征的树节点平均（在森林中的所有树上）减少杂质的程度来衡量该特征的重要性。

更准确地说，它是一个加权平均值，其中每个节点的权重等于与其关联的训练样本的数量



在 **`RandomForestClassifier`**里特征重要性是通过 **平均减少杂质**（Mean Decrease in Impurity, `MDI`）来衡量的。

核心思想：

* 如果一个特征在分裂数据时，能显著减少节点的不纯度（impurity），那么它对模型预测的贡献就越大。
* 计算时会考虑 **该分裂节点覆盖的样本数**，因为分裂一个样本数很多的节点影响更大。

计算步骤：

1. **计算节点的不纯度（Impurity）**
   常用 Gini 指数或熵：

   * **Gini**（二分类）：

     $$
     G(t) = 1 - p_0^2 - p_1^2
     $$

     其中 $p_k$ 是节点中类别 $k$ 的比例。
   * **熵**（Entropy）：

     $$
     H(t) = - \sum_k p_k \log_2 p_k
     $$

2. **计算该节点分裂带来的杂质减少量（Impurity Decrease）**
   对于节点 $t$，设：

   * $N_t$：到达节点 $t$ 的样本数
   * $I(t)$：该节点的不纯度
   * 左子节点 $t_L$ 样本数 $N_L$，不纯度 $I(t_L)$
   * 右子节点 $t_R$ 样本数 $N_R$，不纯度 $I(t_R)$
     那么：

   $$
   \Delta I_t = I(t) - \frac{N_L}{N_t} I(t_L) - \frac{N_R}{N_t} I(t_R)
   $$

3. **加权节点的重要性贡献**
   `sklearn `会用 **样本数权重**来放大重要性：
   $$
   \text{contribution}_t = N_t \times \Delta I_t
   $$
   
   这样一个大节点的分裂（即使 impurity decrease 不算特别大）也能体现出它的重要性。
   
4. **汇总每个特征的贡献**
   对森林中所有树的所有节点，凡是用特征 $j$ 分裂的，累计它们的 contribution：
   $$
   \text{score}_j = \sum_{\text{nodes split on } j} N_t \times \Delta I_t
   $$
   
5. **归一化**
   把所有特征的 score 相加得到总和 $S$，然后：

   $$
   \text{importance}_j = \frac{\text{score}_j}{S}
   $$

   这样所有特征的重要性之和等于 1。


`Scikit-Learn`会在训练后为每个特征自动计算该分数，然后对结果进行缩放以使所有重要性的总和等于1。

可以使用feature_importances_变量来访问结果。例如，以下代码在鸢尾花数据集上训练了`RandomForestClassifier`，并输出每个特征的重要性。看起来最重要的特征是花瓣长度(44%)和宽度(42%)，而花萼的长度和宽度则相对不那么重要（分别是11%和2%)

```python
from sklearn.datasets import load_iris
iris = load_iris(as_frame=True)
rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)
rnd_clf.fit(iris.data, iris.target)

rnd_clf.feature_importances_
np.sum(rnd_clf.feature_importances_)
for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):
    print(round(score, 2), name)
```

```python
list(zip(rnd_clf.feature_names_in_, rnd_clf.feature_importances_))
```

同样，如果在MNIST数据集上训练随机森林分类器，并绘制每个像素的重要性。

```python
from sklearn.datasets import fetch_openml

X_mnist, y_mnist = fetch_openml('mnist_784', return_X_y=True, as_frame=False,
                                parser='auto')

X_mnist.shape  # 70000 * 784(28*28)
y_mnist # 70000,  (具体的数字）

```

```python
rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rnd_clf.fit(X_mnist, y_mnist)
```

```python
heatmap_image = rnd_clf.feature_importances_.reshape(28, 28)
plt.imshow(heatmap_image, cmap="hot")
cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(),
                           rnd_clf.feature_importances_.max()])
cbar.ax.set_yticklabels(['Not important', 'Very important'], fontsize=14)
plt.axis("off")
plt.show()
```


## 提升法

提升法（boosting)是指可以将几个弱学习器结合成一个强学习器的任意集成方法。

大多数提升法的总体思路是循环训练预测器，每一次都对其前序做出一些改成。

### AdaBoost

新预测器对其前序进行纠正的办法之一，就是更多地关注前序欠拟合的训练实例。从而使新的预测器不断地越来越专注于难缠的问题，这就是`AdaBoost`使用的技术。

例如，当训练`AdaBoost`分类器时，该算法首先训练一个基础分类器（例如决策树），并使用它对训练集进行预测。然后，该算法会增加分类错误的训练实例的相对权重。之后，它使用更新后的权重训练第二个分类器，并再次对训练集进行预测，更新实例权重，以此类推

第一个分类器产生了许多错误实例，所以这些实例的权重得到提升。因此第二个分类器在这些实例上的表现有所提升，然后第三个、第四个……右图绘制的是相同预测器序列，唯一的差别在于学习率减半（即错误分类的实例权重在每次迭代中提升得更少）。可以看出，AdaBoost这种依序循环的学习技术跟梯度下降有一些异曲同工之处，差别只在于——不再是调整单个预测器的参数使代价函数最小化，而是不断在集成中加入预测器，使模型越来越好。

在make_moon数据集上5个连续的预测器的决策边界（在本例中，每个预测器都是使用RBF内核函数的高度正则化的SVM分类器， 注意SVM只是用来举例，一般AdaBoost不会配合SVM使用）。

一旦全部预测器训练完成，集成整体做出预测时就跟bagging或pasting方法一样了，除非预测器有不同的权重，因为它们总的精度基于加权后的预测器的预测。

这种依序学习技术有一个重要的缺点：训练不能并行化，因为每个预测器只能在前一个预测器被训练和评估后才能被训练。因此，在扩展方面，它的表现不如bagging和pasting方法。


 仔细看看 AdaBoost 算法。每个实例权重 $w^{(i)}$ 最初设置为 $1/m$。对第一个预测器进行训练，并根据训练集计算其加权误差率 $r_1$，见以下公式 1。

关注：1. 样本权重如何更新的？   2. 预测器的权重怎么来的？

**公式 1：第 $j$ 个预测器的加权误差率**

$$
r_j = \sum_{i=1}^m w^{(i)} \cdot \mathbf{1}\left( \hat{y}_j^{(i)} \neq y^{(i)} \right)
$$

其中，$\hat{y}_j^{(i)}$ 是第 $i$ 个实例的第 $j$ 个预测器的预测。

预测器的权重 $\alpha_j$ 通过公式 2 来计算，其中 $\eta$ 是学习率超参数（默认为 1）$\eta$。
预测器的准确率越高，其权重就越高。如果它只是随机猜测，则其权重接近于零。
但是，如果在大部分情况下它都是错的（也就是精度比随机猜测还低），那么它的权重为负。

**公式 2：预测器权重**

$$
\alpha_j = \eta \log \frac{1 - r_j}{r_j}
$$

接下来，AdaBoost 算法使用公式 3 更新实例权重，从而提高了误分类实例的权重。

**公式 3：权重更新规则**

对于 $i = 1, 2, \ldots, m$

$$
w^{(i)} \leftarrow
\begin{cases}
w^{(i)}, & \hat{y}_j^{(i)} = y^{(i)} \\
w^{(i)} \exp(\alpha_j), & \hat{y}_j^{(i)} \neq y^{(i)}
\end{cases}
$$

然后对所有实例权重进行归一化（即除以 $\sum w^{(i)}$）。

最后，使用更新后的权重训练一个新的预测器，然后重复整个过程
（计算新预测器的权重，更新实例权重，然后对另一个预测器进行训练，等等）。
当到达所需数量的预测器或得到完美的预测器时，算法停止。

预测时，AdaBoost 通过计算所有预测器的预测结果，并使用预测器权重 $\alpha$ 对它们进行加权。
最后，得到多数加权投票的类别就是预测器给出的预测类别（见公式 4）。

**公式 4：AdaBoost 预测**

$$
\hat{y}(x) = \arg\max_k \sum_{j=1}^N \alpha_j \cdot \mathbf{1}\left( \hat{y}_j(x) = k \right)
$$

其中 $N$ 是预测器的数量。

```python
m = len(X_train)


fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)

for subplot, learning_rate in ((0, 1), (1, 0.5)):
    sample_weights = np.ones(m) / m  # [1/m, 1/m, .... 1/m]
    plt.sca(axes[subplot])
    for i in range(5):
        svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)
        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)   # 通过指定权重，强调出错的实例
        y_pred = svm_clf.predict(X_train)

         # 公式1
        error_weights = sample_weights[y_pred != y_train].sum()
        r = error_weights / sample_weights.sum()  # 第i个预测器的错误率

        # 公式2
        alpha = learning_rate * np.log((1 - r) / r)  # 第i个预测器的权重

        # 公式3
        sample_weights[y_pred != y_train] *= np.exp(alpha)

        sample_weights /= sample_weights.sum()  # 归一化

        plot_decision_boundary(svm_clf, X_train, y_train, alpha=0.4)
        plt.title(f"learning_rate = {learning_rate}")
    if subplot == 0:
        plt.text(-0.75, -0.95, "1", fontsize=16)
        plt.text(-1.05, -0.95, "2", fontsize=16)
        plt.text(1.0, -0.95, "3", fontsize=16)
        plt.text(-1.45, -0.5, "4", fontsize=16)
        plt.text(1.36,  -0.95, "5", fontsize=16)
    else:
        plt.ylabel("")

plt.show()
```

```python
# 随堂练习：基于刚才的训练结果 去集成预测一下训练集里的数据 样本的分类
learning_rate = 1
models = []
alphas = []
sample_weights = np.ones(m) / m  # [1/m, 1/m, .... 1/m]

for i in range(5):
    svm_clf = SVC(C=0.2, gamma=0.6, random_state=42)
    svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)   # 通过指定权重，强调出错的实例
    y_pred = svm_clf.predict(X_train)
    models.append(svm_clf)

     # 公式1
    error_weights = sample_weights[y_pred != y_train].sum()
    r = error_weights / sample_weights.sum()  # 第i个预测器的错误率

    # 公式2
    alpha = learning_rate * np.log((1 - r) / r)  # 第i个预测器的权重
    alphas.append(alpha)

    # 公式3
    sample_weights[y_pred != y_train] *= np.exp(alpha)

    sample_weights /= sample_weights.sum()  # 归一化
```

```python
# models：5个， alphas：5个

# model.predict(X_train):  (m_test(测试集的数量），）  == （2,1)    -> (2,m_test)
# model.predict(X_train) == np.array([0,1]).reshape(-1,1)  ： 第一行是 所有预测结果是不是0的布尔值数组；  第二行是所有预测结果是不是1的布尔值数组
# np.where(models[0].predict(X_train) == np.array([0,1]).reshape(-1,1), alphas[0], 0)  单个模型 给预测的0/1 投上 w票

final_predictions_train = sum([np.where(model.predict(X_train) == np.array([0,1]).reshape(-1,1), w, 0)  for model, w in zip(models, alphas)]).argmax(axis=0)  # 找出加权投票最大的分类： 一行代码
final_predictions_train
accuracy_score(y_train, final_predictions_train)
```

```python
# 使用30个 max_depth=1的决策树 （单个决策节点 和 两个叶节点组成的树）作为 基模型的AdaBoostClassifier类

from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=30, learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)

# 如果AdaBoost集成过拟合训练集，可以试试减少估计器的数量，或提高下基础估计器的正则化程度

```


- 注意：AdaBoost也有自己的回归版本：AdaBoostRegressor，和分类去变大分类错误的实例的权重不同，回归去变大平方误差大的实例的权重

- Scikit-learn使用的是AdaBoost的一个多分类版本（AdaBoost明显针对二分类），叫作SAMME（基于多类指数损失的逐步添加模型），这些boost算法最开始也是有损失函数的，它们的算法步骤是靠最小化损失函数推导过来的，具体的推导过程涉及对自变量是函数的函数求导（泛函分析），感兴趣的可以参考SAMME算法的原始论文：https://hastie.su.domains/Papers/samme.pdf

- 如果预测器可以估算类概率（具有predict_proba()方法），Scikit-Learn会使用一种SAMME的变体，称为SAMME.R，它依赖的是类概率而不是类预测，通常表现会更好

```python

# 使用make_circle的数据，感受一下AdaBoostClassifier 用于分类
X_circle, y_circle = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)

ada_clf2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=30, learning_rate=0.5, random_state=42)
ada_clf2.fit(X_circle, y_circle)

plot_decision_boundary(ada_clf2, X_circle, y_circle)

```

```python
ada_clf2.score(X_circle, y_circle)
```


### 梯度提升
另一个提升法是梯度提升。和AdaBoost一样，梯度提升也是逐步在集成中添加预测器，每一个都对其前序做出改正。

不同之处在于，它不是像AdaBoost那样在每个迭代中调整实例权重，而是让新的预测器针对前一个的残差（实际的值 - 预测的值）进行拟合。

```python
# 使用决策树作为基本预测器，按残差去拟合 -> 梯度提升回归树
import numpy as np
from sklearn.tree import DecisionTreeRegressor

np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3 * X[:, 0] ** 2 + 0.05 * np.random.randn(100)  # y = 3x^2 + Gaussian noise（噪声）

tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)   # h1(x) 约等于 y
```

```python
# 针对第一个训练器的残差，训练第二个DecisionTreeRegressor
y2 = y - tree_reg1.predict(X)  # y2 是残差 = 真实值 - 预测值
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=43)
tree_reg2.fit(X, y2) # h2(x) 约等于 y - h1(x)
```

```python
# 针对第二个预测器的残差，训练第三个回归器

y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=44)
tree_reg3.fit(X, y3)  # h3(x) 约等于 y - h1(x) - h2(x)
```

```python
# 三棵树的集成： 第一个预测 h1， 第二个预测残差h2, 第三个预测残差h3；三个预测相加(h1+h2+h3)，对新实例进行预测
X_new = np.array([[-0.4], [0.], [0.5]])
sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
```

```python
# 随堂练习：四棵树的集成 （梯度提升）
y4 = y3 - tree_reg3.predict(X)
tree_reg4 = DecisionTreeRegressor(max_depth=2, random_state=45)
tree_reg4.fit(X, y4)
```

```python
# 第一个预测器进行正常训练，然后对每个连续的预测器（左中和左下）针对先前预测器的残差进行训练
# 右列显示集成预测的结果

def plot_predictions(regressors, X, y, axes, style,
                     label=None, data_style="b.", data_label=None):
    x1 = np.linspace(axes[0], axes[1], 500)
    y_pred = sum(regressor.predict(x1.reshape(-1, 1))
                 for regressor in regressors)
    plt.plot(X[:, 0], y, data_style, label=data_label)
    plt.plot(x1, y_pred, style, linewidth=2, label=label)
    if label or data_label:
        plt.legend(loc="upper center")
    plt.axis(axes)

plt.figure(figsize=(11, 11))

plt.subplot(3, 2, 1)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style="g-",
                 label="$h_1(x_1)$", data_label="Training set")
plt.ylabel("$y$  ", rotation=0)
plt.title("Residuals and tree predictions")

plt.subplot(3, 2, 2)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style="r-",
                 label="$h(x_1) = h_1(x_1)$", data_label="Training set")
plt.title("Ensemble predictions")

plt.subplot(3, 2, 3)
plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.4, 0.6], style="g-",
                 label="$h_2(x_1)$", data_style="k+",
                 data_label="Residuals: $y - h_1(x_1)$")
plt.ylabel("$y$  ", rotation=0)

plt.subplot(3, 2, 4)
plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.2, 0.8],
                  style="r-", label="$h(x_1) = h_1(x_1) + h_2(x_1)$")

plt.subplot(3, 2, 5)
plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.4, 0.6], style="g-",
                 label="$h_3(x_1)$", data_style="k+",
                 data_label="Residuals: $y - h_1(x_1) - h_2(x_1)$")
plt.xlabel("$x_1$")
plt.ylabel("$y$  ", rotation=0)

plt.subplot(3, 2, 6)
plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y,
                 axes=[-0.5, 0.5, -0.2, 0.8], style="r-",
                 label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$")
plt.xlabel("$x_1$")

plt.show()
```

```python
plt.figure(figsize=(11, 5))
plt.subplot(121)
plot_predictions([tree_reg4], X, y4, axes=[-0.5, 0.5, -0.4, 0.6], style="g-",
                 label="$h_4(x_1)$", data_style="k+",
                 data_label="Residuals: $y - h_1(x_1) - h_2(x_1) - h_3(x_1)$")

plt.subplot(122)
plot_predictions([tree_reg1, tree_reg2, tree_reg3, tree_reg4], X, y, axes=[-0.5, 0.5, -0.2, 0.8], style="r-",
                 label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1) + h_4(x_1)$", data_style="b.",
                 )
```

可以使用sklearn的GradientBoostingRegressor类更轻松地训练GBRT集成（还有个用于分类的GradientBoostingClassifier类）

```python
from sklearn.ensemble import GradientBoostingRegressor

# 与RandomForestRegressor类非常相似，它具有控制决策树生长的超参数（例如max_depth, min_samples_leaf),以及控制集成训练的超参数，比如树的数量（n_estimators)
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)  # 这里创建的集成 和前面代码相同
gbrt.fit(X, y)
```

超参数Learning-rate缩放每棵树的贡献。如果将其设置为较低的值（例如0.05），则集成中将需要更多的树来拟合训练集，但预测通常会更好地泛化。（这个正则化技术叫做收缩）

要找到树的最佳数量，可以使用GridSearchCV或RandomizedSearchCV执行交叉验证，但有更简单的方法：

- 将 n_iter_no_change 超参数设置为一个整数值，比如10，那么如果GradientBoostingRegressor 发现最后10棵树没有帮助，它将在训练期间自动停止添加更多树 -> 早停。

下面代码使用这个技巧来训练集成：

```python
gbrt_best = GradientBoostingRegressor(max_depth=2, learning_rate=0.05, n_estimators=500, n_iter_no_change=10, random_state=42)
gbrt_best.fit(X, y)
```

```python
# 如果将 n_iter_no_change 设置得太低，训练可能会早停并且模型会欠拟合。
# 但如果设置得太高，它反而会过拟合。
# 这里设置了个小的学习率 和 大量估计器，但实际估计器数量很少，归功于 n_iter_no_change （早停）
gbrt_best.n_estimators_
```

```python
# GBRT 集成没有足够的预测器 vs 有足够的预测器
fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)

plt.sca(axes[0])
plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style="r-",
                 label="Ensemble predictions")
plt.title(f"learning_rate={gbrt.learning_rate}, "
          f"n_estimators={gbrt.n_estimators_}")
plt.xlabel("$x_1$")
plt.ylabel("$y$", rotation=0)

plt.sca(axes[1])
plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8], style="r-")
plt.title(f"learning_rate={gbrt_best.learning_rate}, "
          f"n_estimators={gbrt_best.n_estimators_}")
plt.xlabel("$x_1$")

plt.show()

# 随堂练习： 树的数量相同，学习率不同，拟合后的效果是什么
#          学习率相同，树的数量不同，拟合后的效果是什么
```

设置n_iter_no_change时，fit()方法会自动将训练集拆分为较小的训练集和验证集：允许在每次添加新树时评估模型的性能。

验证集的大小由validation_fraction超参数控制，默认为10%。 tol超参数确定可以忽略不计的性能改进，默认是0.0001。

GradientBoostingRegressor类还支持一个subsample超参数，指定要用于训练每个树的训练实例的比例，比如subsample=0.25，每棵树都随机选择25%的训练实例进行训练。 -> 高偏差换低方差

- 1. 梯度提升的思路  （拟合残差的理论基础，辅助理解梯度提升在做什么）

要最小化的目标是：

$$
L = \sum_i L(y_i, F(x_i))
$$

这里的“变量”不是参数 $\theta$，而是整个函数 $F(x)$。

在函数空间里做梯度下降，更新公式变成：

$$
F_m(x) = F_{m-1}(x) - \eta \, g_m(x)
$$

其中：

$$
g_m(x_i) = \left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F = F_{m-1}}
$$

也就是说：每个样本的“梯度方向”，我们用一个弱学习器（比如决策树 $h_m(x)$）去拟合它。



- 2. 为什么是“拟合残差”

- 如果损失是平方误差：

$$
L(y, F(x)) = \frac{1}{2}(y - F(x))^2
$$

那么：

$$
g_m(x_i) = - (y_i - F_{m-1}(x_i)) = -r_i
$$

所以 **负梯度 = 残差**。

- 拟合残差的树 $h_m(x)$ 就近似了 $-g_m(x)$。

于是：

$$
F_m(x) = F_{m-1}(x) - \eta \cdot (-r_i) \Rightarrow F_m(x) = F_{m-1}(x) + \eta r_i
$$

也就是说：新预测器 = 旧预测器 + 学习率 × 拟合残差的弱学习器。



3. 总结

梯度提升的每一步就是：

- 在函数空间里做梯度下降；
- “拟合残差”就是在近似负梯度；
- 更新公式就等价于**之前的预测器 + (负梯度)**，


### 基于直方图的梯度提升

Scikit-Learn 还提供了另一种针对大型数据集优化的 GBRT 实现：基于直方图的梯度提升（Histogram-based Gradient Boosting，HGB）。它的工作原理是对输入特征进行分箱，用整数代替它们。箱的数量由 `max_bins` 超参数控制，默认为 255 并且不能设置为高于此值。分箱可以大大减少训练时需要评估的可能阈值的数量。此外，使用整数可以让我们使用更快、更节省内存的数据结构。箱的构建方式也消除了在训练每棵树时对特征进行排序的需要。

因此，此实现的计算复杂度为 $O(b \times m)$ 而不是 $O(n \times m \log(m))$，其中 $b$ 是箱的数量，$m$ 是训练实例的数量，$n$ 是特征的数量。实际上，这意味着 HGB 在大型数据集上的训练速度比常规 GBRT 快数百倍。然而，分箱会导致精度损失，它充当正则化器：根据数据集，这可能有助于减少过拟合，也可能导致欠拟合。

Scikit-Learn 为 HGB 提供了两个分类：`HistGradientBoostingRegressor` 和 `HistGradientBoostingClassifier`。它们类似于 `GradientBoostingRegressor` 和 `GradientBoostingClassifier`，但有一些显著差异：

- 如果实例数大于 10000，则会自动激活早停。你可以通过设置 `early_stopping` 超参数为 `True` 或 `False`，来始终打开或始终关闭早停。
- 不支持子采样。
- `n_estimators` 重命名为 `max_iter`。
- 唯一可以调整的决策树超参数是 `max_leaf_nodes`、`min_samples_leaf` 和 `max_depth`。

HGB 还有不错的特性：它们同时支持分类特征和缺失值。这大大简化了预处理。但是，分类特征必须表示为从 0 到小于 `max_bins` 的整数。你可以为此使用 `OrdinalEncoder`。例如，下面是如何为加州房屋数据集构建和训练完整的流水线：

```python
import pandas as pd
from sklearn.model_selection import train_test_split
import tarfile
from pathlib import Path
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data()

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
housing_labels = train_set["median_house_value"]
housing = train_set.drop("median_house_value", axis=1)
```

```python
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.preprocessing import OrdinalEncoder

# 整个流水线无需填补器，缩放器或独热编码器，十分方便。
hgb_reg = make_pipeline(
    make_column_transformer((OrdinalEncoder(), ["ocean_proximity"]), remainder="passthrough"),
    HistGradientBoostingRegressor(categorical_features=[0], random_state=42)  # categorical_features设置为分类列的索引
)
hgb_reg.fit(train_set, housing_labels)
```

```python
# 评估hgd_reg 的模型性能
from sklearn.model_selection import cross_val_score

hgb_rmses = -cross_val_score(hgb_reg, housing, housing_labels,
                             scoring="neg_root_mean_squared_error", cv=10)
pd.Series(hgb_rmses).describe()
```

```python
# 网格搜索/随机搜索：`max_iter`  learning_rate  max_leaf_nodes`、`min_samples_leaf` 和 `max_depth  HistGradientBoostingRegressor的最佳参数

hgb_reg.steps
```


## 堆叠法


要讨论的最后一个集成方法叫作堆叠法（stacking），又称层叠泛化法。它基于一个简单的想法：与其使用一些简单的函数（比如硬投票）来聚合集成中所有预测器的预测，为什么不训练一个模型来执行这个聚合呢？

下图展示了在新实例上执行回归任务的这样一个集成。底部的三个预测器分别预测了不同的值（3.1、2.7 和 2.9），然后最终的预测器（称为混合器或元学习器）将这些预测作为输入，进行最终预测（3.0）。

<img alt="通过混合预测器聚合预测"  src="./images/random_forest/p2.png" />

要训练混合器，首先需要构建混合训练集。可以在集成中的每个预测器上使用 `cross_val_predict()` 来获得原始训练集中每个实例的样本外预测值（见下图），并将这些用作训练混合器的输入特征；目标变量可以直接从原始训练集中复制。请注意，无论原始训练集中有多少个特征（比如本例中只有一个），混合训练集将包含每个预测器的一个输入特征（比如本例中有三个）。混合训练完毕后，基本预测器将在完整的原始训练集上重新训练一次。

<img alt="在堆叠集成中训练混合器"  src="./images/random_forest/p3.png" />



```python
# sklearn提供了 StackingClassifier 和 StackingRegressor，把应用在 月牙型数据集的 VotingClassifier 替换为 StackingClassifier
from sklearn.ensemble import StackingClassifier

stacking_clf = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(random_state=42)),
        ('rf', RandomForestClassifier(random_state=42)),
        ('svc', SVC(probability=True, random_state=42))
    ],
    final_estimator=RandomForestClassifier(random_state=43),
    cv = 5
)
stacking_clf.fit(X_train, y_train)
```

对于每个预测器，堆叠分类器将调用predict_proba(), 如果predict_proba不可用，它将调用decision_function或者作为最后的选择调用predict。

如果不提供最终估计器，StackingClassifier将使用LogisticRegression, 而StackingRegressor将使用RidgeCV

```python
stacking_clf.score(X_test, y_test)
```

集成方法是多才多艺、强大且相对简单易用的。随机森林、AdaBoost和GBRT是大多数机器学习任务中应该首先测试的模型之一，特别适用于异构表格数据。此外，由于它们需要很少的前期处理，因此它们非常适合快速推出原型。最后，投票分类器和堆叠分类器等集成方法可以帮助将系统的性能推向极限。

```python
# 随堂练习：使用StackingRegressor 去训练房价数据，感受堆叠模型
from sklearn.ensemble import StackingRegressor, RandomForestRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.svm import LinearSVR

stacking_clf = StackingRegressor(
    estimators=[
        ('lr', SGDRegressor()),
        ('rf', RandomForestRegressor(random_state=42)),
        ('svr', LinearSVR())
    ],
    final_estimator=RandomForestRegressor(random_state=43),
    cv = 5
)
```



## 对泰坦尼克数据使用XGBoost

```python
from xgboost import XGBClassifier
import pandas as pd
import numpy as np
#%%
titanic_train = pd.read_csv("homework/HW3/data/titanic/train.csv")
titanic_test = pd.read_csv("homework/HW3/data/test_augmented.csv")
#%%
titanic_train.info()
#%%
feature_columns = ["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"]
label_column = "Survived"

X_train, y_train = titanic_train[feature_columns], titanic_train[[label_column]]
X_test, y_test = titanic_test[feature_columns], titanic_test[[label_column]]
#%%
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer

preprocess = make_column_transformer(
    (make_pipeline(SimpleImputer(strategy="most_frequent"), OneHotEncoder(handle_unknown="ignore", sparse_output=False)), ["Pclass", "Sex", "Embarked"]),
    (SimpleImputer(strategy="mean"), ["Age"]),
    remainder="passthrough"
)

xgb_cat = XGBClassifier(
    tree_method="approx",
    n_estimators=100,
    learning_rate=0.03,
    max_depth=3,
    subsample=0.2,
    max_leaves= 64,
    colsample_bytree=0.4,
    eval_metric="logloss",
    random_state=42,
)

xgboost_pipeline = make_pipeline(preprocess, xgb_cat)

```

一、核心超参数速查

A. 控制过拟合 / 欠拟合

* **n\_estimators（弱学习器棵数）**

  * 作用：越大越复杂，易过拟合；与 `learning_rate` 强耦合。
  * 注意：单独调 n\_estimators 的意义不大，和 `learning_rate` 一起看。

* **learning\_rate（步长 / η）**

  * 作用：越小越稳健，一般需要更大的 `n_estimators`。
  * 常用：`0.02~0.2`（常用 `0.03/0.05/0.1`）。
  * 经验：想要更强泛化→把 `learning_rate` 降低，并提高 `n_estimators`

* **subsample（行采样）**

  * 作用：每棵树采样多少比例的样本；降低方差、抗过拟合。
  * 常用：`0.6~1.0`（常用 `0.7~0.9`）。
  * 注意：太低可能欠拟合，和 `colsample_bytree` 联动调。

* **colsample\_bytree（列采样）**

  * 作用：每棵树采样多少比例的特征；类似随机森林的列采样。
  * 常用：`0.6~1.0`（特征多时可取 `0.6~0.8`）。
  * 注意：与 `subsample` 乘积别太小（<0.4 易欠拟合）。

* **max\_depth（最大深度）**

  * 作用：限制树深；越深表达力越强，过拟合风险越高。
  * 常用：`3~10`（结构噪声多时先试 `3~6`）。
  * 注意：与 `min_child_weight`、`gamma` 共同影响复杂度。

* **max\_leaves（最大叶子数）**

  * 作用：直接限制叶子数量
  * 常用：`16~512`（从小到大尝）。
  * 注意：设置了 `max_leaves` 时，`max_depth` 通常设较大或忽略。

* **min\_child\_weight（叶子最小样本“权重”）**

  * 作用：越大越保守（抑制小样本的叶子），防过拟合。
  * 常用：`1, 2, 5, 10`（样本稀疏/噪声大→用更大）。
  * 直觉：像 `min_samples_leaf` 的“强版”。

* **gamma（又名 `min_split_loss`）**

  * 作用：分裂所需的最小损失下降；>0 则更难分裂，抑制过拟合。
  * 常用：`0 ~ 10`（从 0 开始，按需增至 `1/5/10`）。

* **lambda（`reg_lambda`，L2）**

  * 作用：L2 正则；增大可减小权重、抑制过拟合。
  * 常用：`0~20`（常见 `1/3/5/10`）。
  * 直觉：更稳健、一般先于 `alpha` 调。

* **alpha（`reg_alpha`，L1）**

  * 作用：L1 正则；可稀疏化，提高可解释性，强力抑制过拟合。
  * 常用：`0~10`（常见 `0/0.5/1/5`）。
  * 注意：过大可能欠拟合。

B. 构建算法相关

* **tree\_method：`hist` / `approx`**

  * `hist`：基于直方图，**速度快、显存省**，默认首选；支持大数据、原生分类特征。
  * `approx`：近似分位点法，老方法；一般只有兼容性/特定场景才用。
  * 结论：优先 `hist`。

C. 采样细节

* **sampling\_method：`uniform` / `gradient_based`（配合 `subsample`）**

  * `uniform`：均匀随机抽样（默认）。
  * `gradient_based`：更偏向梯度大的样本（难样本），可能**更快收敛**，但易过拟合；建议在较低 `subsample` 时尝试。
  * 常用：默认先 `uniform`，若数据不均衡或有大量易样本，可试 `gradient_based`。

```python
#%%
xgboost_pipeline.fit(X_train, y_train)
#%%
from sklearn.model_selection import RandomizedSearchCV

rnd_search_boost = RandomizedSearchCV(xgboost_pipeline, param_distributions={
    "xgbclassifier__learning_rate": [0.01, 0.02, 0.03, 0.05, 0.1],
    "xgbclassifier__n_estimators": [50, 100, 150, 200, 250, 300],
    "xgbclassifier__subsample": [0.5, 0.6, 0.7, 0.8, 0.9],
    "xgbclassifier__colsample_bytree": [0.5, 0.6, 0.7, 0.8, 0.9, 1],
    "xgbclassifier__max_depth": [2, 3, 4, 5, 6],
    "xgbclassifier__max_leaves": [16, 32, 64, 128, 256],
    "xgbclassifier__min_child_weight": [1, 2, 5, 10],
    "xgbclassifier__min_split_loss": [0, 1, 5, 10],
    "xgbclassifier__reg_lambda": [0, 1, 3, 5, 10],
    "xgbclassifier__tree_method": ["hist", "approx"]
}, n_iter=500, cv=5, scoring="neg_log_loss")

rnd_search_boost.fit(X_train, y_train)
#%%
pd.DataFrame(rnd_search_boost.cv_results_).sort_values(by="mean_test_score", ascending=False)
#%%
rnd_search_boost.best_params_
#%%
# 验证最佳参数
from sklearn.model_selection import cross_val_score
xgb_best_clf = make_pipeline(preprocess, XGBClassifier(eval_metric="logloss", **{k[len("xgbclassifier__"):] : v for k ,v in rnd_search_boost.best_params_.items()}))

cross_val_score(xgb_best_clf, X_train, y_train, cv=5, scoring="accuracy")
#%%
# 看最佳参数的模型在训练集的表现

xgb_best_clf.fit(X_train, y_train)
xgb_best_clf.score(X_train, y_train)
#%%
# 找在超参数搜索中   交叉验证表现最佳的N个模型，训练，并看最后在测试集的表现

param_columns = ["param_" + name for name in [   'xgbclassifier__tree_method',
                                                 'xgbclassifier__subsample',
                                                 'xgbclassifier__reg_lambda',
                                                 'xgbclassifier__n_estimators',
                                                 'xgbclassifier__min_split_loss',
                                                 'xgbclassifier__min_child_weight',
                                                 'xgbclassifier__max_leaves',
                                                 'xgbclassifier__max_depth',
                                                 'xgbclassifier__learning_rate',
                                                 'xgbclassifier__colsample_bytree']]


xgb_cv_results = pd.DataFrame(rnd_search_boost.cv_results_).sort_values(by="mean_test_score", ascending=False)
xgb_cv_params =xgb_cv_results[param_columns]
#%%
N = 500
best_test_accuracy = 0
best_params = None
best_model = None
for i in range(N):
    param_dict = xgb_cv_params.iloc[i].to_dict()
    xgb_clf = make_pipeline(preprocess, XGBClassifier(eval_metric="logloss", **{k[len("param_xgbclassifier__"):] : v for k ,v in param_dict.items()}))
    xgb_clf.fit(X_train, y_train)

    test_accuracy = xgb_clf.score(X_test, y_test)
    if test_accuracy > best_test_accuracy:
        best_test_accuracy = test_accuracy
        best_model = xgb_clf
        best_params = param_dict
    # print(f"training accuracy: {xgb_clf.score(X_train, y_train)}, test accuracy: {test_accuracy}")

#%%
best_params
# best_test_accuracy
#%%
output_frame = titanic_test[["PassengerId"]].copy()
output_frame["Survived"] = best_model.predict(X_test)
#%%
output_frame

#%%
output_frame.to_csv("./homework/HW3/titanic_pred.csv", index=False)
```

