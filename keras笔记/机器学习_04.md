# tensorflow的自定义训练用法

## Tensorflow介绍

当需要额外的控制来编写自定义损失函数、自定义指标、层、模型、初始化程序、正则化函数、权重约束等时，研究Tensorflow的其他底层API很有用。有时候可能需要完全控制训练循环，例如对梯度使用特殊的变换或约束（不仅仅对它们进行裁剪），或者对网络的不同部分使用多个优化器。还将探讨如何使用TensorFlow的自动图形生成功能来增强自定义模型和训练算法

Tensorflow：
- 它的核心与NumPy非常相似，但支持GPU。
- 它支持分布式计算（跨多个设备和服务器）。·
- 它包含一种即时(JIT)编译器，可使其针对速度和内存使用情况来优化计算。它的工作方式是从Python函数中提取计算图，然后进行优化（通过修剪未使用的节点），最后有效地运行它（通过自动并行运行相互独立的操作）。
- 计算图可以导出为可移植格式，因此可以在一个环境中（例如在Linux上使用Python）训练TensorFlow模型，然后在另一个环境中（例如在Android设备上使用Java）运行TensorFlow模型。
- 它实现了反向模式的自动微分(autodiff) 并提供了一些优秀的优化器，例如RMSProp和Nadam，因此可以轻松地最小化各种损失函数。

TensorFlow在这些核心功能的基础上提供了更多功能：最重要的当然是Keras， 但它还具有数据加载和预处理操作（tf.data、tf.io等），以及图像处理操作(tf.image)和信号处理操作(tf.signal)

<img alt="Tensorflow的Python API" height="500" src="./images/tensorflow/p1.jpg" width="500"/>

在底层，每个TensorFlow操作（以下简称op）都是使用高效的C++代码实现的。许多操作都有称为内核的多种实现：每个内核专用于特定的设备类型，例如CPU、GPU甚至TPU（张量处理单元）。GPU可以通过将整个计算分成许多较小的块并在多个GPU线程中并行运行它们来极大地加快计算速度。TPU甚至更快：它们是专门为深度学习操作而构建的定制ASIC芯片

Tensforflow大多数时候，使用高层API（尤其是Keras和tf.data), 当需要更大的灵活性时，则可以使用较低层的PythonAPI，直接处理张量



TensorFlow不仅可以在Windows、Linux和macOS上运行，还可以在移动设备上运行（使用TensorFlow Lite），包括在iOS和Android设备上。

如果不想使用Python API，也可以使用其他语言的API，例如C++、Java和Swift API。甚至还有一个名为TensorFlow.js的JavaScript实现，可以直接在浏览器中运行模型。

<img alt="Tensorflow的架构" height="500" src="./images/tensorflow/p2.jpg" width="500"/>



## 像NumPy一样使用Tensorflow

TensorFlow的API围绕张量展开，张量从一个操作流向另一个操作，因此得名TensorFlow。张量与NumPy ndarray非常相似：

它通常是一个多维数组，但也可保存标量（数值）。当创建自定义代价函数、自定义指标、自定义层等时，这些张量将非常重要。

### 张量和操作

```python
import tensorflow as tf

t = tf.constant([[1.,2.,3.],
                 [4.,5.,6.]])  # tf.constant创建张量
t
t.shape
t.dtype
type(t)
#
tf.constant(100)
```

```python
t[:, 1:]
t[..., 1, tf.newaxis]		# ... 与: 相同表示选择维度0的所有
t[0,1]
```

```python
# 各种张量操作
t + 10
tf.add(t, 10)
tf.square(t)
t @ tf.transpose(t)
tf.matmul(t, tf.transpose(t))
```

```python
# tf.squeeze, tf.reduce_mean, tf.reduce_sum, tf.reduce_max, tf.math.log
# 构造一个简单的张量
x = tf.constant([[1.0, 2.0, 3.0],
                 [4.0, 5.0, 6.0]])

print("原始张量 x:")
print(x.numpy())

# 1. tf.squeeze：去掉维度为1的维度
x_expand = tf.expand_dims(x, axis=0)   # shape (1, 2, 3)
print("\n扩展后张量形状:", x_expand.shape)
print("squeeze之后:", tf.squeeze(x_expand).shape)

# 2. tf.reduce_mean：求均值
mean_all = tf.reduce_mean(x)                 # 所有元素的平均值
mean_axis0 = tf.reduce_mean(x, axis=0)       # 按列求均值
print("\nreduce_mean 所有元素:", mean_all.numpy())
print("reduce_mean 按列:", mean_axis0.numpy())
#
# 3. tf.reduce_sum：求和
sum_all = tf.reduce_sum(x)
sum_axis1 = tf.reduce_sum(x, axis=1)         # 按行求和
print("\nreduce_sum 所有元素:", sum_all.numpy())
print("reduce_sum 按行:", sum_axis1.numpy())
#
# 4. tf.reduce_max：最大值
max_all = tf.reduce_max(x)
max_axis0 = tf.reduce_max(x, axis=0)
print("\nreduce_max 所有元素:", max_all.numpy())
print("reduce_max 按列:", max_axis0.numpy())
#
# 5. tf.math.log：自然对数
log_x = tf.math.log(x)
print("\nlog(x):")
print(log_x.numpy())
```

许多函数和类都有别名。例如，tf.add()和tf.math.add()是相同的函数。这允许TensorFlow为常见的操作提供简洁的名称

tf.transpose()函数的功能与NumPy的T属性完全不相同：在TensorFlow中，使用自己的转置数据副本创建一个新的张量，而在NumPy中，t.T只是相同数据的转置视图。类似地，tf.reduce_sum()之所以这样命名，是因为其GPU内核（即GPU实现）使用的reduce算法不能保证元素添加的顺序：32位浮点数的精度有限，因此每次调用此操作时，结果可能会稍有不同。tf.reduce_mean()也是如此（当然，tf.reduce_max()是确定性的

### 张量和NumPy

张量可以与NumPy配合使用：可以用NumPy数组创建张量，反之亦然。也可以将TensorFlow操作应用于NumPy数组，将NumPy操作应用于张量

```python
import numpy as np

a = np.array([2.,4.,5.])
tf.constant(a)
t.numpy()  # np.array(t)
tf.square(a)
np.square(t)
```

### 类型转换

类型转换会严重影响性能，并且自动完成的类型转换很容易被忽视。为了避免这种情况，TensorFlow不会自动执行任何类型转换：如果对不兼容类型的张量执行此操作，会引发异常。例如，不能把浮点数张量和整数张量相加，甚至不能将32位浮点数和64位浮点数相加：

```python
# tf.constant(2.) + tf.constant(40)
# tf.constant(2.) + tf.constant(40., dtype=tf.float64)

t2 = tf.constant(40, dtype=tf.float64)
tf.constant(2.0) + tf.cast(t2, tf.float32) # 确实需要转换类型时，使用tf.cast()
```

### 变量

tf.constant创建的Tensor值是不变的：无法修改它们。这意味着不能使用常规张量在神经网络中表示权重，因为它们需要通过反向传播进行调整。

另外，还可能需要随时间改变其他参数（例如动量优化器跟踪过去的梯度）。需要tf.Variable：

```python
v = tf.Variable([[1.,2.,3.], [4.,5.,6.]])
v
```

tf.Variable的行为与tf.Tensor的行为非常相似：可以使用它执行相同的操作，它在NumPy中也可以很好地发挥作用，并且对类型也很挑剔。但是，它也可以使用assign()方法（或assign_add()与assign_sub()，给变量增加或减少给定值）进行修改。

还可以通过单元（或切片）的assign()方法（直接指定将不起作用）或者使用scatter_update()或scatter_nd_update()方法来修改单个单元（或切片），但直接赋值将不起作用

==张量和变量的区别：张量不可变，就像元组，而变量内的各个元素是可以索引赋值改变的==

```python
# 变量赋值
v.assign(2*v)
v
v[0,1].assign(42)
v[:,2].assign([0., 1.])
v.scatter_nd_update(indices=[[0,0], [1,2]], updates=[100., 200.])


# v[1] = [7.,8.,9.] # 直接赋值不起作用
v[1].assign([7,8,9])


```

```python
observation = np.array([111.0, 188.0])
codes = np.array([[102.0, 203.0],
               [132.0, 193.0],
               [45.0, 155.0],
               [57.0, 173.0]])

np.argmin(np.sqrt(np.sum((observation - codes)**2, axis=1)))
# 找到codes里 距离observation最近的codes索引（位置），使用Tensor实现

observation = tf.constant(observation)
codes = tf.constant(codes)

tf.argmin(tf.reduce_sum(tf.square(tf.subtract(observation, codes)), axis=1))
```

## 自定义模型和训练算法

### 自定义损失函数

场景：数据集在删除/修复异常值后仍然有噪声。均方误差可能会对大误差惩罚太多，而导致模型不精确。平均绝对误差不会对异常值惩罚太多，但是训练可能需要一段时间才能收敛，并且训练后的模型可能不太精确。这可能是使用Huber损失而不是MSE的好时机。Huber损失已在Keras中实现（只需使用tf.keras.losses.Huber类的一个实例），但假设它不存在。要实现它，只需创建一个函数，将标签和模型的预测结果作为参数，并使用TensorFlow操作来计算包含所有损失的张量（每个样本一个）

```python
def huber_fn(y_true, y_pred):
    # y_true: (m,), Tensor
    # y_pred: (m,), Tensor

    # 返回 (m,) 损失值 ，Tensor
    error = y_true - y_pred

    # 想从Tensorflow框架的优化功能中受益，应仅使用Tensorflow操作
    is_small_error = tf.abs(error) < 1
    squared_loss = tf.square(error) / 2
    linear_loss = tf.abs(error) - 0.5
    return tf.where(is_small_error, squared_loss, linear_loss)
```

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 3.5))
z = np.linspace(-4, 4, 200)
z_center = np.linspace(-1, 1, 200)
plt.plot(z, huber_fn(0, z), "b-", linewidth=2, label="huber($z$)")
plt.plot(z, z ** 2 / 2, "r:", linewidth=1)
plt.plot(z_center, z_center ** 2 / 2, "r", linewidth=2)
plt.plot([-1, -1], [0, huber_fn(0., -1.)], "k--")
plt.plot([1, 1], [0, huber_fn(0., 1.)], "k--")
plt.gca().axhline(y=0, color='k')
plt.gca().axvline(x=0, color='k')
plt.text(2.1, 3.5, r"$\frac{1}{2}z^2$", color="r", fontsize=15)
plt.text(3.0, 2.2, r"$|z| - \frac{1}{2}$", color="b", fontsize=15)
plt.axis([-4, 4, 0, 4])
plt.grid(True)
plt.xlabel("$z$")
plt.legend(fontsize=14)
plt.title("Huber loss", fontsize=14)
plt.show()
```

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target.reshape(-1, 1), random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)
X_test_scaled = scaler.transform(X_test)

input_shape = X_train.shape[1:]

tf.keras.utils.set_random_seed(42)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
                          input_shape=input_shape),
    tf.keras.layers.Dense(1),
])
```

```python
model.compile(loss=huber_fn, optimizer="nadam", metrics=["mae"])
```

```python
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

### 保存和加载包含自定义组件的模型

保存包含自定义损失函数的模型很方便，直接保存；但加载它时，需要将名称映射到对象

```python
model.save("./models/my_model_with_a_custom_loss.keras")
```

```python
model = tf.keras.models.load_model("./models/my_model_with_a_custom_loss.keras", custom_objects={"huber_fn": huber_fn})

# 如果用@keras.utils.register_keras_serializable()修饰 huber_fn()函数，它将自动可供load_model()函数使用，无须将其包含在custom_objects字典中


model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

```python
# 想要不同的误差阈值，创建函数，让该函数创建已配置的损失函数
def create_huber(threshold=1.0):
    def huber_fn(y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < threshold
        squared_loss = tf.square(error) / 2
        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)
    return huber_fn
```

```python
# 保存模型时，上述方式定义的阈值不会保存
# 创建tf.keras.losses.Loss类的子类， 实现get_config() 自定义要保存的配置
class HuberLoss(tf.keras.losses.Loss):
    def __init__(self, threshold=1.0, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold

    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) < self.threshold
        squared_loss = tf.square(error) / 2
        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2
        return tf.where(is_small_error, squared_loss, linear_loss)

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}  # base_config | {"threshold":self.threshold}
```

```python
# {**{"one":1, "three":3}, "two":2}
{"one":1, "three":3}  | {"two": 2}
```

构造函数接受**kwargs并将它们传递给父类构造函数，该父类构造函数处理标准超参数：损失的名称和用于聚合单个实例损失的归约算法。

call()方法获取标签和预测结果，计算所有实例损失，然后将其返回。

get_config()方法返回一个字典，将每个超参数名称映射到其值。它首先调用父类的get_config()方法，然后将新的超参数添加到此字典中

```python
model.compile(loss=HuberLoss(2.), optimizer="nadam")  # 编译使用损失类的实例
model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
```

```python
model.save("./models/my_model_with_a_custom_class.keras") # 当保存模型时，Keras会调用损失实例的get_config()方法，并将配置以SavedModel格式保存
```

```python
model = tf.keras.models.load_model("./models/my_model_with_a_custom_class.keras", custom_objects={"HuberLoss": HuberLoss}) # 加载模型时，它在HuberLoss类上调用from_config()类方法：此方法由基类(Loss)实现，并创建该类的实例，将**config传递给构造函数。
model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
```

```python
model.loss
model.loss.threshold
```

### 自定义激活函数，初始化，正则化和约束

多数Keras功能，例如损失、正则化、约束、初始化、指标、激活函数、层甚至完整模型，都可以以几乎相同的方式进行自定义。在大多数情况下，只需要编写带有适当输入和输出的简单函数即可。以下是自定义激活函数（等同于tf.keras.activations.softplus()或tf.nn.softplus()）、自定义Glorot初始化（等同于tf.keras.initializers.glorot_normal()）、自定义l1正则化（等同于tf.keras.regularizers.l1(0.01)），以及确保权重均为正的自定义约束（相当于tf.keras.constraints.nonneg()或tf.nn.relu()）：

```python
def my_softplus(z):
    return tf.math.log(1.0 + tf.exp(z))


def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01*weights))

def my_positive_weights(weights):
    return tf.where(weights < 0, tf.zeros_like(weights), weights)  # tf.nn.relu(weights)

# 这些函数接收什么参数，取决于自定义函数的类型
```

```python
layer = tf.keras.layers.Dense(1,  kernel_initializer=my_glorot_initializer,
                              kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights)  # 只要参数和返回值能对上，可以正常使用这些自定义函数
```

1. 自定义出 he初始化 （normal/uniform)
2. 自定义出 l2正则化
3. 自定义 swish / relu

用自定义这些组件 去构建Dense层， 然后训练一下数据试试

```python
def my_l2_regularizer(weights):
    return tf.reduce_sum(0.01*tf.square(weights))

def my_relu(z):
    return tf.nn.relu(z)

def my_he_normal(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / shape[0])
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation=my_relu, kernel_initializer=my_he_normal, kernel_regularizer=my_l2_regularizer,input_shape=input_shape),
    layer
])

model.compile(loss=huber_fn, optimizer="nadam", metrics=["mae"])
model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))
```

激活函数将应用于此密集层的输出，其结果将传递到下一层。层的权重将使用初始化程序返回的值进行初始化。在每个训练步骤中，将权重传递给正则化函数以计算正则化损失，并将其添加到主损失中以得到用于训练的最终损失。最后，在每个训练步骤之后，将调用约束函数，并将层的权重替换为约束权重。

如果函数具有需要与模型一起保存的超参数，那么需要继承适当的类，例如tf.keras.regularizers.Regularizer、tf.keras.constraints.Constraint、tf.keras.initializers.Initializer或tf.keras.layers.Layer（适用于任何层，包括激活函数）。就像自定义损失时所做的一样，这是一个用于l1正则化的简单类，它保存了其factor超参数

```python
class MyL1Regularizer(tf.keras.regularizers.Regularizer):
    def __init__(self, factor):
        self.factor = factor

    def __call__(self, weights):
        return tf.reduce_sum(tf.abs(self.factor * weights))

    def get_config(self):
        # 不需要调用父类构造函数或get_config()方法，因为它们不是由父类定义的
        return {"factor": self.factor}

# 注意：必须为损失，层（包括激活函数）和模型实现call方法，或者为正则化，初始化和约束实现__call__()方法。
```

### 自定义指标

损失和指标在概念上不是一回事：损失（例如交叉熵）被梯度下降用来训练模型，因此它们必须是可微的（至少在评估它们的点上），并且梯度除了局部最小值不应为0。另外，即使人类不容易解释它们也没有问题。

相反，指标（例如精度）用于评估模型：它们必须更容易被解释，并且可以是不可微的，在各处也可以具有0梯度。

在大多数情况下，定义自定义指标函数与定义自定义损失函数完全相同。实际上，甚至可以将之前创建的Huber损失函数用作指标。

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
                          input_shape=input_shape),
    tf.keras.layers.Dense(1),
])


model.compile(loss="mse", optimizer="nadam", metrics=[create_huber(2.0)])
model.fit(X_train_scaled, y_train, epochs=2)
```

对于训练期间的每一个批次，Keras都会计算该指标并跟踪自轮次0以来的均值，但有的时候不想要这样的效果

例如，考虑一个二元分类器的准确率。精确率是真阳性的数量除以阳性预测结果（包括真阳性和假阳性）的数量。假设该模型在第一个批次中预测了5个阳性结果，其中4个是正确的：即80%的精确率。假设该模型在第二个批次中预测了3个阳性结果，但它们都是不正确的：准确率为0%。如果仅计算这两个准确率的均值，则可以得到40%。但是，这并不是模型在这两个批次上的准确率！实际上，在8个(5+3)阳性预测结果中，总共有4个(4+0)真阳性，因此总体准确率为50%，而不是40%。

需要的是一个对象，它应该可以跟踪真阳性的数量和假阳性的数量，并可以在请求时根据这些数据计算准确率。这正是tf.keras.metrics.Precision类所做的

```python
# 流式评价
precision = tf.keras.metrics.Precision()  # 创建了一个Precision对象，然后将其用作函数

# 将第一个批次的标签和预测结果以及第二个批次的标签和预测结果传递给它
precision([0,1,1,1,0,1,0,1], [1,1,0,1,0,1,0,1])
precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])

#
#  调用result()方法来获取指标的当前值。使用variables属性查看其变量（跟踪真阳性和假阳性的数量），并使用reset_states()方法重置这些变量：
precision.result()
precision.variables						# 返回一个列表，其中两个tf.Variable，一个记录真阳性的个数，一个记录假阳性的个数
precision.reset_state()
```

==`precision`先传入的是真实值，后传入的是预测值，以此来计算精确率==

```python
precision.result()
precision.variables
```

```python
# 自定义流式指标，创建tf.keras.metrics.Metric类的子类
class HuberMetric(tf.keras.metrics.Metric):
    def __init__(self, threshold=1.0, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)


        self.total = self.add_weight(name="total", initializer="zeros") # tf.Variable(0, dtype=tf.float32)
        self.count = self.add_weight(name="count", initializer="zeros")

    def update_state(self, y_true, y_pred, sample_weight=None):
        sample_metrics = self.huber_fn(y_true, y_pred)
        self.total.assign_add(tf.reduce_sum(sample_metrics))  # self.total = self.total + tf.reduce_sum(sample_metrics)
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32)) # self.count = self.count + len(y_true)

    def result(self):
        # return tf.divide(self.total, self.count)
        return self.total / self.count

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}


```

```python
class MyPrecision(tf.keras.metrics.Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self.true_positives = self.add_weight(name="true_positives", initializer="zeros")
        self.false_positives = self.add_weight(name="false_positives", initializer="zeros")

    def update_state(self, y_true, y_pred):
        total_pred_positives = tf.cast(tf.reduce_sum(y_pred), tf.float32)
        true_positives = tf.reduce_sum(tf.multiply(tf.cast(y_true, tf.float32), tf.cast(y_pred, tf.float32)))
        self.true_positives.assign_add(true_positives)
        self.false_positives.assign_add(total_pred_positives - true_positives)

    def result(self):
        if tf.equal(self.true_positives + self.false_positives, 0):
            return self.true_positives
        return self.true_positives / (self.true_positives + self.false_positives)
```

```python
m = MyPrecision()

m(tf.constant([0,1,1,1,0,1,0,1]), tf.constant([1,1,0,1,0,1,0,1]))
m([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
```

- 构造函数使用add_weight()方法创建用于跟踪多个批次的指标状态所需的变量，在本例中，这些变量包括所有Huber损失的总和(total)以及到目前为止看到的实例数(count)。
- 也可以手动创建变量。Keras会跟踪任何设置为属性的tf.Variable（一般而言，指任何“可跟踪”的对象，例如层或模型）。
- 当使用此类的实例作为函数时（像对Precision对象所做的那样），将调用update_state()方法。给定一个批次的标签和预测结果，它会更新变量。
- result()方法计算并返回最终结果，在本例中为所有实例的平均Huber损失。当使用指标作为函数时，首先调用update_state()方法，然后调用result()方法，并返回其输出。
- 还实现了get_config()方法来确保阈值与模型一起被保存。
- reset_states()方法的默认实现将所有变量重置为0.0（但是可以根据需要覆盖它）。

当使用简单的函数定义指标时，Keras会自动为每个批次调用该指标，它会跟踪每个轮次的均值，因此HuberMetric类的唯一好处是保存阈值。

但是，某些指标（如精确率）不能简单地按批次平均：在这些情况下，除了实现流式指标之外，别无选择。

Keras的可跟踪机制：keras会自动收集自定义类（比如层/模型）的可跟踪对象（比如变量），放进父类的 变量列表（layer.trainable_variables / layer.non_trainable_variables）。

这样的作用是：

1. 参数能被自动训练

	* 当你调用 `model.compile(..., optimizer=...)` 并训练时，优化器会去找 `model.trainable_variables`。
	* 因为 Keras 已经把子层的变量挂到父层上，所以优化器也能“看到”子层的权重并更新它们。
	  否则，子层的参数就不会被训练。

2. 保存 / 加载模型时参数不会丢

	* 当调用 `model.save()` 或 `model.get_weights()` 时，Keras 会把所有层（包括嵌套子层）的参数序列化。  序列化：内存的对象 转成 可被被保存到磁盘/数据库/网络传输的二进制形式
	* 这依赖于刚才那个“变量列表”，否则模型恢复时，子层权重可能不在里面。

3. 方便层级管理

	* 可以直接访问父层的 `.variables` 或 `.summary()`，会自动列出子层的参数。
	* 这对调试和可视化很方便。

**总结**：
“添加到该层的变量列表里”就是保证 优化器能更新它们，模型能保存/恢复它们，用户能统一管理它们 —— 这是 Keras 的“可跟踪机制”。

```python
m = HuberMetric(2.)
#  squared_loss = tf.square(error) / 2
#  linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2

# total = 2 * |10 - 2| - 2²/2 = 14
# count = 1
# result = 14 / 1 = 14
m(tf.constant([[2.]]), tf.constant([[10.]]))
```

```python
# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21
# count = count + 2 = 3
# result = total / count = 21 / 3 = 7
m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))
```

```python
m.result()
m.variables
#
m.reset_state()
m.variables
```

```python
# 检查自定义指标正常运行
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
                          input_shape=input_shape),
    tf.keras.layers.Dense(1),
])
```

```python
model.compile(loss=create_huber(2.0), optimizer="nadam",
              metrics=[HuberMetric(2.0)])
model.fit(X_train_scaled, y_train, epochs=2)
```

```python
model.save("./models/my_model_with_a_custom_metric.keras")
```

```python
model = tf.keras.models.load_model(
    "./models/my_model_with_a_custom_metric.keras",
    custom_objects={
        "huber_fn": create_huber(2.0),
        "HuberMetric": HuberMetric
    }
)
```

```python
model.fit(X_train_scaled, y_train, epochs=2)
```

```python
# 也可以这样自定义指标，这个类对形状处理更好，支持样本权重
class HuberMetric(tf.keras.metrics.Mean):
    def __init__(self, threshold=1.0, name='HuberMetric', dtype=None):
        self.threshold = threshold
        self.huber_fn = create_huber(threshold)
        super().__init__(name=name, dtype=dtype)

    def update_state(self, y_true, y_pred, sample_weight=None):
        metric = self.huber_fn(y_true, y_pred)
        super(HuberMetric, self).update_state(metric, sample_weight)

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "threshold": self.threshold}
```

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
                          input_shape=input_shape),
    tf.keras.layers.Dense(1),
])
```

```python
model.compile(loss=tf.keras.losses.Huber(2.0), optimizer="nadam",
              weighted_metrics=[HuberMetric(2.0)])

sample_weight = np.random.rand(len(y_train))
print(sample_weight)
history = model.fit(X_train_scaled, y_train, epochs=2,
                    sample_weight=sample_weight)
```

```python
print(history.history["loss"][0],
 history.history["HuberMetric"][0] * sample_weight.mean())  # todo: 为什么这两个值接近

model.save("./models/my_model_with_a_custom_metric_v2.keras")
```

```python
model = tf.keras.models.load_model("./models/my_model_with_a_custom_metric_v2.keras",
                                   custom_objects={"HuberMetric": HuberMetric})
```

```python
model.fit(X_train_scaled, y_train, epochs=2)
```

### 自定义层

想构建一个架构，其中包含TensorFlow未提供默认实现的奇异层。或者可能只是想构建一个具有重复结构的架构（其中特定的层块重复多次），将每个块视为一个层会很方便。

对于这种情况需要构建一个自定义层。

有些层没有权重，例如tf.keras.layers.Flatten或tf.keras.layers.ReLU。如果想创建一个没有任何权重的自定义层，最简单的方法是编写一个函数并将其包装在tf.keras.layers.Lambda层中。例如，以下的层将对其输入应用指数函数：

```python
exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x)) # 可以通过顺序API，函数式API或子类化API 像其他层一样使用此自定义层
```

```python
exponential_layer(tf.constant([-1., 0., 1.])) #  # 像其他层对象那样，可以当成函数使用
```

```python
# 如果要预测的值为正数且尺度差异很大（例如 0.001、10、10000），则在回归模型的输出处添加指数层会很有用。
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", input_shape=input_shape),
    tf.keras.layers.Dense(1),
    exponential_layer
])
model.compile(loss="mse", optimizer="sgd")
model.fit(X_train_scaled, y_train, epochs=5,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)

# 事实上，指数函数是Keras中的标准激活函数之一，可以用activation="exponential"

```

```python
# 自定义有状态的层（具有权重的层），需要创建tf.keras.layers.Layer类的子类

# Dense层的简化自定义实现  # 形状 输出的长度，激活函数， W， b

class MyDense(tf.keras.layers.Layer):
    def __init__(self, units, activation=None, **kwargs):
        super().__init__(**kwargs)
        self.units = units  # units 输出的数量 （这个时候还不知道输入有几个神经元）
        self.activation = tf.keras.activations.get(activation)  # 字符串 获取到 字符串对应的激活函数

    def build(self, batch_input_shape):
        self.kernel = self.add_weight(
            name="kernel", shape=[batch_input_shape[-1], self.units],
            initializer="glorot_normal")
        self.bias = self.add_weight(
            name="bias", shape=[self.units], initializer="zeros"
        )
        super().build(batch_input_shape)

    def call(self, X):
        # 执行前向传播
        return self.activation(X @ self.kernel + self.bias)

    def get_config(self):
        base_config = super().get_config()
        return {**base_config, "units": self.units, "activation": tf.keras.activations.serialize(self.activation)}
```

- 构造函数将所有超参数用作参数（在此示例中，指输入的 activation），重要的是，它还接受 **kwargs 参数。它调用父类构造函数，并将其传递给 kwargs。这负责处理标准参数，例如 input_shape、trainable 和 name，然后，它将超参数保存为属性，使得 tf.keras.activations.get() 函数将 activation 参数转换为适当的激活函数（它接受函数、标准符号，如 "relu" 或 "swish"，或者 None）。

- build() 方法的作用是通过为每个权重调用 add_weight() 方法来创建层的变量。首次使用该层时，将调用 build() 方法。在这一点上，Keras 知道该层的输入形状，并将其传给 build() 方法，这对于创建某些层而言通常是必需的。例如，我们需要知道上一层中神经元的数量，以便创建连接权重矩阵（即 "kernel"）；这对应于输入的最后一个维度的大小。在 build() 方法调用的最后（并且仅在最后），必须调用父类的 build() 方法：这告诉 Keras 这一层已被构建（它设置了 self.built=True）。

- call() 方法执行前向传播。在本示例中，我们计算输入 x 与层的核的矩阵积，加上偏置向量，并对结果应用激活函数，从而获得层的输出。

- get_config() 方法就像在以前的自定义类中一样。请注意，通过调用 tf.keras.activations.serialize() 保存激活函数的完整配置。


现在，可以像使用其他层一样使用 MyDense 层、

```python
model = tf.keras.Sequential([
    MyDense(30, activation="relu", input_shape=input_shape),
    MyDense(1)
])
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
model.save("./models/my_model_with_a_custom_layer.keras")
```

```python
model = tf.keras.models.load_model("./models/my_model_with_a_custom_layer.keras",
                                   custom_objects={"MyDense": MyDense})
model.fit(X_train_scaled, y_train, epochs=2,
          validation_data=(X_valid_scaled, y_valid))
```

```python
# 要创建具有多个输入的层（例如Concatenate），call()方法的参数应该是一个包含所有输入的元组。
# 要创建有多个输出的层，call()方法应该返回输出列表
class MyMultiLayer(tf.keras.layers.Layer):
    def call(self, X):
        # 两个输入，三个输出
        X1, X2 = X
        print("X1.shape: ", X1.shape ," X2.shape: ", X2.shape)  # 额外代码
        return X1 + X2, X1 * X2, X1 / X2
```

```python
# 这种层不能用不能使用Sequential API（仅接受具有一共输入和一个输出的层），但可以用函数式API和子类化API
# 用 符号化的输入来测试
inputs1 = tf.keras.layers.Input(shape=[2])
inputs2 = tf.keras.layers.Input(shape=[2])
MyMultiLayer()((inputs1, inputs2))
```

```python
# 具体数值测试
X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]])
MyMultiLayer()((X1, X2))
```

```python
class CustomConcat(tf.keras.layers.Layer):
    def __init__(self, axis=-1, **kwargs):
        super().__init__(**kwargs)
        self.axis = axis

    def call(self, X):
        return tf.concat(X, axis=self.axis)
```

```python
# X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]])
# CustomConcat()((X1, X2))

input_ = tf.keras.layers.Input(shape=input_shape)
hidden1 = tf.keras.layers.Dense(30, activation="relu")(input_)
hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
concat = CustomConcat()((input_, hidden2))
output = tf.keras.layers.Dense(1)(concat)

model = tf.keras.Model(inputs=[input_], outputs=[output])
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=3,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

如果层在训练和测试期间需要不同的行为（例如如果使用Dropout 或 BatchNormalization层），则必须将training参数添加到call()方法并使用此参数，来决定 训练 和 不训练分别做什么

创建一个在训练期间添加高斯噪声（用于正则化）但在测试期间不执行任何操作的层（Keras具有相同功能的层是tf.keras.layers.GaussianNoise）：

```python
class MyGaussianNoise(tf.keras.layers.Layer):
    def __init__(self, stddev, **kwargs):
        super().__init__(**kwargs)
        self.stddev = stddev

    def call(self, X, training=None):
        if training:
            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
            return X + noise
        else:
            return X
```

```python
model = tf.keras.Sequential([
    MyGaussianNoise(stddev=1.0, input_shape=input_shape),
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1)
])
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=3,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

```python
model.evaluate(X_test_scaled, y_test)
```

### 自定义模型

之前讨论过子类化API，直接继承tf.keras.Model类，在构造函数中创建层和变量，并实现call()方法来自定义模型执行的操作，现在构建如下模型：

![自定义ResidualBlock层的模型](./images/tensorflow/p3.png)

输入经过第一个密集层，之后经过由两个密集层和加法运算（残差块将其输入与输出相加）组成的残差块（ResidualBlock层），然后经过相同的残差块3次或者更多次，再然后通过第二个残差块，最终结果通过密集输出层。

该模型没有多大意义：这只是一个示例，它说明可以轻松构建所需的任何模型，即使是包含循环和跳过连接的模型

注意：循环 1 个 ResidualBlock 3 次，就是把同一个 ResidualBlock 层在前向传播时重复调用 3 次。等价于用 相同的一组参数（权重共享） 对输入数据依次做 3 次残差映射。参数只有 1 个残差块的参数；放置了 3 个独立的 ResidualBlock 层，它们各自有 不同的参数。

```python
# 要实现此模型，首先创建一个ResidualBlock层
class ResidualBlock(tf.keras.layers.Layer):
    def __init__(self, n_layers, n_neurons, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [tf.keras.layers.Dense(n_neurons, activation="relu", kernel_initializer="he_normal") for _ in range(n_layers)]

    def call(self, inputs):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        return inputs + Z

# Keras会自动检测到hidden属性，该属性包含可跟踪对象（在这个示例中是层），因此它们的变量会自动添加到该层的变量列表中
```

```python
class ResidualRegressor(tf.keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden1 = tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal")

        self.block1 = ResidualBlock(2, 30)
        self.block2 = ResidualBlock(2, 30)
        self.out = tf.keras.layers.Dense(output_dim)

    def call(self, inputs):
        Z = self.hidden1(inputs)
        for _ in range(1 + 3):
            Z = self.block1(Z)
        Z = self.block2(Z)
        return self.out(Z)
```

构造函数中创建层，并在call()方法中使用它们。然后，可以像使用其他模型一样使用此模型（对其进行编译、拟合、评估并使用它进行预测）

如果还希望能够使用save()方法保存模型并使用tf.keras.models.load_model()函数加载模型，则必须在ResidualBlock类和ResidualRegressor类中都实现get_config()方法。

另外，可以使用save_weights()和load_weights()方法保存并加载权重。Model类是Layer类的子类，因此可以像定义和使用层一样定义并使用模型。

但是模型具有一些额外的功能，包括compile()、fit()、evaluate()和predict()方法（以及一些变体）以及get_layer()方法（可以按名称或按索引返回模型的任何层）和save()方法（支持tf.keras.models.load_model()和tf.keras.models.clone_model()）。

既然模型提供的功能远不止层，为什么不将每个层都定义为模型？从技术上讲，这是可以的，但是通常可以轻松地将模型的内部组件（即层或可重复使用的层块）与模型本身（即要训练的对象）区分开来。前者应继承Layer类，而后者应继承Model类。

```python
model = ResidualRegressor(1)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=2)
score = model.evaluate(X_test_scaled, y_test)
```

```python
model.summary()

# tf.keras.utils.plot_model(model) 画不出 Model子类的结构，因为是靠代码决定的模型结构
```

```python
model.save("./models/my_custom_model.keras")
```

```python
model = tf.keras.models.load_model(
    "./models/my_custom_model.keras",
    custom_objects={"ResidualRegressor": ResidualRegressor}
)
history = model.fit(X_train_scaled, y_train, epochs=2)
model.predict(X_test_scaled[:3])
```

```python
# 这个模型也可以用顺序API实现
block1 = ResidualBlock(2, 30)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu",
                          kernel_initializer="he_normal"),
    block1, block1, block1, block1,
    ResidualBlock(2, 30),
    tf.keras.layers.Dense(1)
])
```

```python
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=2)
```

```python
model.summary()
```

### 基于模型内部数据的损失和指标

可以在模型里面自己定义一些额外的损失函数，而不仅仅是依赖训练目标（比如预测的值和真实值的误差）。Keras 提供了 `add_loss()`，让你把“自定义损失”加进去。

例子：

* 搭了一个多层感知机（MLP），有 5 层隐藏层和 1 个输出层。
* 在比较高的一层隐藏层上，你再接一个“辅助输出”，这个输出尝试把输入数据**重建**回来。
* 然后计算“输入”和“重建结果”的均方误差（MSE）。这个就是所谓的“重建损失”。

这样一来，训练时的总损失 = 主任务的损失（比如回归任务的 MSE） + 重建损失。

这么做的意义：

* 让模型在学习预测任务的同时，还要“保留”输入的信息。
* 虽然这些信息可能和预测目标没直接关系，但保留下来往往能提升模型的泛化能力（算是一种正则化方式）。

另外，Keras 还提供了 `add_metric()` 方法，可以顺便把自己定义的指标（比如重建误差）加进来，这样在训练日志里就能直接看到。

总结：**就是在训练时，强行让模型一边做主任务，一边顺手做一个“输入重建”副任务，把它的损失加到总损失里，以此帮助模型学得更稳健。**

```python
class ReconstructingRegressor(tf.keras.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [tf.keras.layers.Dense(30, activation="relu",
                                             kernel_initializer="he_normal")
                       for _ in range(5)]
        self.out = tf.keras.layers.Dense(output_dim)


        self.reconstruction_mean = tf.keras.metrics.Mean(name="reconstruction_error")   # 评价指标

    def build(self, batch_input_shape):
        # build会告诉模型，输入的形状是什么
        n_inputs = batch_input_shape[-1]

        self.reconstruct = tf.keras.layers.Dense(n_inputs)    # 隐藏层 之后 跟一个重建层，重建层神经元数量 肯定要和 模型的 输入数量的一致的

    def call(self, inputs, training=None):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        reconstruction = self.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.05 * recon_loss)

        if training:
            result = self.reconstruction_mean(recon_loss)
            self.add_metric(result)


        return self.out(Z)
```

==小结：总体来说分为继承自`tf.keras.layers.Layer`与`tf.keras.Model`两种，`tf.keras.layers.Layer`在返回时返回的时该层的输出的tensor，此种tensor是可以传入training关键字参数来区分是否经过dropout操作，而在`tf.keras.Model`中要将模型的架构进行连接，最后返回输出的tensor==

```python
model = ReconstructingRegressor(1)
model.compile(loss="mse", optimizer="nadam")
history = model.fit(X_train_scaled, y_train, epochs=5)
y_pred = model.predict(X_test_scaled)
```

### 使用自动微分计算梯度

自定义训练循环本身之前，了解一下TensorFlow中自动计算梯度

```python
def f(w1, w2):
    return 3*w1**2 + 2*w1*w2
```

如果了解微积分，则可以通过分析发现该函数关于w1的偏导数为6*w1+2*w2，关于w2的偏导数是2*w1。例如，在点(w1，w2)=(5，3)处，这些偏导数分别等于36和10，因此此点的梯度向量为(36，10)。但是，如果这是一个神经网络，则该函数将更加复杂，通常具有数以万计的参数，并且通过手动分析找到偏导数几乎是不可能完成的任务。一种解决方案可能是，通过在调整相应参数时测量函数输出的变化来计算每个偏导数的近似值：

```python
w1, w2 = 5, 3
eps = 1e-6
(f(w1 + eps, w2) - f(w1, w2)) / eps
(f(w1, w2 + eps) - f(w1, w2)) / eps
```

这只是一个近似值，更重要的是，每个参数至少要调用一次f()（不是两次，因为只计算一次f(w1，w2)）。每个参数至少需要调用f()一次，这种方法对于大型神经网络来说很棘手。因此，应该使用反向模式自动微分, 上面函数可以构造出如下计算图：

![函数f的计算图](./images/tensorflow/p4.png)

按 **反向自动微分（reverse-mode AD** 写出这张图的求导公式 (可以不理解细节，重点是感受有了计算图可以用程序化 + 高效运行的方式完成任何函数的导数计算过程）。记号
$\bar{x}\equiv \partial f/\partial x$（adjoint）。初始化 $\bar f=1$，其余为 0，并按计算图反向拓扑次序累计 “+=”。

**前向定义**

$$
\begin{aligned}
v_1 &= w_1\cdot w_1 \\
v_2 &= w_1\cdot w_2 \\
v_3 &= 3\cdot v_1 \\
v_4 &= 2\cdot v_2 \\
f   &= v_3+v_4
\end{aligned}
$$

**局部梯度规则**

* 加法 $z=x+y$: $\bar x += \bar z,\ \bar y += \bar z$
* 标量乘 $z=cx$: $\bar x += c\,\bar z$
* 乘法 $z=xy$: $\bar x += y\,\bar z,\ \bar y += x\,\bar z$
* 平方 $z=x^2$（也可看作乘法的特例）: $\bar x += 2x\,\bar z$

**反向传播**

$$
\begin{aligned}
&\bar v_3 += \bar f \qquad\qquad\ \ (\text{from } f=v_3+v_4)\\
&\bar v_4 += \bar f \\[2mm]
&\bar v_1 += 3\,\bar v_3 \qquad\ (\text{from } v_3=3v_1)\\
&\bar v_2 += 2\,\bar v_4 \qquad\ (\text{from } v_4=2v_2)\\[2mm]
&\bar w_1 += 2w_1\,\bar v_1 \qquad(\text{from } v_1=w_1^2)\\
&\bar w_1 += w_2\,\bar v_2 \qquad\ (\text{from } v_2=w_1w_2)\\
&\bar w_2 += w_1\,\bar v_2 \qquad\ (\text{from } v_2=w_1w_2)
\end{aligned}
$$

代入 $\bar f=1$ 可得最终梯度：

$$
\boxed{\frac{\partial f}{\partial w_1}=6w_1+2w_2},\qquad
\boxed{\frac{\partial f}{\partial w_2}=2w_1}.
$$

（若也对常数求梯度：$\bar 3 = v_1,\ \bar 2 = v_2$，通常优化时不需要。）

(w1,w2) = log(w1) + w1*w2 - sin(w2)
w1,w2 = 2 ,5

1. 自己套公式验证一下，w1， w2导数值会是什么
2. 画一下这个函数的计算图，试一下能不能反向自动微分算出来，f关于w1，和w2的导数
3. tf.GradientTape验证一下，最终结果

```python
import tensorflow as tf
w1 = tf.Variable(2.)
w2 = tf.Variable(5.)
def f_(w1, w2):
    return tf.math.log(w1) + w1 * w2 - tf.math.sin(w2)

with tf.GradientTape() as tape:
    # GradientTape 会开始“录制”接下来的运算轨迹：
    # 哪些张量被用来计算
    # 做了哪些操作（加减乘除、平方，根号，矩阵乘法、激活函数……）
    z = f_(w1, w2)

gradients = tape.gradient(z, [w1, w2]) # 反向自动微分得到梯度
gradients

```

```python
2- tf.math.cos(5.)
```

```python
# Tensorflow可以调用以下代码，可以直接使用反向模式自动微分
w1, w2= tf.Variable(5.), tf.Variable(3.)
with tf.GradientTape() as tape:
    # GradientTape 会开始“录制”接下来的运算轨迹：
    # 哪些张量被用来计算
    # 做了哪些操作（加减乘除、平方，根号，矩阵乘法、激活函数……）
    z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2]) # 反向自动微分得到梯度
gradients
```

tape.gradient(...)不仅结果是正确的（准确率仅受浮点误差限制，符合数学公式）， 而且无论输入有多少变量，只要输出只有一个，这个方法只需要反向遍历一次，就能计算出所有梯度，非常高效

- with ... as ...怎么实现的
with ... as ... 是 上下文管理器语法糖。
它让你把“获取资源 → 使用 → 无论是否出错都正确释放”这件事写得简洁安全。

```python
# with EXPR as VAR:   BODY
mgr = EXPR                         # 生成上下文管理器对象
value = mgr.__enter__()            # 进入上下文
try:
    VAR = value
    BODY                           # 这里可能抛异常
except BaseException as e:
    # __exit__ 返回 True 表示把异常吞掉；False/None 表示继续抛出
    if not mgr.__exit__(type(e), e, e.__traceback__):
        raise
else:
    mgr.__exit__(None, None, None) # 正常结束
```

把 with 块看作一个受控的环境：进入时建立不变式（打开文件、加锁、开始记录…），离开时无条件恢复（关闭、解锁、停止记录…）。

```python
# 1) 内置例子：文件
# with open("data.txt", "w") as f:
#     f.write("hello")   # 出错也会自动关闭文件

# 2) TensorFlow：GradientTape（开始/停止“记录”）
# import tensorflow as tf
# x = tf.Variable(2.0)
# with tf.GradientTape() as tape:   # __enter__ 返回 tape
#     y = 3 * x**2
# dy_dx = tape.gradient(y, x)

import time

class Timer:
    def __enter__(self):
        self.t0 = time.perf_counter()
        return self               # as t -> t 就是这个对象

    def __exit__(self, exc_type, exc, tb):
        self.dt = time.perf_counter() - self.t0
        print(f"耗时 {self.dt:.3f}s")
        return False              # 不吞异常


with Timer() as t:
    # 做些事...
    time.sleep(3)
```

为了节省内存，tf.GradientTape() 里面只放那些计算损失并对可训练参数求梯度所必需的运算，其它不需要反向微分的东西（指标统计、打印、argmax、数据记录等）要么放到 with 外面，要么在 with 里面用 tape.stop_recording() 暂停录制。

```python
# 调用tape的gradient()方法后，tape会立即被自动擦除，因此如果尝试两次调用gradient，会报错
with tf.GradientTape() as tape:
    z = f(w1, w2)

dz_dw1 = tape.gradient(z, w1)
# dz_dw2 = tape.gradient(z, w2) 报错
```

```python
# 如果需要多次调用gradient()，则必须使该tape具有持久性，并在每次使用完该tape后将其删除，释放资源
with tf.GradientTape(persistent=True) as tape:
    z = f(w1, w2)
dz_dw1 = tape.gradient(z, w1)
dz_dw2 = tape.gradient(z, w2)
del tape
```

```python
# 默认情况下，tape仅跟踪涉及变量的操作 针对变量以外的任何其他对象计算z梯度，结果为None
c1, c2 = tf.constant(5.), tf.constant(3.)
with tf.GradientTape() as tape:
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2])
gradients
```

```python
# 可以强制tape观察任何张量，以记录涉及它们的所有操作
# 然后针对这些张量计算梯度，就好像它们是变量一样

with tf.GradientTape() as tape:
    tape.watch(c1)
    tape.watch(c2)
    z = f(c1, c2)

gradients = tape.gradient(z, [c1, c2])
gradients
```

在某些情况下，watch可能有用，例如如果要实现正则化损失，从而在输入变化不大时惩罚那些变化很大的激活函数：损失将基于激活函数相对于输入的梯度而定。

由于输入不是变量，因此需要告诉tape观察它们： 比如传进网络的输入 x_batch = tf.constant(...)。 它不是“可训练参数”，只是数据，不会被 GradientTape 自动跟踪。 如果想对输入本身算梯度，需要利用tape.watch

在大多数情况下，梯度tape用来计算单个值（通常是损失）相对于一组值（通常是模型参数）的梯度。这就是反向模式自动微分有用的地方，因为它只需执行一次前向传播和一次反向传播即可一次获得所有梯度。如果尝试计算向量（例如包含多个损失的向量）的梯度，那么TensorFlow将计算向量和的梯度。因此，如果需要获取单独的梯度（例如每种损失相对于模型参数的梯度），则必须调用tape的jacobian()方法：它对向量中的每个损失执行一次反向模式自动微分（默认情况下全部并行）。它甚至可以计算二阶偏导数（即偏导数的偏导数），但实际上很少用到

```python
# if given a vector, tape.gradient() will compute the gradient of the vector's sum.
with tf.GradientTape() as tape:
    z1 = f(w1, w2 + 2.)
    z2 = f(w1, w2 + 5.)
    z3 = f(w1, w2 + 7.)

tape.gradient([z1, z2, z3], [w1, w2])
```

```python
# 和上个格子的计算结果一样
with tf.GradientTape() as tape:
    z1 = f(w1, w2 + 2.)
    z2 = f(w1, w2 + 5.)
    z3 = f(w1, w2 + 7.)
    z = z1 + z2 + z3

tape.gradient(z, [w1, w2])

```

```python
with tf.GradientTape() as tape:
    z1 = f(w1, w2 + 2.)
    z2 = f(w1, w2 + 5.)
    z3 = f(w1, w2 + 7.)
    y = tf.stack((z1,z2,z3))

tape.jacobian(y, [w1, w2])
```

```python
# 阻止梯度在神经网络的某些部分反向传播，使用tf.stop_gradient()函数
# 该函数在前向传播过程中返回其输入（比如tf.identity()), 但在反向传播期间不让梯度通过（类似tf.constant)
def f(w1, w2):
    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)

with tf.GradientTape() as tape:
    z = f(w1, w2)

gradients = tape.gradient(z, [w1, w2])
gradients
```

```python
x = tf.Variable(1e-50)
x
```

```python
# 计算梯度遇到的数值问题
x = tf.Variable(1e-50)  # 根号x 在x=10**(-50)的梯度，结果是无穷大，超过了32位浮点数可以处理的范围
with tf.GradientTape() as tape:
    z = tf.sqrt(x)

tape.gradient(z, [x])

# 解决方式：计算平方根时给x加一个小值（10**(-6))
with tf.GradientTape() as tape:
    z = tf.sqrt(x + 1e-6)

tape.gradient(z, [x])
```

```python
print(tf.math.log(1.0+tf.exp(100.0)))  # inf 无穷大
# 数值稳定版的 softplus
def my_softplus(z):
    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)
```

以下是该等式等于 log(1 + exp(z)) 的证明：

- softplus(z) = log(1 + exp(z))
- softplus(z) = log(1 + exp(z)) - log(exp(z)) + log(exp(z)) ；只是加减同一个值
- softplus(z) = log[(1 + exp(z)) / exp(z)] + log(exp(z)) ；因为 log(a) - log(b) = log(a / b)
- softplus(z) = log[(1 + exp(z)) / exp(z)] + z ；因为 log(exp(z)) = z
- softplus(z) = log[1 / exp(z) + exp(z) / exp(z)] + z ；因为 (1 + a) / b = 1 / b + a / b
- softplus(z) = log[exp(-z) + 1] + z ；因为 1 / exp(z) = exp(–z)，且 exp(z) / exp(z) = 1
- softplus(z) = softplus(–z) + z  ；log[exp(-z) + 1]就是softplus(-z)
- softplus(z) = softplus(–|z|) + max(0, z) ；如果你考虑两种情况，即 z < 0 或 z ≥ 0，会发现这是成立的

在极少数情况下，数值稳定的函数可能仍然具有数值不稳定的梯度。在这种情况下，必须告诉TensorFlow使用哪个方程计算梯度，而不是让它使用自动微分。为此，必须在定义函数时使用@tf.custom_gradient装饰器，并返回函数通常的结果和计算梯度的函数。例如，更新my_softplus()函数以返回一个数值稳定的梯度函数：

```python
@tf.custom_gradient
def my_softplus(z):
    def my_softplus_gradients(grads): # 这里的grads是反向自动微分的上一层的累积梯度
        return grads * (1 - 1/(1 + tf.exp(z)))  # softplus的稳定梯度
    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)
    return result, my_softplus_gradients
```

```python
x = tf.Variable([1000.])
with tf.GradientTape() as tape:
    z = my_softplus(x)

z, tape.gradient(z, [x])
```

练习：1. 自己实验一下@tf.custom_gradient， 看看tensorflow按自己定的导数计算  和 tensorflow本身内部计算是不是一致的

```python
@tf.custom_gradient
def my_gongshi(z):
    def my_gongshis_gradients(grads):           # 1/
        return grads * (1/(2 * tf.sqrt(z) * tf.exp(z) ))
    result = 1/tf.exp(tf.sqrt(z))
    return result, my_gongshis_gradients


```

log(1+exp(z)) 的导数是 exp(z)/(1+exp(z))。但这种形式并不稳定：对于较大的 z 值，它最终计算会导致无穷大除以无穷大，返回 NaN。但是，通过一些代数运算，你可以证明它等于 1-1/(1+exp(z))，这是稳定的。
my_softplus_gradients() 函数使用这个方案来计算梯度。请注意，此函数将接收到目前为止反向传播的梯度作为输入，向下传播到 my_soft_plus() 函数，并且根据链式法则，我们必须将它们与该函数的梯度相乘。

此外，当计算 my_softplus() 函数的梯度时，得到了正确的结果，即使对于大输入值也是如此。

现在可以计算任何函数的梯度（前提是它在计算点的点上是可微的），甚至在需要时阻止反向传播，并编写自己的梯度函数了  接下来，来看如何自定义训练循环。

### 自定义训练循环

在少数情况下，fit()方法可能不够灵活而无法满足你的需要。例如, 宽深神经网络的论文使用了两种不同的优化器：一种用于宽路径，另一种用于深路径。由于fit()方法只使用一个优化器（在编译模型时指定的优化器），因此实现该论文需要编写自己的自定义循环。

有时想编写自定义训练循环，只是为了让自己更有信心，确信它们按照自己的意图进行操作，但是编写自定义训练循环会使代码更长，更容易出错并且更难以维护。

除非正在学习或者真的需要额外的灵活性，否则应该更倾向使用fit()方法，而不是实现自己的训练循环

```python
# 创建一个模型，无需编译，因为手动处理训练循环意味着，计算损失，优化器都在循环里自己调用
l2_reg = tf.keras.regularizers.l2(0.05)
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal", kernel_regularizer=l2_reg),
    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg),
])
```

```python
# 从训练集中随机采样一批实例
# for i in range(总轮次）
#    循环这个轮次下每个批次：需要从样本 提取批次的步骤 （
#        .
def random_batch(X,y, batch_size=32):
    idx = np.random.randint(len(X), size=batch_size)
    return X[idx], y[idx]

# 定义一个函数，以显示训练状态，包括步数，步总数，从轮次开始以来的平均损失
def print_status_bar(step, total, loss, metrics=None):
    metrics = " - ".join([f"{m.name}: {m.result():.4f}" for m in [loss] + (metrics or [])])
    end = "" if step < total else "\n"

    print(f"\r{step}/{total} - " + metrics, end=end)   # \r和end=""配合确保状态栏始终打印在同一行上
```

```python
m = 10000
np.random.randint(m, size=32)
```

```python
X_batch, y_batch = random_batch(X_train_scaled, y_train)
y_pred = model(X_batch, training=True)

loss_fn = tf.keras.losses.MeanSquaredError()
loss_fn(y_batch,y_pred)
```

```python
tf.reduce_mean(loss_fn(y_batch,y_pred))
```

```python
# 定义一些超参数，选择优化器，损失函数和指标（MAE）
n_epochs = 5
batch_size = 32
n_steps = len(X_train) // batch_size

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()
mean_loss = tf.keras.metrics.Mean(name="mean_loss")
metrics = [tf.keras.metrics.MeanAbsoluteError()]

# 构建自定义循环
for epoch in range(1, n_epochs + 1):
    print(f"Epoch {epoch}/{n_epochs}")
    for step in range(1, n_steps + 1):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)

        with tf.GradientTape() as tape:
            y_pred = model(X_batch, training=True) #  relu(X@W1+b1) @ W2 + b2 # model.__call__() -> model.call()
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  # tf.reduce_mean可以不需要

            # tf.add_n([main_loss, 其余的损失（正则化，layer自己加的）])  ->  main_loss + 其余的损失  -> 最终的损失
            loss = tf.add_n([main_loss] + model.losses)  # model.losses 可以把所有的损失 汇总一个python列表里

        gradients = tape.gradient(loss, model.trainable_variables)  # 计算 损失 关于 所有可训练的参数的梯度 （内部用反向微分）

        optimizer.apply_gradients(zip(gradients, model.trainable_variables))  #   [(损失关于可训练参数1的梯度,可训练的参数1）, ... (损失关于可训练参数n的梯度,可训练的参数n)]

        # 如果想给模型添加权重约束（kernel_constraint/bias_constraint) 在apply_gradients()之后立即应用这些约束
        for variable in model.variables:
            if variable.constraint is not None:
                variable.assign(variable.constraint(variable))

        mean_loss(loss)  # 这个批次的损失传给mean_loss, 返回这个轮次的平均损失
        for metric in metrics:
            metric(y_batch, y_pred)
        print_status_bar(step, n_steps, mean_loss, metrics)

    for metric in [mean_loss] + metrics:
        metric.reset_states()  # reset_states(): 下个轮次，评价指标从头统计
```

- 创建了两个嵌套循环：一个用于轮次，另一个用于轮次内的批处理。

- 然后从训练集中抽取一个随机批次。

- 在 tf.GradientTape() 块中，我们对一个批次进行了预测（使用模型作为函数），并计算了损失：它等于主损失加其他损失（在此模型中，每层都有一个正则化损失）。由于 mean_squared_error() 函数每个实例返回一个损失，因此我们使用 tf.reduce_mean() 计算批次的平均值（如果要对每个实例应用不同的权重，则可以在这里进行操作）。正则化损失已经归约到单个标量，因此我们只需要对它们进行求和（使用 tf.add_n() 即可对具有相同形状和数据类型的多个张量求和）。

- 接下来，要 tape 针对每个可训练变量（不是所有变量！）计算损失的梯度，并用优化器来执行“梯度下降”步骤。

- 然后，更新平均损失和指标（在当前轮次内），并显示状态栏。

- 在每个轮次结束时，我们重置平均损失和指标的状态。

如果想用梯度裁剪，则设置优化器的clipnorm或clipvalue超参数；如果想对梯度做任何其他变换，只需要调用apply_gradients方法之前做就行

```python
class WideDeepModel(tf.keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.wideLayer= tf.keras.layers.Dense(1)
        self.deepLayer = [tf.keras.layers.Dense(30, activation="relu",
                                             kernel_initializer="he_normal")
                       for _ in range(2)]

    def call(self, inputs, **kwargs):
        wide_output = self.wideLayer(inputs)
        Z = inputs
        for l in self.deepLayer:
            Z = l(Z)
        return wide_output + Z

```

```python
model = WideDeepModel()
model.compile(loss="mse", optimizer="nadam")
model.fit(X_train_scaled, y_train, epochs=3,
          validation_data=(X_valid_scaled, y_valid))
model.evaluate(X_test_scaled, y_test)
```

```python
# 自定义训练的流程： 宽层用一个优化器（SGD/Adam), 深层用和宽层不一样的优化器（SGD/ADAM）

# 提示：model.wideLayer
# model.wideLayer.trainable_variables
# model.deepLayer.trainable_variables

model = WideDeepModel()

n_epochs = 5
batch_size = 32
n_steps = len(X_train) // batch_size

optimizer_wide = tf.keras.optimizers.SGD(learning_rate=0.01)
optimizer_deep = tf.keras.optimizers.Adam(learning_rate=0.01)

loss_fn = tf.keras.losses.MeanSquaredError()
mean_loss = tf.keras.metrics.Mean(name="mean_loss")
metrics = [tf.keras.metrics.MeanAbsoluteError()]


# 构建自定义循环
for epoch in range(1, n_epochs + 1):
    print(f"Epoch {epoch}/{n_epochs}")
    for step in range(1, n_steps + 1):
        X_batch, y_batch = random_batch(X_train_scaled, y_train)

        with tf.GradientTape(persistent=True) as tape:
            y_pred = model(X_batch, training=True)  # model.__call__() -> model.call()
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))  # tf.reduce_mean可以不需要

            # tf.add_n([main_loss, 其余的损失（正则化，layer自己加的）])  ->  main_loss + 其余的损失  -> 最终的损失
            loss = tf.add_n([main_loss] + model.losses)  # model.losses 可以把所有的损失 汇总一个python列表里

        gradients_wide = tape.gradient(loss, model.wideLayer.trainable_variables)
        gradients_deep = tape.gradient(loss, model.deepLayer.trainable_variables)
        del tape

        optimizer_wide.apply_gradients(zip(gradients_wide, model.wideLayer.trainable_variables))  #   [(损失关于可训练参数1的梯度,可训练的参数1）, ... (损失关于可训练参数n的梯度,可训练的参数n)]
        optimizer_deep.apply_gradients(zip(gradients_deep, model.deepLayer.trainable_variables))

        # 如果想给模型添加权重约束（kernel_constraint/bias_constraint) 在apply_gradients()之后立即应用这些约束
        for variable in model.variables:
            if variable.constraint is not None:
                variable.assign(variable.constraint(variable))

        mean_loss(loss)  # 这个批次的损失传给mean_loss, 返回这个轮次的平均损失
        for metric in metrics:
            metric(y_batch, y_pred)
        print_status_bar(step, n_steps, mean_loss, metrics)

    for metric in [mean_loss] + metrics:
        metric.reset_states()  # reset_states(): 下个轮次，评价指标从头统计
```

## TensorFlow函数和图

```python
def cube(x):
    return x ** 3
```

```python
cube(2)
cube(tf.constant(2.0))
```

```python
tf_cube = tf.function(cube)
tf_cube # Tensorflow 函数 （TF函数）
```

tf.function修饰函数后，TensorFlow 在第一次调用时会跟踪（Tracing）每一步运算，最终把这些操作记录成一张 计算图（Graph）。

```python
@tf.function # 更常用的写法，直接换成装饰器
def tf_cube(x):
    print(f"x = {x}")
    return x ** 3

# tf_cube = tf.function(tf_cube)
```

```python
tf_cube.python_function(2) # 使用原始的python函数
```

```python
tf_cube2(tf.constant(2.0))
```

TensorFlow可以优化计算图，修剪未使用的节点，简化表达式等。准备好优化的图后，TF函数会以适当的顺序有效地执行图中的操作。因此，TF函数通常比原始的Python函数运行得更快，尤其是在执行复杂计算的情况下。

大多数时候，想利用tensorflow加速Python函数时，不需了解更多，只需将其转换为TF函数即可。

此外，如果在调用tf.function()时设置jit_compile=True，则TensorFlow将使用加速线性代数(XLA)为图编译专用内核，通常会融合多个操作。例如，如果TF函数调用tf.reduce_sum(a * b+c)，那么在没有XLA的情况下，该函数首先需要计算a * b并将结果存储在一个临时变量中，然后将c添加到该变量，最后对结果调用tf.reduce_sum()。使用XLA，整个计算被编译到一个内核中，它将一次完成tf.reduce_sum(a*b+c)计算，不使用任何临时变量。这不仅速度会更快，而且使用的RAM更少。

当编写自定义损失函数、自定义指标、自定义层或任何其他自定义函数并在Keras模型中使用它时，Keras会自动将函数转换为TF函数——无须使用tf.function()。而如果想让Keras使用XLA，只需要在调用compile()方法时设置jit_compile=True即可。

默认情况下，TF函数会为每个不同的输入形状和数据类型生成一个新图，并将其缓存以供后续调用。例如，如果调用tf_cube(tf.constant(10))，将为形状为[ ]的int32张量生成图。如果调用tf_cube(tf.constant(20))，则会重用相同的图。但是，如果随后调用tf_cube(tf.constant([10，20]))，则会为形状为[2]的int32张量生成一个新图。这就是TF函数处理多态（即变化的参数类型和形状）的方式。但是，这仅适用于张量参数：

如果将Python数值传递给TF函数，则将为每个不同的值生成一个新图，例如，调用tf_cube(10)和tf_cube(20)将生成两个图。

如果用不同的Python数值多次调用TF函数，则会生成许多图，这会降低程序运行速度并消耗大量RAM（必须删除TF函数才能释放它）

```python
result = tf_cube(tf.constant(2.0))   #  跟踪python代码
result = tf_cube(2)    #  跟踪python代码
result = tf_cube(3)   #  跟踪python代码
result = tf_cube(tf.constant([[1., 2.]])) #  跟踪python代码
result = tf_cube(tf.constant([[3., 4.], [5., 6.]])) # 跟踪python代码
```

```python
result = tf_cube(tf.constant([[7., 8.], [9., 10.]]))  # 一样的形状，没有生成图
```

```python
# 指定 特定的input_signature
@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])
def shrink(images):
    print("Tracing", images)
    return images[:, ::2, ::2]
```

```python
img_batch_1 = tf.random.uniform(shape=[100, 28, 28])
img_batch_2 = tf.random.uniform(shape=[50, 28, 28])

preprocessed_images = shrink(img_batch_1)   # 跟踪了函数
preprocessed_images = shrink(img_batch_2)   # 没有跟踪

img_batch_3 = tf.random.uniform(shape=[2, 2, 2])
# preprocessed_images = shrink(img_batch_3)   # 和input_signature不兼容， 会报错
```

### AutoGraph和跟踪

上一部分属于跟踪运算，生成计算图；但是Python 的控制流（if/for/while）不是运算符，没法像 + * 那样被“跟踪”（跟踪 __add__, __mul__), AutoGraph 直接把你python 源码翻译一遍，把 if / for / while 换成等价的 tf.cond() / tf.while_loop()。

所以 AutoGraph 在 编译阶段 做代码转换，让这些控制流能进图。

接下来，TensorFlow将此函数称为“升级”函数，但不传递参数，而是传递符号张量——没有任何实际值的张量，仅包含名称、数据类型和形状。例如，如果调用 sum_even_linear_graph(tf.constant(3.0), tf.constant(10))，TensorFlow 实际上传入的是两个符号张量：x 的 dtype 为 float32、shape 为 []；n 的 dtype 为 int32、shape 为 []。该函数将在图模式下运行，这意味着每个TensorFlow操作都会在图中添加一个节点来表示自身及其输出张量（与常规模式相对，称为eager执行或eager模式）。在图模式下，TF操作不执行任何计算,真正的数值运算发生在执行阶段。

<img alt="Tensorflow如何使用Autograph和跟踪生成图" height="500" src="./images/tensorflow/p5.png" width="500"/>

```python
# 1) 同一段逻辑：对 0..n-1 中的偶数 i，计算 s += i * x
def sum_even_linear_eager(x, n):
    s = tf.constant(0., dtype=tf.float32)
    for i in range(n):            # 纯 Python for（Eager 下逐句执行）
        if i % 2 == 0:            # 纯 Python if
            s = s + float(i) * x
    return s

@tf.function  # 2) 编译成图：会触发 AutoGraph + Tracing
def sum_even_linear_graph(x, n):
    s = tf.constant(0., dtype=tf.float32)
    # 用 tf.range 有助于避免频繁 retrace（也能让逻辑更“张量化”）
    for i in tf.range(n):         # Python for → AutoGraph → tf.while_loop
        if (i % 2) == 0:          # Python if  → AutoGraph → tf.cond
            s = s + tf.cast(i, tf.float32) * x
    return s

x = tf.constant(3.0)
n = tf.constant(10)

print("\n[Eager] 直接执行：")
print(sum_even_linear_eager(x, int(n.numpy())))  # -> 3*(0+2+4+6+8) = 3*20 = 60

print("\n[Graph] 第一次调用会 tracing（建图），随后直接执行已编译图：")
print(sum_even_linear_graph(x, n))               # 同样应为 60

# 3) 看 AutoGraph 把 Python 源码转换成了什么
print("\n[AutoGraph 转换后的函数源码片段]:")
converted_src = tf.autograph.to_code(sum_even_linear_graph.python_function)
print("\n".join(converted_src.splitlines()[:40]))  # 只看前 40 行
```

**为什么要把函数变成计算图**：

1. **算梯度（自动微分）**
	* 深度学习训练的核心是 **反向传播**。
	* 反向传播需要知道前向运算的“依赖关系”。
	* 计算图正好就是一张依赖图：每个节点有输入、有输出。
	* TensorFlow 就可以在这张图上“从输出往输入”走，自动套链式法则。 所以 **图 = 梯度计算的路线图**。
	
2. **硬件优化（加速）**
	* 图是静态的，TF 能在图级别做很多 **全局优化**：
	  - **算子融合**（多个小操作合并成一个大核，减少 GPU/TPU 调用开销）。
	  
	  - **常量折叠**（提前把常量算掉，节省运行时开销）。
	  
	  - **内存复用**（释放/重用张量内存，避免爆显存）。
	  
	  - **跨设备调度**（哪些节点跑 GPU，哪些跑 CPU/TPU）。Eager 模式下，操作是一步步立即执行，没机会做这些整体优化。
	  
	    所以 **图 = 优化器能发挥的舞台**。
	
3. **部署 & 可移植性**
	* 图是语言无关的（一个 JSON/ProtoBuf 格式）。
	* 你训练好模型后，可以把图导出：
	  - **TF Serving**（服务器上推理）。
	  - **TF Lite**（移动端/嵌入式）。
	  - **TF.js**（浏览器）。
	* 没有图，只能在 Python 环境里跑，没法跨平台部署。
	  	所以 **图 = 通用模型格式**。
4. **调试 & 可视化**

	* 图能在 **TensorBoard** 里显示：层的结构、数据流、梯度规模、耗时瓶颈。
	* 这对理解模型和性能优化非常直观。

	**变成计算图 = 为了能自动算梯度 + 方便全局优化 + 跨平台部署 + 可视化调试。**



### TF函数规则

在大多数情况下，将执行 TensorFlow 操作的 Python 函数转换为 TF 函数很简单：用 `tf.function` 装饰它或让 Keras 处理。
但是，有一些规则需要遵守：

* **避免调用外部库**
  如果调用任何外部库，包括 NumPy 基础标准库，此调用将在跟踪过程中执行，它不会成为图的一部分。
  实际上，TensorFlow 图只能包含 TensorFlow 结构（张量、运算、变量、数据集等）。
  因此，请使用 `tf.reduce_sum()` 代替 `np.sum()`，使用 `tf.sort()` 代替内置的 `sorted()` 函数。
  （除非确实希望这些代码只在跟踪过程中运行。）

* **随机数生成必须使用 TF 提供的方法**
  如果你写了一个返回 `np.random.rand()` 的 TF 函数 `f(x)`，那么随机数只会在**跟踪函数时**生成一次。

  * 要生成真正的随机数，请使用 `tf.random.uniform([])`。这样每次操作都会生成新的随机数，因为它属于图的一部分。

* **避免副作用**
  如果非 TensorFlow 代码具有副作用（例如写日志、更新 Python 计数器），不要期望每次调用 TF 函数都会发生这些副作用，因为它们只会在**跟踪**函数时执行。

* **慎用 `py_function`**
  可以在 `tf.py_function()` 操作中包装任何 Python 代码，但这样会降低性能，因为 TensorFlow 无法对其中代码做任何图优化。
  这会降低可移植性，因为该代码只能在安装了 Python 的平台上运行。

* **Python 函数要符合规则**
  可以调用其他 Python 函数，它们也必须遵循相同的规则，因为 TensorFlow 会在计算图中捕获它们的操作。
  注意：这些函数本身需要用 `@tf.function` 修饰。

* **变量必须只创建一次**
  如果在 TF 函数内部创建了变量（例如数据集或张量列表），必须确保只创建一次，否则会引发错误。
  常见做法：在 `__init__` 或 `build()` 方法中创建变量，更新时用 `assign()`，而不是重新赋值。

* **源码可用性**
  Python 源码必须可用才能用于 TensorFlow。
  如果函数定义在交互式 Python shell 中，或者没有源码（比如 `.pyc` 编译文件），生成图可能失败。

* **避免使用 Python 原生循环遍历 Dataset**
  TensorFlow 图会捕获张量级别的循环（`tf.range()`），而不会捕获 Python 原生 `for` 循环。
  如果用 Python 的 `for` 循环迭代 Dataset，则无法在图模式下被捕获，也无法在分布式环境下正常运行。（不在图里的计算，无法重现到其他机器）

* 出于性能原因，应尽可能使用向量化实现，而不是使用循环。

## 其他Tensor类型

### 字符串

是tf.string类型的常规张量。它们表示字节字符串，而不是Unicode字符串，因此如果使用Unicode字符串（常规的Python 3字符串，例如"café"）创建字符串张量，则它将自动被编码为UTF-8（例如b"caf\xc3\xa9"）。或者，你可以使用类型为tf.int32的张量来表示Unicode字符串，其中每一项都表示一个Unicode代码点（例如[99，97，102，233]）。tf.strings包（带有s）包含用于字节字符串和Unicode字符串的操作（并将它们相互转换）。重要的是要注意，tf.string是原子级的，这意味着它的长度不会出现在张量的形状中。一旦将其转换为Unicode张量（即包含Unicode代码点的tf.int32类型的张量），长度就会显示在形状中。

```python
tf.constant("hello world")
tf.constant("你好 世界")
tf.constant(b"hello world")

```

```python
u = tf.constant([ord(c) for c in "诞生于1996,梦想做说唱领袖"])
u
```

```python
b = tf.strings.unicode_encode(u, "UTF-8")
b
```

```python
tf.strings.length(b, unit="UTF8_CHAR")
```

```python
tf.strings.unicode_decode(b, "UTF-8")
```

```python
p = tf.constant(["全民制作人", "你们好", "cxk", "wzy"])
p

# tf.strings.length(p, unit="UTF8_CHAR")
# r = tf.strings.unicode_decode(p, "UTF8")
# r
```

### 张量数组（tf.TensorArray)

张量的列表，默认固定长度，可以动态增长；包含的张量必须具有相同的形状和数据类型

```python
array = tf.TensorArray(dtype=tf.float32, size=3)


array = array.write(0, tf.constant([1., 2.]))
array = array.write(1, tf.constant([3., 10.]))
array = array.write(2, tf.constant([5., 7.]))


tensor1 = array.read(1)  #  tf.constant([3., 10.]) , 输出位置清0
tensor1

array
```

```python
array.stack()
```

```python
array2 = tf.TensorArray(dtype=tf.float32, size=3, clear_after_read=False)
array2 = array2.write(0, tf.constant([1., 2.]))
array2 = array2.write(1, tf.constant([3., 10.]))
array2 = array2.write(2, tf.constant([5., 7.]))
tensor2 = array2.read(1)  # return tf.constant([3., 10.])
array2.stack()
```

```python
# 长度动态增长
array3 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
array3 = array3.write(0, tf.constant([1., 2.]))
array3 = array3.write(1, tf.constant([3., 10.]))
array3 = array3.write(2, tf.constant([5., 7.]))
tensor3 = array3.read(1)
print(tensor3)
array3.stack()
```





# tensorflow预处理与加载



加载和预处理数据是机器学习/深度学习项目的重要组成部分。使用Pandas加载和探索数据集（存储在CSV文件中），并应用Scikit-Learn的转换程序进行预处理。这些工具非常方便，可能会经常使用它们，尤其是在使用数据进行探索和实验时。

但是，在大型数据集上训练TensorFlow模型时，更适合使用TensorFlow自己的数据加载和预处理API，它称为tf.data。它能够非常高效地加载和预处理数据，使用多线程和队列并行读取多个文件，混淆和批处理样本，等等。

此外，它可以即时完成所有这些操作——它跨多个CPU内核加载和预处理下一批数据，而GPU或TPU则忙于训练当前批次的数据。tf.data API可以让你处理内存中放不下的数据集，并让你充分利用硬件资源，从而加快训练速度。

现成的tf.data API可以从文本文件（例如CSV文件）、具有固定大小记录的二进制文件以及使用TensorFlow TFRecord格式（支持不同大小记录）的二进制文件中读取数据。TFRecord是一种灵活高效的二进制格式，通常包含协议缓冲区（一种开源二进制格式）。tf.data API还支持从SQL数据库读取数据。

Keras还带有强大但易于使用的预处理层，可以将其嵌入模型中：这样，当你将模型部署到生产环境时，它将能够直接摄取原始数据，而无须添加任何额外的预处理代码。这消除了训练期间使用的预处理代码与生产环境中使用的预处理代码不匹配的风险。如果在使用不同编程语言编码的多个应用程序中部署模型，则不必多次重新实现相同的预处理代码，这也降低了不匹配的风险。这两个API可以联合使用——例如同时受益于tf.data数据加载的高效性和Keras预处理层的便利性。

首先介绍tf.data API和TFRecord格式。然后，探索Keras预处理层以及如何将它们与tf.data API结合使用。最后，将快速浏览一些对加载和预处理数据有用的相关库，例如TensorFlow Datasets和TensorFlow Hub。

## tf.data API

```python
import tensorflow as tf
```

```python
# tf.data API围绕 tf.data.Dataset
X = tf.range(10)
dataset = tf.data.Dataset.from_tensor_slices(X) # from_tensor_slices: 从数据张量创建一个数据集，
dataset
```

from_tensor_slices()函数接受一个张量并创建一个tf.data.Dataset,其元素是X沿第一个维度的所有切片

tf.data.API是一种流式API：可以非常高效地遍历数据集的元素，但该API不是为索引或切片而设计的

```python
for item in dataset:
    print(item)
```

```python
d = tf.data.Dataset.range(10)
for item in d:
    print(item)
```

数据集还可能包含张量元组或名称/张量对字典，甚至是嵌套的元组和张量字典。当对元组、字典或嵌套结构进行切片时，数据集将仅对其包含的张量进行切片，同时保留元组/字典结构。

```python
X_nested = {"a": ([1,2,3], [4,5,6],[2,3,0]), "b": [7,8,9]}
dataset = tf.data.Dataset.from_tensor_slices(X_nested)
for item in dataset:
    print(item)
```

```python
dataset = tf.data.Dataset.from_tensor_slices({
    "feature": tf.constant([1, 2, 3]),
    "label": tf.constant([10, 20, 30])
})

for element in dataset:
    print(element)
```

### 链式转换

有了数据集后，就可以通过调用其转换方法对其进行各种转换。每个方法都返回一个新的数据集，因此可以进行链式转换

数据集方法不会修改数据集，而是创建新数据集，因此请保留对这些新数据集的引用（例如使用dataset=...），否则将不会发生任何事情。

```python
# 链式转换
dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))
dataset = dataset.repeat(3).batch(7)

for item in dataset:
    print(item)
```

![数据链式转换](./images/tensorflow/p6.png)

在此示例中，首先在原始数据集上调用repeat()方法，它返回一个新数据集，该数据集将重复原始数据集的元素3次。当然，这不会将内存中的所有数据复制三遍

如果不带任何参数调用此方法，则新数据集将永远重复源数据集，因此遍历该数据集的代码必须决定何时停止。

然后，在此新数据集上调用batch()方法，并再次创建一个新的数据集。这把先前数据集的元素以7个元素为一个批次分组。

最后，遍历此最终数据集的元素。batch()方法最后输出一个大小为2而不是7的最终批次，但是如果希望它删除最终批次，可以使用drop_remainder=True调用它，使所有批次具有完全相同的大小。

```python
# 使用map转换元素
dataset = dataset.map(lambda x: x * 2)  # x是一个批次
for item in dataset:
    print(item)
```

此map()方法是将调用的对数据应用预处理的方法。有时，这可能包括非常密集的计算，例如改变图像形状或旋转图像，

因此通常需要生成多个线程来加快速度。这可以通过将num_parallel_calls参数设置为要运行的线程数或tf.data.AUTOTUNE来完成（（根据可用的CPU动态选择正确的线程数）。注意，传递给map()方法的函数必须可以转换为TF函数

```python
# 过滤数据集
dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)
for item in dataset:
    print(item)
```

```python
# 查看数据集中的几个元素
for item in dataset.take(2):
    print(item)
```

### 乱序数据

当训练集中的实例独立同分布(IID)时，梯度下降效果最佳。

确保这一点的一种简单方法是使用shuffle()方法对实例进行乱序处理。它会创建 一个新的数据集，该数据集首先将源数据集的第一项元素填充到缓冲区中。然后，无论何时要求提供一个元素，它都会从缓冲区中随机取出一个元素，并用源数据集中的新元素替换它，直到完全遍历完源数据集为止。它将继续从缓冲区中随机抽取元素直到其为空。必须指定缓冲区的大小，重要的是要使其足够大，否则乱序处理不会非常有效。

不要超出所拥有的RAM的数量，即使有足够的RAM，也不要超出数据集的大小。如果每次运行程序都需要相同的随机顺序，那么可以提供随机种子

```python
# 乱序数据
dataset = tf.data.Dataset.range(10).repeat(2)
dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)
for item in dataset:
    print(item)
```

如果在经过乱序处理的数据集上调用repeat()，则默认情况下它在每次迭代时生成一个新次序数据集。，

但如果希望在每次迭代中重用相同的顺序（例如用于测试或调试），则可以在调用shuffle()时设置reshuffle_each_iteration=False。

```python
dataset_test = tf.data.Dataset.range(10).shuffle(buffer_size=4, seed=42).repeat(2).batch(10)
for item in dataset_test:
    print(item)
```

对于内存放不下的大型数据集，这种简单的缓冲区乱序方法可能不够用，因为与数据集相比，缓冲区很小。一种解决方法是对源数据本身进行乱序处理（例如在Linux上，可以使用shuf命令打乱文本文件）。这会大大改善乱序效果。

即使源数据已经乱序了，通常也希望对其进行更彻底的乱序处理，否则在每个轮次都有重复的相同顺序，该模型最终可能会产生偏差（例如，由于源数据顺序中偶然出现了一些虚假模式）。为了进一步打乱实例，一种常见的方法是将源数据拆分为多个文件，然后在训练过程中以随机顺序读取它们。

但是，位于同一文件中的实例仍然相互接近。为避免这种情况，可以随机选择多个文件并同时读取它们，并且将它们的记录穿插在一起。还可以使用shuffle()方法添加一个乱序缓冲区。 tf.data API只需几行代码

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target.reshape(-1, 1), random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, random_state=42)
```

```python
# 把训练数据集分成20份，保存成csv文件
# 对于内存中无法容纳的非常大的数据集，通常需要先将其拆分为多个文件，然后让 TensorFlow 并行读取这些文件。
import numpy as np
from pathlib import Path

def save_to_csv_files(data, name_prefix, header=None, n_parts=10):
    housing_dir = Path() / "datasets" / "housing"
    housing_dir.mkdir(parents=True, exist_ok=True)
    filename_format = "my_{}_{:02d}.csv"

    filepaths = []
    m = len(data)
    chunks = np.array_split(np.arange(m), n_parts)
    for file_idx, row_indices in enumerate(chunks):
        part_csv = housing_dir / filename_format.format(name_prefix, file_idx)
        filepaths.append(str(part_csv))
        with open(part_csv, "w") as f:
            if header is not None:
                f.write(header)
                f.write("\n")
            for row_idx in row_indices:
                f.write(",".join([str(col) for col in data[row_idx]]))
                f.write("\n")
    return filepaths

train_data = np.c_[X_train, y_train]
valid_data = np.c_[X_valid, y_valid]
test_data = np.c_[X_test, y_test]
header_cols = housing.feature_names + ["MedianHouseValue"]
header = ",".join(header_cols)

train_filepaths = save_to_csv_files(train_data, "train", header, n_parts=20)
valid_filepaths = save_to_csv_files(valid_data, "valid", header, n_parts=10)
test_filepaths = save_to_csv_files(test_data, "test", header, n_parts=10)
```

```python
train_filepaths
```

```python
print("".join(open(train_filepaths[0]).readlines()[:4]))  # 展示文件前几行
```

```python
# 创建一个仅包含以下文件路径的数据集
# list_files()函数返回一个乱序的文件路径的数据集，不希望打乱可以设置shuffle=False
filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)
```

```python
# filepath_dataset被打成乱序
for filepath in filepath_dataset:
    print(filepath)
```

```python
# 调用interleave()方法一次读取5个文件并使它们的行交织，使用skip()方法跳过每个文件的第一行
n_readers = 5
dataset = filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
                                      cycle_length=n_readers)
```

interleave()方法将创建一个数据集，该数据集将从filepath_dataset中拉出5个文件路径，对于每个路径，它将调用提供的函数（在此示例中为lambda）来创建新的数据集（TextLineDataset）。

在此阶段总共有7个数据集：文件路径数据集、交织数据集和由交织数据集在内部创建的5个TextLineDataset。当遍历交织数据集时，它将循环遍历这5个TextLineDataset，每次读取一行，直到所有数据集都读出为止。

然后，它将从filepath_dataset获取下五个文件路径，并以相同的方式交织它们，以此类推，直到读完文件路径。为了使交织工作更好地进行，最好使文件具有相同的大小，否则最大的文件将无法参与交织。

默认情况下，interleave()不使用并行机制，它只是依次从每个文件中一次读取一行。如果想让它真正地并行读取文件，则可以将interleave()方法的num_parallel_calls参数设置为想要的线程数（map()方法也有这个参数）。也可以将其设置为tf.data.AUTOTUNE，以使TensorFlow根据可用的CPU动态选择正确的线程数

```python
# 打印随机选择的5个csv文件的第一行
for line in dataset.take(5):
    print(line)
```

### 预处理数据

数据以字符串的张量形式返回每个实例，需要做一些预处理：解析字符串+缩放数据

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
```

```python
X_mean, X_std = scaler.mean_, scaler.scale_
n_inputs = 8

def parse_csv_line(line):
    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
    fields = tf.io.decode_csv(line, record_defaults=defs)
    return tf.stack(fields[:-1]), tf.stack(fields[-1:])

def preprocess(line):
    x, y = parse_csv_line(line)
    return (x - X_mean) / X_std, y
```

- 代码假设已经预先计算了训练集中每个特征的均值和标准差。X_mean和X_std是一维张量（或NumPy数组），其中包含8个浮点数，每个输入特征一个。可以在足够大的数据集随机样本上使用Scikit-Learn StandardScaler来完成，后面改用Keras预处理层。
- parse_csv_line()函数接受一个CSV行并对其进行解析。为此，它使用tf.io.decode_csv()函数，该函数带有两个参数：第一个是要解析的行，第二个是一个包含CSV文件中每一列的默认值的数组。这个数组不仅告诉TensorFlow每列的默认值，而且告诉TensorFlow列数及其类型。在此示例中，告诉它所有特征列都是浮点数，缺失值应默认为0，还提供了一个类型为tf.float32的空数组作为最后一列（目标值）的默认值：该数组告诉TensorFlow该列包含浮点数，但没有默认值，因此如果遇到缺失值，它会引发异常。
- tf.io.decode_csv()函数返回标量张量（每列一个）的列表，但是需要返回一维张量数组。因此，在除最后一个（目标值）之外的所有张量上调用tf.stack()：这会将这些张量堆叠到一维数组中。然后，对目标值执行相同的操作，这使其成为具有单个值的一维张量数组，而不是标量张量。tf.io.decode_csv()函数已经完成，因此它返回输入特征和目标值。
- 最后，自定义preprocess()函数仅调用parse_csv_line()函数，通过减去特征均值然后除以特征标准差来缩放输入特征，并返回包含缩放特征和目标值的元组。

```python
# 测试
preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')  # preprocess函数从字节字符串转换为标准化后的张量
```

### 上述操作合并

为了使可重用，将到讨论的所有内容放到另外一个辅助函数中：它将创建并返回一个数据集，该数据集有效地从多个CSV文件中住房数据，对其进行预处理，对其进行随机乱序处理，并进行分批处理

<img alt="从多个CSV文件加载和预处理数据" height="500" src="./images/tensorflow/p7.png" width="500"/>

```python
def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,
                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,
                       batch_size=32):
    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)
    dataset = dataset.interleave(
        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),
        cycle_length=n_readers, num_parallel_calls=n_read_threads)
    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)
    return dataset.batch(batch_size).prefetch(1)
```

```python
# 数据集的前2个批次
example_set = csv_reader_dataset(train_filepaths, batch_size=3)
for X_batch, y_batch in example_set.take(2):
    print("X =", X_batch)
    print("y =", y_batch)
    print()
```

现在已经了解到如何构建有效的输入流水线，从多个文本文件加载和预处理数据。讨论了最常见的数据集方法，下面讨论prefetch（预取）的作用

### 预取

通过在自定义csv_reader_dataset()函数末尾调用prefetch(1)，正在创建一个数据集，该数据集将尽最大可能总是提前准备一个批次。换句话说，当训练算法处理一个批次时，数据集已经在准备下一批次了（例如从磁盘读取数据并对其进行预处理）。如图所示，这可以显著提高性能。

<img alt="预取可以做到当GPU处理一个批次时，CPU处理下一批次" height="500" src="./images/tensorflow/p8.png" width="500"/>

如果确保使用多线程进行数据加载和预处理（通过在调用interleave()和map()时设置num_parallel_calls），那么可以利用多个CPU内核，希望准备一个批次数据的时间比在GPU上执行一个训练步骤的时间要短一些：这样，GPU将达到几乎100%的利用率（从CPU到GPU的数据传输时间除外），并且训练会运行得更快。

如果打算购买GPU卡，它的处理能力和内存大小当然非常重要（特别是大容量的RAM对于大型计算机视觉或自然语言处理模型至关重要）。与良好性能同样重要的是GPU的内存带宽。这是它每秒可以进出RAM的数据的千兆字节数。

b如果数据集足够小，能够容纳于内存里，则可以使用数据集的cache()方法将其内容缓存到RAM中，从而显著加快训练速度。通常应该在加载和预处理数据之后，但在乱序、重复、批处理和预取之前执行此操作。这样，每个实例仅被读取和预处理一次（而不是每个轮次一次），但数据仍会在每个轮次有不同的乱序，并且仍会提前准备下一批次。 

### 在Keras中使用数据集

使用csv_reader_dataset()函数为训练集，验证集，测试集创建数据集。它们会在每个轮次进行打乱

```python
train_set = csv_reader_dataset(train_filepaths)
valid_set = csv_reader_dataset(valid_filepaths)
test_set = csv_reader_dataset(test_filepaths)
```

```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation="relu", kernel_initializer="he_normal",
                          input_shape=X_train.shape[1:]),
    tf.keras.layers.Dense(1),
])
model.compile(loss="mse", optimizer="sgd")
model.fit(train_set, validation_data=valid_set, epochs=5)   # 传递创建的训练集
```

```python
# 数据集传递给evaluate和predict方法
test_mse = model.evaluate(test_set)
new_set = test_set.take(3)
y_pred = model.predict(new_set)
```

```python
# 自定义训练 也可以用上 迭代训练集
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()

n_epochs = 5
for epoch in range(n_epochs):
    for X_batch, y_batch in train_set:
        print("\rEpoch {}/{}".format(epoch + 1, n_epochs), end="")
        with tf.GradientTape() as tape:
            y_pred = model(X_batch)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
```

```python
# 创建TF函数来训练整个轮次的模型，加快训练速度
@tf.function
def train_one_epoch(model, optimizer, loss_fn, train_set):
    for X_batch, y_batch in train_set:
        with tf.GradientTape() as tape:
            y_pred = model(X_batch)
            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
            loss = tf.add_n([main_loss] + model.losses)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()
for epoch in range(n_epochs):
    print("\rEpoch {}/{}".format(epoch + 1, n_epochs), end="")
    train_one_epoch(model, optimizer, loss_fn, train_set)
```

## TFRecord格式

之前知道了如何使用tf.data API构建强大的输入流水线，然而，到目前为止，一直在使用CSV文件，这些文件通用、简单、方便但效率不高，并且不能很好地支持大型或复杂的数据结构（如图像或音频）。接下来，看看如何改用TFRecord文件。

注意：如果很满意CSV文件（或任何其他格式），就不必使用TFRecord文件了。俗话说，如果没有破裂，就不要修理！当训练过程中的瓶颈是加载和解析数据时，TFRecord格式很有用。

TFRecord格式是TensorFlow首选的格式，用于存储大量数据并有效读取数据。这是一种非常简单的二进制格式，只包含大小不同的二进制记录序列（每条记录由一个长度、一个用于检查长度是否损坏的CRC校验和、实际数据以及最终CRC校验和组成）。可以使用tf.io.TFRecordWriter类轻松创建TFRecord文件：

```python
with tf.io.TFRecordWriter("datasets/my_data.tfrecord") as f:
    f.write(b"This is the first record")
    f.write(b"And this is the second record")
```

```python
# 使用tf.data.TFRecordDataset读取一个或多个TFRecord文件
filepaths = ["datasets/my_data.tfrecord"]
dataset = tf.data.TFRecordDataset(filepaths)
for item in dataset:
    print(item)
```

默认情况下，TFRecordDataset将一个一个地读取文件，但你可以通过向构造函数传递文件路径列表并将num_parallel_reads设置为大于1的数字，使其并行读取多个文件并交织记录。也可以使用list_files()和interleave()（就像我们之前做的那样）读取多个CSV文件。

```python
filepaths = ["my_test_{}.tfrecord".format(i) for i in range(5)]
for i, filepath in enumerate(filepaths):
    with tf.io.TFRecordWriter(filepath) as f:
        for j in range(3):
            f.write("File {} record {}".format(i, j).encode("utf-8"))

dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=3)
for item in dataset:
    print(item)
```

有时，压缩的TFRecord文件可能很有用，尤其是在需要通过网络连接加载它们时。可以通过设置options参数来创建压缩的TFRecord文件：

```python
options = tf.io.TFRecordOptions(compression_type="GZIP")
with tf.io.TFRecordWriter("my_compressed.tfrecord", options) as f:
    f.write("诞生于1996 梦想做说唱领袖".encode())
    f.write("so pretty so jiggy 没有太多讲究".encode())
    f.write("想唱跳舞的男孩 创作是他的王牌".encode())
    f.write("随时随地可以表演 生活就是我的舞台".encode())
    f.write("大家的目光像是我的兴奋剂，大家好，我是来自BBT的：王·子·异".encode())

# 当读取压缩的TFRecord文件时，需要指定压缩类型
dataset = tf.data.TFRecordDataset(["my_compressed.tfrecord"], compression_type="GZIP")
for item in dataset:
    print(item)
    print(item.numpy().decode())
```

### 协议缓冲区

即使每条记录可以使用你想要的任何二进制格式，TFRecord文件通常包含序列化的协议缓冲区（也称为protobuf）。这是一种可移植、可扩展且高效的二进制格式，由Google开发并开源。protobuf现在被广泛使用，尤其是网络传输数据/远程函数调用，它由以下方式简单定义：

```proto
syntax = "proto3";
message Person {
    string name = 1;
    int32 id = 2;
    repeated string email = 3;
}
```

写到文件里，后缀名.proto

这个protobuf定义说正在使用protobuf格式的版本3，它指定每个Person对象具有字符串类型的name，类型int32的id和零个或多个email字段（每个都是字符串类型）。数字1、2和3是字段标识符，它们用在每条记录的二进制表示形式中。在.proto文件中定义后，就可以对其进行编译。这要求protoc（protobuf编译器）生成Python（或其他语言）的访问类。请注意，通常会在TensorFlow中使用的protobuf定义已经编译好，并且它们的Python类是TensorFlow库的一部分，因此不需要使用protoc。需要知道的是如何在Python中使用protobuf访问类

```shell
# !protoc person.proto --python_out=. --descriptor_set_out=person.desc --include_imports  装了protoc可以执行这个指令
```

```python
# 成功执行了上面的指令可以运行这部分代码
from person_pb2 import Person  # import the generated access class

person = Person(name="Al", id=123, email=["a@b.com"])  # create a Person
print(person)  # display the Person


person.name  # read a field
person.name = "Alice"  # modify a field
person.email[0]  # repeated fields can be accessed like arrays
person.email.append("c@d.com")  # add an email address

serialized = person.SerializeToString()  # serialize person to a byte string
serialized

person2 = Person()  # create a new Person
person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)
```

导入由protoc生成的Person类，创建一个实例并使用它，可视化并读写一些字段，然后使用SerializeToString()方法对其进行序列化。这是准备通过网络保存或传输的二进制数据。当读取或接收此二进制数据时，可以使用ParseFromString()方法对其进行解析，然后得到序列化对象的副本。

可以将序列化的Person对象保存到TFRecord文件中，然后加载并解析它：一切都正常进行。但是，ParseFromString()不是TensorFlow的操作，所以不能在tf.data流水线的预处理函数中使用它。

但是，可以使用tf.io.decode_proto()函数，它可以解析任何protobuf，前提是为其提供protobuf定义。

在实践中，通常希望使用TensorFlow提供专用解析操作的预定义protobuf。

```python
import tensorflow as tf
person_tf = tf.io.decode_proto(
    bytes=serialized,
    message_type="Person",
    field_names=["name", "id", "email"],
    output_types=[tf.string, tf.int32, tf.string],
    descriptor_source="person.desc")

person_tf.values
```

### Tensorflow protobuf

TFRecord文件中通常使用的主要protobuf是Example protobuf，它表示数据集中的一个实例。它包含一个已命名特征的列表，其中每个特征可以是字节字符串列表、浮点数列表或整数列表。以下是protobuf的定义

```proto
syntax = "proto3";

message BytesList { repeated bytes value = 1; }
message FloatList { repeated float value = 1 [packed = true]; }
message Int64List { repeated int64 value = 1 [packed = true]; }
message Feature {
    oneof kind {
        BytesList bytes_list = 1;
        FloatList float_list = 2;
        Int64List int64_list = 3;
    }
};
message Features { map<string, Feature> feature = 1; };
message Example { Features features = 1; };
```

BytesList、FloatList和Int64List的定义非常简单。请注意，[packed=true]用于重复的数值字段，以实现更有效的编码。Feature包含BytesList或FloatList或Int64List。Features包含将特征名称映射到相应特征值的字典。最后，Example仅包含Features对象。

为什么还要定义Example，既然它只包含一个Features对象？TensorFlow的开发人员可能有一天会决定向其中添加更多字段。只要新的Example定义仍然包含features字段，具有相同的ID，它就会向后兼容。这种可扩展性是protobuf的一大特色。

```python
# 使用tf.train.Example来表示之前的person
from tensorflow.train import BytesList, FloatList, Int64List
from tensorflow.train import Feature, Features, Example

person_example = Example(
    features=Features(
        feature={
            "name": Feature(bytes_list=BytesList(value=[b"Alice"])),
            "id": Feature(int64_list=Int64List(value=[123])),
            "emails": Feature(bytes_list=BytesList(value=[b"a@b.com",
                                                   b"c@d.com"]))
        }))
```

```python
with tf.io.TFRecordWriter("my_contacts.tfrecord") as f:
    for _ in range(5):
        f.write(person_example.SerializeToString())
```

假如要改造项目组的数据管线：
1. 创建一个从当前格式（例如CSV文件）读取的转换脚本：为每个实例创建一个Exampleprotobuf，将它们序列化并将它们保存到多个TFRecord文件中。
2. 理想情况下会在此过程中对它们进行乱序处理。这需要做一些工作，所以再次确保它确实有必要（也许原本的数据流水线就可以很好地处理CSV文件）



### 加载和解析Example

要加载序列化的Example protobuf，再次使用tf.data.TFRecordDataset，并使用tf.io.parse_single_example()解析每个Example。它至少需要两个参数：一个包含序列化数据的字符串标量张量，以及每个特征的描述。这种描述是一个字典，将每个特征名称映射到表示特征形状、类型和默认值的tf.io.FixedLenFeature描述符，或者映射到仅表示类型的tf.io.VarLenFeature描述符（如果特征列表的长度可能有所不同的话，例如对于"emails"特征）。

```python
# 定义描述字典
feature_description = {
    "name": tf.io.FixedLenFeature([], tf.string, default_value=""),
    "id": tf.io.FixedLenFeature([], tf.int64, default_value=0),
    "emails": tf.io.VarLenFeature(tf.string),
}

def parse(serialized_example):
    # tf.io.parse_single_example
    # 第一个参数： 字符串的张量（字符串是序列化的 （二进制））
    # 第二个参数：特征描述（字典）
    return tf.io.parse_single_example(serialized_example, feature_description)

# 创建TFRecordDataset 并对其应用自定义解析函数，解析数据集里的每个序列化Example protobuf
dataset = tf.data.TFRecordDataset(["my_contacts.tfrecord"]).map(parse)


for parsed_example in dataset:
    # print(parsed_example["emails"].values)
    # print("*" * 100)
    print(parsed_example)
```

固定长度特征被解析为常规张量，而可变长度特征被解析为稀疏张量。可以使用tf.sparse.to_dense()将稀疏张量转换为密集张量，访问值更简单

```python
tf.sparse.to_dense(parsed_example["emails"], default_value=b"")
parsed_example["emails"].values
```

```python
# 随堂练习
import pandas as pd
file_name = "./datasets/housing/my_train_00.csv"
df = pd.read_csv(file_name)


data_dict = df.to_dict(orient="list")
data_dict

data_example = Example(
    features=Features(
        feature={
           k: Feature(float_list=FloatList(value=v)) for k, v in data_dict.items()
        }))
#
with tf.io.TFRecordWriter("train00_to_binary.tfrecord") as f:
    f.write(data_example.SerializeToString())
```

```python
feature_description = {
        k: tf.io.VarLenFeature(tf.float32) for k in data_dict
    }
# print(feature_description)

def parse(serialized_example):
    # tf.io.parse_single_example
    # 第一个参数： 字符串的张量（字符串是序列化的 （二进制））
    # 第二个参数：特征描述（字典）
    return tf.io.parse_single_example(serialized_example, feature_description)

dataset = tf.data.TFRecordDataset(["train00_to_binary.tfrecord"]).map(parse)
for parsed_example in dataset:
    for v in parsed_example.values():
        print(v.values)
```

```python
tf.sparse.to_dense(parsed_example["AveBedrms"])
```

BytesList可以包含任何二进制数据，包括序列化的对象。例如，可以使用tf.io.encode_jpeg()和JPEG格式对图像进行编码，然后将此二进制数据放入BytesList。稍后，当代码读取TFRecord时，它将先解析Example，然后它需要调用tf.io.decode_jpeg()来解析数据并获取原始图像（或者使用tf.io.decode_image()来解码BMP、GIF、JPEG或PNG图像）。

还可以使用tf.io.serialize_tensor()来序列化张量，并将生成的字节字符串放入BytesList特征中，从而将张量存储在BytesList中。稍后当解析TFRecord时，可以使用tf.io.parse_tensor()解析此数据。

```python
import matplotlib.pyplot as plt
from sklearn.datasets import load_sample_images
import tensorflow as tf
import numpy as np

img = load_sample_images()["images"][0]
img2 = load_sample_images()["images"][1]

plt.subplot(121)
plt.imshow(img)
plt.axis("off")

plt.subplot(122)
plt.imshow(img2)
plt.axis("off")
plt.show()
```

```python
img2.shape
```

```python
data = tf.io.encode_jpeg(img)
data2 = tf.io.encode_jpeg(img2)

example_with_image = Example(features=Features(feature=
    {
    "images": Feature(bytes_list=BytesList(value=[data.numpy(), data2.numpy()]))
    }))
serialized_example = example_with_image.SerializeToString()
with tf.io.TFRecordWriter("my_images.tfrecord") as f:
    f.write(serialized_example)
```

```python
feature_description = {"images": tf.io.VarLenFeature(tf.string) }

def parse(serialized_example):
    example_with_image = tf.io.parse_single_example(serialized_example,
                                                    feature_description)
    return example_with_image

dataset = tf.data.TFRecordDataset(["my_images.tfrecord"]).map(parse)


for image in dataset:
    all_imgs =  image["images"].values
    plt.subplot(121)

    img1 =  tf.io.decode_image(all_imgs[0])
    plt.imshow(img1)
    plt.axis("off")

    plt.subplot(122)
    img2 =  tf.io.decode_image(all_imgs[1])
    plt.imshow(img2)
    plt.axis("off")
    plt.show()
```

```python
# 张量序列化 + 解析
tensor = tf.constant([[0., 1.], [2., 3.], [4., 5.]])
serialized = tf.io.serialize_tensor(tensor)
serialized

tf.io.parse_tensor(serialized, out_type=tf.float32)

sparse_tensor = parsed_example["emails"]
serialized_sparse = tf.io.serialize_sparse(sparse_tensor)
serialized_sparse

BytesList(value=serialized_sparse.numpy())
```

### 使用SequenceExample protobuf处理

Example protobuf非常灵活，因此它可能足以满足大多数用例。但是，当处理列表的列表时，使用它可能有些麻烦。例如，假设要对文本文档进行分类，每个文档可以被表示为句子的列表，其中每个句子被表示为单词的列表。也许每个文档也都有一个注释列表，其中每个注释都表示为单词的列表。也可能有一些上下文数据，例如文档的作者、标题和出版日期。TensorFlow的SequenceExample protobuf是针对此类用例设计的。

以下是SequenceExample protobuf的定义：
```proto
syntax = "proto3";

message FeatureList { repeated Feature feature = 1; };
message FeatureLists { map<string, FeatureList> feature_list = 1; };
message SequenceExample {
    Features context = 1;
    FeatureLists feature_lists = 2;
};

```

SequenceExample包含一个用于上下文数据的Features对象，以及一个FeatureLists对象［包含一个或多个命名的FeatureList对象（一个名为"content"，另一个名为"comments"）］。每个FeatureList包含一个Feature对象的列表，每个对象可以是一个字节字符串列表、一个64位整数列表或一个浮点数列表（在本示例中，每个Feature代表一个句子或一个注释，可能以单词标识符列表的形式存在）。构建SequenceExample并对其进行序列化和解析的过程与Example的构建、序列化和解析过程相似，但是必须使用tf.io.parse_single_sequence_example()解析单个SequenceExample或者使用tf.io.parse_sequence_example()来批量解析。这两个函数都返回一个包含上下文特征（作为字典）和特征列表（也作为字典）的元组。如果特征列表包含大小不同的序列，则可能需要使用tf.RaggedTensor.from_sparse()将它们转换为不规则的张量

```python
from tensorflow.train import FeatureList, FeatureLists, SequenceExample

context = Features(feature={
    "author_id": Feature(int64_list=Int64List(value=[123])),
    "title": Feature(bytes_list=BytesList(value=[b"A", b"desert", b"place", b"."])),
    "pub_date": Feature(int64_list=Int64List(value=[1623, 12, 25]))
})

content = [["When", "shall", "we", "three", "meet", "again", "?"],
           ["In", "thunder", ",", "lightning", ",", "or", "in", "rain", "?"]]

comments = [["When", "the", "hurlyburly", "'s", "done", "."],
            ["When", "the", "battle", "'s", "lost", "and", "won", "."]]

def words_to_feature(words):
    return Feature(bytes_list=BytesList(value=[word.encode("utf-8")
                                               for word in words]))

content_features = [words_to_feature(sentence) for sentence in content]
comments_features = [words_to_feature(comment) for comment in comments]

sequence_example = SequenceExample(
    context=context,
    feature_lists=FeatureLists(feature_list={
        "content": FeatureList(feature=content_features),
        "comments": FeatureList(feature=comments_features)
    }))
```

```python
sequence_example
```

```python
serialized_sequence_example = sequence_example.SerializeToString()
```

```python
context_feature_descriptions = {
    "author_id": tf.io.FixedLenFeature([], tf.int64, default_value=0),
    "title": tf.io.VarLenFeature(tf.string),
    "pub_date": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),
}
sequence_feature_descriptions = {
    "content": tf.io.VarLenFeature(tf.string),
    "comments": tf.io.VarLenFeature(tf.string),
}
```

```python
parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
    serialized_sequence_example, context_feature_descriptions,
    sequence_feature_descriptions)

parsed_context, parsed_feature_lists
parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists["content"])
parsed_content
```

```python
parsed_context
```

```python
parsed_context["title"].values
```

```python
parsed_feature_lists
```

```python
print(tf.RaggedTensor.from_sparse(parsed_feature_lists["content"]))
```

## Keras预处理层

为神经网络准备数据通常需要对数值特征进行归一化，对分类特征和文本进行编码，裁剪和调整图像大小等。有几种选择：
- 预处理可以在准备训练数据文件时提前完成，使用任何工具，例如NumPy、Pandas或Scikit-Learn。需要在生产环境中应用完全相同的预处理步骤，以确保生产模型接收到与训练时相似的预处理输入。
- 可以在使用tf.data加载数据时动态预处理数据，方法是使用数据集的map()方法对数据集的每个元素应用预处理函数。同样，也需要在生产环境中应用相同的预处理步骤。
- 最后一种方法是将预处理层直接包含在模型中，这样它就可以在训练期间即时预处理所有输入数据，然后在生产环境中使用相同的预处理层。剩余部分将研究最后一种方法。

Keras提供了许多可以包含在模型中的预处理层：它们可以应用于数值特征、分类特征、图像和文本。接下来介绍数值特征和分类特征，以及基本的文本预处理和图片预处理。

### 归一化层

Keras提供了一个归一化层(Normalization)，可以使用它来标准化输入特征。可以在创建层时指定每个特征的均值和方差，或者更简单地在拟合模型之前将训练集传递给层的adapt()方法，这样层就可以在训练模型之前自行测量特征均值和方差：

```python
norm_layer = tf.keras.layers.Normalization()
model = tf.keras.models.Sequential([
    norm_layer,
    tf.keras.layers.Dense(1)
])
model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))

norm_layer.adapt(X_train)  # 计算每个特征的均值+方差
model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)
```

传递给adapt()方法的数据样本必须足够大才能代表数据集，但它不一定是完整的训练集：对于Normalization层，从训练集中随机抽取的几百个实例通常足以很好地估计特征均值和方差。

由于在模型中包含了Normalization层，因此可以将这个模型部署到生产环境中，而不必再次担心归一化：模型会处理它。这种方法完全消除了预处理不匹配的风险，当人们试图为训练环境和生产环境维护不同的预处理代码但更新一个而忘记更新另一个时，就会发生预处理不匹配的情况。然后，生产环境模型最终会收到以其不希望的方式预处理的数据。如果幸运的话，会得到一个明显的错误。否则，模型的精度就会悄悄下降。

将预处理层直接包含在模型中既好又直接，但它会减慢训练速度：事实上，由于预处理是在训练过程中即时执行的，因此每个轮次都会发生一次。最好在训练前对整个训练集一次性进行归一化。为此，可以以独立的方式使用Normalization层

```python
norm_layer = tf.keras.layers.Normalization()
norm_layer.adapt(X_train)
X_train_scaled = norm_layer(X_train)
X_valid_scaled = norm_layer(X_valid)
```

```python
# 这回训练速度应该加快
model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])
model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))
model.fit(X_train_scaled, y_train, epochs=5,
          validation_data=(X_valid_scaled, y_valid))
```

```python
# 模型部署时，可以包装自适应后（adapt后）的Normalization层和刚刚训练的模型

final_model = tf.keras.Sequential([norm_layer, model])
X_new = X_test[:3]
y_pred = final_model(X_new)
```

此外，Keras预处理层与tf.data API配合得很好。例如，可以将tf.data.Dataset传递给预处理层的adapt()方法，也可以使用数据集的map()方法将Keras预处理层应用于tf.data.Dataset。例如，以下是如何将自适应(adapt后）Normalization层应用于数据集中每个批次的输入特征： 

```python
dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(5)
dataset = dataset.map(lambda X, y: (norm_layer(X), y))
list(dataset.take(1))
```

```python
# 也可以随时编写自定义的层  来做预处理
class MyNormalization(tf.keras.layers.Layer):
    def adapt(self, X):
        self.mean_ = np.mean(X, axis=0, keepdims=True)
        self.std_ = np.std(X, axis=0, keepdims=True)

    def call(self, inputs):
        eps = tf.keras.backend.epsilon()  # 获取很小的epsilon
        return (inputs - self.mean_) / (self.std_ + eps)


my_norm_layer = MyNormalization()
my_norm_layer.adapt(X_train)
X_train_scaled = my_norm_layer(X_train)
```

### 离散化层

离散化层的目标是通过将值范围（称为bin）映射到类别来将数值特征转换为分类特征。这有时对于具有多峰分布的特征或与目标具有高度非线性关系的特征很有用。例如，以下代码将一个数值特征age映射到三个类别［小于18岁、18～50岁（不包括）和50岁及以上］：

```python
age = tf.constant([[10.],
                   [93.],
                   [57.],
                   [18.],
                   [37.],
                   [5.]])

discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])
age_categories = discretize_layer(age)
age_categories
```

```python
discretize_layer = tf.keras.layers.Discretization(num_bins=3) # 改为提供所需的bin数量，然后调用层的adapt()方法，根据值的百分位数找到合适的边界。 bins=3， -> 0 33 66 1
discretize_layer.adapt(age)
age_categories = discretize_layer(age)
age_categories
```

### 类别编码层

tf.keras.layers.CategoryEncoding,类别编码是将原本的数据展成稀疏的one_hot矩阵，将唯一值展成列，再将其排序，每一个列下对应的是原矩阵在当前维度的这个唯一值个数，所以tf.keras.layers.CategoryEncoding,可以接收的不止是一维的数据，也适配多维，超参数num_tokens代表的是在展成系数矩阵时最多的唯一值（列）数

```python
onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)
onehot_layer(age_categories)
```

```python
# 一次对多个分类特征进行编码:输出张量为任何输入特征中存在的每个类别中包含一个1 (多热编码)

# [1 1 0]
# [0 0 1]
# [1 0 1]
two_age_categories = np.array([[1, 1],
                               [2,2],
                               [2,1]])
onehot_layer(two_age_categories)
```

```python
# 想了解每个类别的出现次数,可以在创建CategoryEncoding层设置output_mode = "count"
onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3, output_mode="count")  # 计数编码
onehot_layer(two_age_categories)
```

```python
# 不知道类别是来自第几个分类, 多热编码和计数编码都会丢失信息([0,1]和[1,0]都被编码成[1.,1.,0.]

onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)
onehot_layer(two_age_categories + [0, 3])  # 给第二个分类+3,调整类别标识符使它们不重叠
```

```python
onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3,
                                                output_mode="one_hot")
tf.keras.layers.concatenate([onehot_layer(cat)
                             for cat in tf.transpose(two_age_categories)])
```

```python
tf.keras.layers.Flatten()(tf.one_hot(two_age_categories, depth=3))
```

### StringLookup层

```python
# 用于分类文本特征

cities = ["Auckland", "Paris", "Paris", "San Francisco"]
str_lookup_layer = tf.keras.layers.StringLookup()  # 创建StringLookup层
str_lookup_layer.adapt(cities)  # 适应数据

str_lookup_layer([["Paris"], ["Auckland"], ["Auckland"], ["Beijing"]])  # 模拟测试数据
# str_lookup_layer(cities)  # 使用该层对文本编码, 已知类别按照从最频繁出现的类别 到最不频繁出现的类别    从1开始编号
```

```python
# 在创建StringLookup层时设置output_mode = "one_hot", 它将为每个列别输出一个独热向量,而部署一个整数
str_lookup_layer = tf.keras.layers.StringLookup(output_mode="one_hot")
str_lookup_layer.adapt(cities)
str_lookup_layer([["Paris"], ["Auckland"], ["Auckland"], ["Beijing"], ["Tianjin"]])
```

如果训练集非常大，将层调整为适应(adapt)训练集的随机子集可能会很方便。在这种情况下，层的adapt()方法可能会遗漏一些罕见的类别。默认情况下，它会将它们全部映射到类别0，使模型无法区分它们。为了降低这种风险（同时仍然只在训练集的一个子集上调整），可以将num_oov_indices设置为大于1的整数。这是要使用的词汇表外桶[out-of-vocabulary(OOV)buckets]的数量：每个未知类别将伪随机映射到OOV桶之一，使用哈希函数对OOV桶的数量取模（取余数）。这将使模型至少能够区分一些罕见的类别。例如：

```python
str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)
str_lookup_layer.adapt(cities)


str_lookup_layer([["Paris"], ["Auckland"], ["Foo"], ["Bar"], ["Baz"]])
```

由于有5个OOV桶，第一个已知类别的ID现在是5("Paris")。但是"Foo"、"Bar"和"Baz"是未知的，所以它们都被映射到某个OOV桶中。"Bar"有自己的专用桶（ID为3），但是"Foo"和"Baz"恰好映射到同一个桶（ID为4），因此模型无法区分它们。这称为哈希冲突。降低冲突风险的唯一方法是增加OOV桶的数量。但是，这也会增加类别的总数，一旦类别被独热编码，这将需要更多的RAM和额外的模型参数。所以，不要将这个数字增加太多。

```python
# 一个用IntegerLookup层的例子:
ids = [123, 456, 789, 123,123, 456]
int_lookup_layer = tf.keras.layers.IntegerLookup()
int_lookup_layer.adapt(ids)

int_lookup_layer([[123], [456], [123], [111]])

# 1 2  1  0
```

### 哈希层

对于每个类别，Keras哈希层计算一个哈希值，对桶（或bin）的数量取模。映射完全是伪随机的，但跨运行和平台稳定（即相同的类别将始终映射到相同的整数，只要bin的数量不变)

```python
hashing_layer = tf.keras.layers.Hashing(num_bins=10)
hashing_layer([["Paris"], ["Tokyo"], ["Auckland"], ["Montreal"]])
```

这个层不需要任何调整,不需要去适应(adapt)数据,在核外学习中(数据集太大无法放入内存),会很有用. 但是依然会有哈希冲突的问题: 两个不同的词映射到了相同的ID,使得模型无法区分它们.通常使用StringLookup层.

### 使用嵌入编码分类特征

嵌入是一些高维数据（例如类别或词汇表中的单词）的密集表示。如果有50000个可能的类别，则独热编码将产生50000维的稀疏向量（即主要包含零）。

相反，嵌入将是一个相对较小的密集向量，例如，只有100个维度。在深度学习中，嵌入通常被随机初始化，然后与其他模型参数一起通过梯度下降进行训练。例如，房价数据集中的"NEAR BAY"类别最初可以由一个随机向量（例如[0.131，0.890])表示，而"NEAR OCEAN"类别可能由另一个随机向量（例如[0.631，0.791]）表示。

![训练期间嵌入会逐渐改善](./images/tensorflow/p9.png)

在此示例中，使用二维嵌入，但维数是可以调整的超参数。由于这些嵌入是可训练的，它们会在训练过程中逐渐改善。由于在本例中它们代表非常类似的类别，梯度下降最终肯定会将它们推得更近，同时它倾向于将它们从"INLAND"类别的嵌入中移开.

事实上，类别表示越好，神经网络就越容易做出准确的预测，因此训练倾向于使嵌入成为有用的类别表示。这称为表示学习

Keras提供了一个Embedding层，它包装了一个嵌入矩阵：在该矩阵中，每个类别一行，每个嵌入维度一列。默认情况下，它是随机初始化的。要将类别ID转换为嵌入，Embedding层只是查找并返回对应于该类别的行

```python
# 初始化一个包含5行和二维嵌入的Embedding层，并用它来编码一些类
embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)
embedding_layer(np.array([2, 4, 2]))
```

Embedding层是随机初始化的，因此在模型外部将其用作独立的预处理层没有意义，除非使用预训练权重对其进行初始化

如果想嵌入一个分类文本属性，那么可以简单地连接一个StringLookup层和一个Embedding层

注意，嵌入矩阵的行数应等于词汇表的大小，即类别总数，包括已知类别加上OOV桶（默认情况下只有一个）。StringLookup类的vocabulary_size()方法可以方便地返回这个数字

```python
ocean_prox = ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
str_lookup_layer = tf.keras.layers.StringLookup()
str_lookup_layer.adapt(ocean_prox)
lookup_and_embed = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),
    str_lookup_layer,
    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),
                              output_dim=2)  # 这里使用2维嵌入,嵌入通常有10--300个维度,具体取决于任务,词汇表大小和训练集大小,需要调整这个超参数
])
lookup_and_embed(np.array(["<1H OCEAN", "ISLAND", "<1H OCEAN"]))
```

```python
# 搭建Keras模型:可以处理分类文本特征以及常规数值特征,并学习每个类别(以及每个OOV桶)的嵌入(embedding)

# 生成随机假数据
X_train_num = np.random.rand(10_000, 8)
X_train_cat = np.random.choice(ocean_prox, size=10_000).astype(object)
y_train = np.random.rand(10_000, 1)
X_valid_num = np.random.rand(2_000, 8)
X_valid_cat = np.random.choice(ocean_prox, size=2_000).astype(object)
y_valid = np.random.rand(2_000, 1)

num_input = tf.keras.layers.Input(shape=[8], name="num")
cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name="cat")
cat_embeddings = lookup_and_embed(cat_input)
encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])
outputs = tf.keras.layers.Dense(1)(encoded_inputs)
model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])
model.compile(loss="mse", optimizer="sgd")
history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,
                    validation_data=((X_valid_num, X_valid_cat), y_valid))
```

该模型采用两个输入：num_input（每个实例包含8个数值特征，以及cat_input（每个实例包含一个分类文本输入）。该模型使用之前创建的lookup_and_embed模型将每个海洋邻近类别编码为相应的可训练嵌入。接下来，它使用concatenate()函数连接数值输入和嵌入以生成完整的编码输入，这些输入已做好准备被提供给神经网络。此时，我们可以添加任何类型的神经网络，但为了简单起见，我们只添加一个密集输出层，然后使用刚刚定义的输入和输出创建Keras模型。接下来，编译模型并训练它，同时传递数值输入和分类输入。

由于Input层被命名为"num"和"cat"，我们也可以使用字典而不是元组将训练数据传递给fit()方法：{"num"：X_train_num，"cat"：X_train_cat}。或者，可以传递包含批次的tf.data.Dataset，每个批次表示为((X_batch_num，X_batch_cat)，y_batch)或({"num"：X_batch_num，"cat"：X_batch_cat}，y_batch)。当然，验证数据也是如此。

```python
train_set = tf.data.Dataset.from_tensor_slices(
    ({"num":X_train_num, "cat":X_train_cat}, y_train)).batch(32)
valid_set = tf.data.Dataset.from_tensor_slices(
    ((X_valid_num, X_valid_cat), y_valid)).batch(32)
history = model.fit(train_set, epochs=5,
                    validation_data=valid_set)
```

#### 词嵌入

嵌入通常不仅是当前任务的有用表示，而且很多时候可以成功地重用于其他任务。最常见的示例是词嵌入（即单个单词的嵌入) 在执行自然语言处理任务时，与训练自己的词嵌入相比，重用预先训练的词嵌入通常效果更好。使用向量来表示单词的想法可以追溯到20世纪60年代，许多复杂的技术已被用来生成有用的向量，包括神经网络技术。

但是这种表示真正取得成功是在2013年，当时Google研究人员发表了一篇论文(https://arxiv.org/abs/1310.4546)， 描述了一种使用神经网络学习词嵌入的有效技术，这种技术大大优于以前的尝试。这使他们能够在非常大的文本语料库上学习嵌入：他们训练了一个神经网络来预测任何给定单词附近的单词，并获得了惊人的词嵌入。例如，同义词具有非常接近的嵌入，France、Spain和Italy等与语义相关的词最终聚类在一起。但是，这不仅与邻近性有关：词嵌入还沿着嵌入空间中有意义的轴进行组织。这是一个著名的示例：如果计算King-Man+Woman（加减这些单词的嵌入向量），则结果非常接近Queen单词的嵌入。换句话说，词嵌入编码了性别的概念！同样，你可以计算Madrid-Spain+France，其结果接近Paris，这似乎表明首都的概念也在嵌入中进行了编码。

![相似单词的词嵌入接近,词有了空间位置的概念](./images/tensorflow/p10.png)

独热编码后跟一个Dense层（没有激活函数，也没有偏置）相当于一个Embedding层。然而，Embedding层使用的计算量更少，因为它避免了许多零乘法——当嵌入矩阵的大小增加时，性能差异变得明显。Dense层的权重矩阵起到了嵌入矩阵的作用。例如，使用大小为20的独热向量和具有10个单元的Dense层等同于使用input_dim=20和output_dim=10的Embedding层。

注意：使用比Embedding层后面的层中的单元数更多的嵌入维度是一种浪费，嵌入的维度进后面的层会被压缩

### 文本预处理

Keras为基本的文本预处理提供了一个TextVectorization层。与StringLookup层非常相似，必须在创建层时将词汇表传递给它，或者让它使用adapt()方法从一些训练数据中学习词汇表。

```python
train_data = ["To be", "!(to be)", "That's the question", "Be, be, be."]
text_vec_layer = tf.keras.layers.TextVectorization()
text_vec_layer.adapt(train_data)

text_vec_layer(["Be good!", "Question: be or be?"])
```

两个句子"Be good！"和"Question：be or be？"分别编码为[2，1，0，0]和[6，2，1，2]。词汇是从训练数据中的4个句子中学习的：be=2，to=3等。为了构造词汇，adapt()方法首先将训练句子转换为小写并删除标点符号，这就是“Be” “be”和“be？”都编码为be=2的原因。接下来，按空格拆分句子，将生成的单词按频率降序排列，产生最终词汇表。当对句子进行编码时，未知单词被编码为1。最后，由于第一句比第二句短，所以用0进行填充。

TextVectorization层有很多选项。例如，可以根据需要保留大小写和标点符号，方法是设置standardize=None，或者可以传递任何标准化函数作为standardize参数。可以通过设置split=None来防止拆分，或者可以传递自己的拆分函数。可以设置output_sequence_length参数以确保输出序列都被裁剪或填充到所需的长度，或者可以设置ragged=True以获得不规则张量而不是常规张量。

```python
text_vec_layer = tf.keras.layers.TextVectorization(ragged=True)
text_vec_layer.adapt(train_data)
text_vec_layer(["Be good!", "Question: be or be?"])
```

对单词ID进行编码，通常使用Embedding层。或者，可以将TextVectorization层的output_mode参数设置为"multi_hot"或"count"以获取相应的编码。然而，简单地计算单词数通常并不理想：像to和the这样的单词出现频率太高，以至于它们根本无关紧要，而basketball等不常见的单词提供的信息量要大得多。因此，与其将output_mode设置为"multi_hot"或"count"，不如将其设置为"tf_idf"，它代表“术语频率×逆向文档频率”(term-frequency×inverse-document-frequency，TF-IDF)。这类似于计数编码，但是在训练数据中频繁出现的单词被降低权重，反之，稀有单词被提高权重

TextVectorization层实现它的方式是将每个单词计数乘以等于log(1+d/(f+1))的权重，其中d是训练数据中的句子总数（又名文档），f计算有多少训练句子包含给定的单词。例如，在本例中，训练数据中有d=4个句子，其中f=3个出现了单词be。因为be这个词在"Question：be or be？"这个句子中出现了两次，它被编码为2×log(1+4/(1+3))≈1.3862944。question这个词只出现了一次，但由于它是一个不太常见的词，它的编码也非常高：1×log(1+4/(1+1))≈1.0986123。请注意，对于未知词，使用平均权重。

```python
text_vec_layer = tf.keras.layers.TextVectorization(output_mode="tf_idf")
text_vec_layer.adapt(train_data)

# ["To be", "!(to be)", "That's the question", "Be, be, be."]
print(text_vec_layer.get_vocabulary()) # 词表里一共收集到了 6 个词/标记（不包括未知的 [UNK]）。
text_vec_layer(["Be good!", "Question: be or be?"])
```

```python
import numpy as np
2 * np.log(1 + 4 / (1 + 3))
1 * np.log(1 + 4 / (1 + 1))
```

这种文本编码方法使用起来很简单，它可以为基本的自然语言处理任务提供相当好的结果，但它有几个重要的限制：它只适用于用空格分隔单词的语言，它不区分同文异义词（例如to bear与teddy bear），它不会向模型暗示evolution和evolutionary等词是相关的，等等。如果使用多热编码、计数编码或TF-IDF编码，那么单词的顺序就会丢失。一种选择是使用[TensorFlow文本库](https://tensorflow.org/text)， 它提供比TextVectorization层更高级的文本预处理功能。例如，它包括几个能够将文本拆分为小于单词的词元(token)的子词分词器，这使得模型可以更容易地检测到evolution和evolutionary有一些共同点，另一种选择是使用预训练模型组件

[TensorFlow Hub库](https://tensorflow.org/hub)  可以轻松地在自己的模型中重用预训练模型组件，用于处理文本、图像、音频等。这些模型组件称为模块。只需浏览TF Hub存储库(https://tfhub.dev)， 找到需要的那个，然后将代码示例复制到项目中，该模块将自动下载并捆绑到Keras层中，可以直接将Keras层包含在模型中。模块通常包含预处理代码和预训练权重，它们通常不需要进行额外训练（当然，模型的其余部分肯定需要训练）

```python
import tensorflow_hub as hub

hub_layer = hub.KerasLayer("https://tfhub.dev/google/nnlm-en-dim50/2")
sentence_embeddings = hub_layer(tf.constant(["To be", "Not to be"]))
sentence_embeddings.numpy().round(2)
```

hub.KerasLayer层从给定的URL下载模块。这个特殊的模块是一个句子编码器：它将字符串作为输入并将每个字符串编码为单个向量（在本例中为50维向量）。在内部，它解析字符串（按空格拆分单词）并使用在巨大语料库［Google News 7B语料库（70亿单词长）］上预训练的嵌入矩阵嵌入每个单词。然后，它计算所有词嵌入的平均值，结果就是句子嵌入

Hugging Face(https://huggingface.co/docs/transformers) 的优秀开源Transformer库也让我们可以轻松地将强大的语言模型组件包含在自己的模型中。浏览Hugging Face Hub(https://huggingface.co/models)， 选择想要的模型，然后使用它提供的代码示例即可。它过去只包含语言模型，但现在已经扩展到包括图像模型等。

```python
paris = hub_layer(tf.constant(["Paris"]))
france = hub_layer(tf.constant(["France"]))
china = hub_layer(tf.constant(["China"]))
beijing = hub_layer(tf.constant(["Beijing"]))
shanghai = hub_layer(tf.constant(["Shanghai"]))

beijing - (paris - france + china)
```

### 图像预处理层

Keras 预处理 API 包括三个图像预处理层：

- `tf.keras.layers.Resizing` 将输入图像调整为所需大小。例如，Resizing(height=100, width=200) 将每个图像的大小调整为 100×200 像素，可能会扭曲图像。如果设置 crop_to_aspect_ratio=True，则图像将按裁剪比例避免失真。
- `tf.keras.layers.Rescaling` 重新缩放像素值。例如，Rescaling(scale=2/255, offset=-1) 将值从 0～255 缩放到 -1～1。
- `tf.keras.layers.CenterCrop` 裁剪图像，只保留所需高度和宽度的中心部分图像。

```python
import tensorflow as tf
from sklearn.datasets import load_sample_images

images = load_sample_images()["images"]
crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)
cropped_images = crop_image_layer(images)
```

```python
import matplotlib.pyplot as plt
plt.imshow(tf.cast(cropped_images[0],tf.uint8))
plt.axis("off")
plt.show()
```

Keras还包括多个用于数据增强的层，例如RandomCrop、RandomFlip、RandomTranslation、RandomRotation、RandomZoom、RandomHeight、RandomWidth和RandomContrast。这些层仅在训练期间处于活动状态，并且它们随机对输入图像应用一些变换。数据增强会人为地增加训练集的大小，这通常可以提高性能，只要转换后的图像看起来像真实的（未增强的）图像

## Tensorflow数据集项目

[TensorFlow数据集(TensorFlow Datasets，TFDS)](https://tensorflow.org/datasets) 项目使得加载常见数据集［从MNIST或Fashion MNIST等小型数据集到ImageNet等大型数据集（需要相当多的磁盘空间！）］变得非常容易。它包括图像数据集、文本数据集（包括翻译数据集）、音频和视频数据集、时间序列等。 https://www.tensorflow.org/datasets/catalog/overview#all_datasets 网站里有每个数据集的描述

```python
import tensorflow_datasets as tfds   # TFDS未和TensorFlow绑定在一起

datasets = tfds.load(name="mnist")
mnist_train, mnist_test = datasets["train"], datasets["test"]
```

```python
for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):
    images = batch["image"]
    labels = batch["label"]
    # 可以对images 和 labels处理

```

```python
# 数据集的每个元素都是包含特征和标签的字典
# Keras期望每个元素都是一个包含两项（同样是特征和标签）的元组，使用map完成这个转换
mnist_train = mnist_train.shuffle(10_000, seed=42).batch(32)
mnist_train = mnist_train.map(lambda items: (items["image"], items["label"]))
mnist_train = mnist_train.prefetch(1)
```

```python
# TFDS提供了一种使用split参数拆分数据的便捷方法。例如，如果想使用前90%的训练集进行训练，剩下的10%进行验证，整个测试集进行测试，那么可以设置split=["train：90%]"，"train[90%:]"，"test"]。load()函数将返回这3个集合
train_set, valid_set, test_set = tfds.load(
    name="mnist",
    split=["train[:90%]", "train[90%:]", "test"],
    as_supervised=True  # as_supervised=True 返回的数据是 二元组 (input, label)。
)

train_set = train_set.shuffle(10_000, seed=42).batch(32).prefetch(1)

valid_set = valid_set.batch(32).cache()
test_set = test_set.batch(32).cache()
tf.random.set_seed(42)

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(10, activation="softmax")
])
model.compile(loss="sparse_categorical_crossentropy", optimizer="nadam",
              metrics=["accuracy"])
history = model.fit(train_set, validation_data=valid_set, epochs=5)
test_loss, test_accuracy = model.evaluate(test_set)
```

