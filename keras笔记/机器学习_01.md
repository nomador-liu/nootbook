# 机器学习

## 环境准备

0. 环境删除：`conda remove -n 环境名称 --all`，删除环境前需要deactivate
1. `conda env create -f environment.yml`
2. `conda activate homl3`
3. （可选）`python -m ipykernel install --user --name=python3 `这条命令的作用：把当前` Python `环境注册为 `Jupyter Notebook` 或 `JupyterLab` 中的一个可选内核（`kernel`），并命名为 `python3`。
4. `pycharm`里设置项目的`pytho`n解释器路径：   `......\anaconda3\envs\homl3\python.exe`

## 机器学习介绍
### 什么是机器学习
- 机器学习是一门通过编程让计算机从数据中进行学习的科学
- 通用定义：机器学习是一个研究领域让计算机无须进行明确编程就具备学习能力
- 工程化定义：一个计算机程序利用经验E来学习任务T，性能是P，如果针对任务T的性能P随着经验E不断增长，则称为机器学习

例子：垃圾邮件过滤器，可以根据给定的垃圾邮件（由用户标记）和普通电子邮件（非垃圾邮件）学习标记垃圾邮件。系统用来学习的样例称为训练集，每个训练样例称为训练实例（或样本）。机器学习系统中学习和做出预测的部分称为模型。在这个示例中，任务T是标记新邮件是否为垃圾邮件，经验E是训练数据，需要定义性能度量P，例如，可以使用正确分类电子邮件的比率，这种性能指标称为精度，用于分类任务。

反例：可以下载所有百度百科文章到电脑，这样计算机会拥有很多数据，但它不会擅长任何任务，这不是机器学习


### 为什么要用机器学习？
假设不用机器学习，用传统的编程技术编写垃圾邮件过滤器：

a. 明确需求

定义“垃圾邮件”可能具有以下特征：

含有特定关键词（如“中奖”、“免费”、“贷款”、“点击领取”）

发件人可疑（如随机邮箱、陌生域名）

格式异常（全是大写字母、颜色字体奇怪）

带有链接或附件

b. 为各个明确定义的模式编写一个检测算法：

```python
def is_spam(email):
    if "中奖" in email.subject:
        return True
    if "点击领取" in email.body:
        return True
    if email.sender.endswith("@weird-domain.cn"):
        return True
    if has_suspicious_links(email.body):
        return True
    return False
```
还有其他函数去检测这些规则，例如：文本匹配（正则表达式），判断发件人域名是否合法，统计超链接数量，检测是否有不常见附件（`.exe`）

c. 测试与调优

手工收集一些样本邮件（正常邮件 + 垃圾邮件），观察这些规则的效果：

有多少真正的垃圾邮件被识别出来（召回率）

有多少正常邮件被误判为垃圾（准确率）

然后反复：
- 添加规则
- 修改规则条件
- 增加例外情况（白名单）

d. 维护与更新

垃圾邮件是不断变化的，攻击者会「绕过关键词检测」，例如：

用“中￥奖”代替“中奖”

加空格“点 击 领 取”

图片代替文字

所以你必须不断：分析最新垃圾邮件样本，修改规则，增加黑名单

传统编程做垃圾邮件过滤的局限性：
| 问题           | 描述                               |
| -------------- | ---------------------------------- |
| 规则写不全     | 很难穷举所有垃圾邮件的特征         |
| 误判多         | 好邮件中可能也有“免费”等词         |
| 难以维护       | 垃圾邮件套路常变，规则需要频繁更新 |
| 不具备泛化能力 | 不能“学会”识别新类型的垃圾邮件     |


用机器学习的方式，实现垃圾邮件过滤器的流程：

| 步骤        | 传统编程         | 机器学习                                   |
| ----------- | ---------------- | ------------------------------------------ |
| 1. 收集数据 | 手动写规则       | 准备“带标签”的邮件样本（含是否垃圾的标注） |
| 2. 表示邮件 | 自然语言处理困难 | 把文本转成数字向量（如词袋模型）           |
| 3. 判断     | 写 if/else 规则  | 训练模型自动判断                           |
| 4. 调整     | 修改规则         | 调整模型参数或加新数据                     |

1. 准备数据集（带上标签，标记是否为垃圾邮件)
```python
texts = ["免费 办理 信用卡", "老板 开会", ...]
labels = [1, 0, ...]
```

2. 将文本转成数字 （文本——>向量)
```python
from sklearn.feature_extraction.text import CountVectorizer

texts = ["免费 办理 信用卡", "老板 开会"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)
```

3. 根据数据训练一个模型
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# 示例标签：0表示正常，1表示垃圾邮件
y = [1, 0]

# 拆分数据集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 训练模型
model = MultinomialNB()
model.fit(X_train, y_train)

# 测试预测
pred = model.predict(X_test)
print(pred)
```

4. 使用模型
```python
email = ["点击这里 免费 领取大奖"]
X_new = vectorizer.transform(email)
print(model.predict(X_new))  # 输出 1 表示垃圾邮件
```


- 传统开发流程和机器学习开发流程对比：
| 传统开发流程   | 机器学习开发流程         |
| -------------- | ------------------------ |
| 明确需求和规则 | 收集数据                 |
| 写程序         | 预处理数据               |
| 测试调试       | 训练模型                 |
| 修复 bug       | 调整模型参数、评估性能   |
| 上线维护       | 部署模型，收集反馈再训练 |

- 为什么垃圾邮件过滤器机器学习更合适？
| 传统编程           | 机器学习                     |
| ------------------ | ---------------------------- |
| 人工制定规则       | 自动从数据中学习规则         |
| 难以泛化，容易绕过 | 能从多样化样本中学习泛化特征 |
| 难维护，需反复更新 | 模型可重新训练，自我改进     |
| 高误判/漏判风险    | 可量化并优化准确率           |

- 总结：
  机器学习非常适合：
1. 现有解决方案需要大量微调或一长串规则来解决的问题 （通过训练模型简化代码，而且比传统方法执行的更好）
2. 使用传统方法无法解决的复杂问题，但机器学习技术可能可以找到解决方法 （识别图片里是否有行人；自动标出图片里行人出现的区域）
3. 变化的环境（机器学习系统可以很容易地根据新数据重新训练，保持最新的状态）
4. 定义明确的复杂问题（不是通过定义规则能解决的），且有大量数据


### 应用场景
- 图像
    1. 分析生产线上的产品图像来对产品进行自动分类 （图像分类）
    2. 通过脑部CT发现肿瘤 （语义图像分割，图像中的每个像素都被分类）
- 自然语言
    1. 自动分类新闻文章 （文本分类）
    2. 自动标记网上的恶评 （文本分类）
- 其他
    1. 根据许多绩效指标来预测公司下一年的收入 （回归任务，根据输入预测值）
    2. 检测信用卡欺诈（异常检测）
    3. 根据客户的购买记录对客户进行细分 （聚类）
    4. 根据过去的购买情况推荐客户可能感兴趣的产品 （推荐系统）

应用场景可以一直延伸，主要了解任务的广度和复杂性，以及每项任务对应到的技术类型。


### 机器学习的类别：监督和无监督
| 维度         | **监督学习 (Supervised Learning)**                           | **无监督学习 (Unsupervised Learning)**                       |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **核心思想** | 通过\*\*“已有答案”\*\*（标签）学习输入与输出的映射。         | 让算法自行发现数据中的**隐藏结构或分布**。                   |
| **典型任务** | - **分类**：垃圾邮件识别、肿瘤良恶性判断。<br>- **回归**：房价预测、股票价格预估。 | - **聚类**：客户细分、图像分组。<br>- **降维 / 表示学习**：特征压缩、异常检测、可视化。 |
| **常见算法** | 线性回归、逻辑回归、决策树、随机森林、支持向量机、深度网络等。 | K-Means、 PCA等。                                            |
| **所需数据** | 数据集中每个样本都附有**标签 y**。                           | 只有特征 **x**，没有标签；或仅少量标签。                     |
| **评价指标** | Accuracy、RMSE等与真实标签比对的度量。                       | 可解释性定性评估等。                                         |
| **典型输出** | 一个可对新样本给出具体预测的模型。                           | – 样本簇标签或相似度矩阵<br>– 降维后的新特征表示<br>– 发现的规则/共现模式。 |


### 机器学习应用的挑战
|      | 挑战                          | 书中要点（一句话版）                                 | 产生的典型问题                             |
| ---- | ----------------------------- | ---------------------------------------------------- | ------------------------------------------ |
| 1    | **训练数据量不足**            | 机器学习≠魔法：若样本太少，模型学不到统计规律。      | 预测不稳定、对新样本泛化差。               |
| 2    | **训练数据不具代表性**        | 采样偏差 → 训练分布≠真实分布。                       | 线上表现大幅掉点，甚至完全失效。           |
| 3    | **数据质量差**                | 含噪声、错误标签、缺失值、重复记录。                 | 模型误学“噪声”，需要费时清洗。             |
| 4    | **特征无关或信息稀薄**        | 原始字段与目标变量关联弱；或维度过高导致“维度灾难”。 | 学习效率低、计算量爆炸、易过拟合。         |
| 5    | **过拟合（学习到噪声）**      | 模型太复杂 / 约束太弱，在训练集成绩亮眼但无法泛化。  | 线上精度骤降；需正则化、简化模型或增数据。 |
| 6    | **欠拟合（学习不足）**        | 模型太简单 / 训练不足，连训练集都学不好。            | 表现整体偏低；需增加特征或换更强模型。     |
| 7    | **数据泄漏（信息穿越）**      | 未来信息或目标变量被不小心放入特征。                 | 离线分数“虚高”，上线瞬间翻车。             |
| 8    | **评估与调参不当**            | 划分集不严格、指标选错、过度调参。                   | 误判模型优劣，耗时却得不到收益。           |
| 9    | **计算资源限制**              | 大数据 + 大模型超出内存 / GPU / 时间预算。           | 需要采样、分布式或模型压缩。               |
| 10   | **数据/概念漂移（模型过期）** | 生产环境的输入分布或业务逻辑随时间变化。             | 性能随时间衰减，需监控和周期性再训练。     |



## 机器学习系统的类型

### 无监督学习

在无监督学习中，正如你可能猜到的那样，训练数据是未标记的。系统试图在没有老师的情况下进行学习。例如，假设你有大量关于博客访客的数据。你可能想要运行==聚类算法==来检测相似访客的分组。你不会告诉算法访客属于哪个组：它无须你的帮助即可找到这种关联。例如，它可能会注意到40%的访客是喜欢漫画书的青少年，通常在放学后阅读你的博客，而20%的访客是喜欢科幻小说的成年人，并通常在周末访问。如果你使用层次聚类算法，它还可以将每个组细分为更小的组。这可以帮助你针对不同的组来发布博客内容。

![无监督学习的训练集](./images/机器学习介绍1.png)

![聚类](./images/机器学习介绍2.png)





==可视化算法==也是无监督学习的一个好示例：你提供大量复杂且未标记的数据，算法轻松绘制输出2D或3D的数据表示。这些算法试图尽可能多地保留结构（例如，试图防止输入空间中的单独集群在可视化中重叠)，以便于你可以了解数据的组织方式，并可能识别出一些未知的模式。



![语义可视化示例](./images/机器学习介绍3.png)






与之相关的一个任务是降维，其目标是在不丢失太多信息的情况下简化数据。一种方法是将几个相关的特征合并为一个。例如，一辆汽车的行驶里程可能与其车龄有很强的相关性，因此降维算法会将它们合并为一个代表汽车磨损的特征。这称为特征提取。
在将训练数据提供给另一个机器学习算法（例如监督学习算法）之前，先使用降维算法减少训练数据的维度通常是个好主意。算法会运行得更快，数据会占用更少的磁盘和内存空间，并且在某些情况下，它还可能表现得更好。

另一个重要的无监督任务是==异常检测==，例如，检测异常的信用卡交易来防止欺诈、发现制造缺陷，或者在将数据集提供给另一个学习算法之前自动从数据集中删除异常值。系统在训练期间主要使用正常实例，因此它会学习识别它们。然后，当看到一个新实例时，系统可以判断这个新实例看起来是正常的还是异常的（见图1-10）。一个非常类似的任务是新颖性检测，它旨在检测看起来与训练集中所有实例不同的新实例。这需要有一个非常“干净”的训练集，没有任何你希望算法能够检测到的实例。例如，如果你有几千张狗的照片，其中1%代表吉娃娃犬，那么新颖性检测算法不应将吉娃娃犬的新图片视为新颖。另外，异常检测算法可能认为这些狗非常稀有并且与其他狗如此不同，以至于可能会将它们归类为异常

![异常检测](./images/机器学习介绍4.png)

最后，还有一个常见的无监督任务是==关联规则学习==，其目标是挖掘大量数据并发现属性之间有趣的关系。例如，假设你开了一家超市，在销售日志上运行关联规则可能会发现购买烧烤酱和薯片的人也倾向于购买牛排。因此，你可能希望将这几样商品摆放得更近一些。



### 半监督学习

由于标记数据通常既耗时又昂贵，因此你通常会有很多未标记的实例而很少有已标记的实例。一些算法可以处理一部分已标记的数据。这称为半监督学习。

一些照片托管服务（例如Google相册）就是很好的示例。一旦你将所有的家庭照片上传到该服务，它会自动识别出同一个人A出现在照片1、5和11中，而另一个人B出现在照片2、5和7中。这是算法（聚类）的无监督部分。现在系统只需要你告诉它这些人是谁。你只需为每个人添加一个标签[插图]，系统就可以为每张照片中的每个人命名，这对于搜索照片非常有用。

大多数半监督学习算法是无监督和监督算法的组合。例如，可以使用聚类算法将相似的实例分组在一起，然后每个未标记的实例都可以用其集群中最常见的标签进行标记。一旦标记了整个数据集，就可以使用任何监督学习算法。

![具有两个类别的半监督学习](./images/机器学习介绍5.png)

### 自监督学习

机器学习的另一种方法可以是从完全未标记的数据集生成完全标记的数据集。同样，一旦标记了整个数据集，就可以使用任何监督学习算法。这种方法称为自监督学习。例如，如果你有一个很大的未标记图像数据集，你可以随机屏蔽每个图像的一小部分，然后训练一个模型来恢复出原始图像。在训练期间，屏蔽的图像用作模型的输入，原始图像用作标签。生成的模型本身可能非常有用。例如，修复损坏的图像或从图片中删除不想要的对象。但通常情况下，使用自监督学习训练的模型并不是你的最终目标。你通常需要针对稍微不同的任务（你真正关心的任务）来调整和微调模型。

![自监督学习示例](./images/机器学习介绍6.png)

例如，假设你真正想要的是一个宠物分类模型：给定一张宠物的照片，模型会告诉你这只宠物属于什么物种。如果你有大量未标记的宠物照片数据集，则可以先使用自监督学习来训练一个图像修复模型。如果模型表现良好，则它应该能够区分不同的宠物种类：当它修复一张蒙着脸的猫的图像时，它必须知道不要添加狗的脸。假设你的模型架构允许，那么你就可以调整模型，使它能预测宠物种类而不是修复图像。最后一步是在已标记的数据集上微调模型：模型已经知道猫、狗和其他宠物的样子，因此只需要这个步骤，模型就可以学习它已知的物种和我们期望从中得到的标签之间的映射。

将知识从一项任务转移到另一项任务称为==迁移学习==，它是当今机器学习中最重要的技术之一，尤其是在使用深度神经网络（即由多层神经元组成的神经网络）时。我们将在第二部分详细讨论这一点。有些人认为自监督学习是无监督学习的一部分，因为它处理的是未标记的数据集。但是　自监督学习在训练期间是使用（生成的）标签的，因此在这方面它更接近于监督学习。在处理聚类、降维或异常检测等任务时，通常会使用术语“无监督学习”，而自监督学习侧重于与监督学习相同的任务，主要是分类和回归。最好将自监督学习视为一个单独的类别。

### 强化学习

强化学习是一种非常不同的机器学习类型。这个学习系统可以观察环境，选择和执行动作，并获得回报（或负回报形式的惩罚）。然后它必须自行学习什么是最好的方法，称为策略，以便随着时间的推移获得最大的回报。策略定义了智能体在给定情况下应该选择的动作。

![强化学习](./images/机器学习介绍7.png)

例如，许多机器人采用强化学习算法来学习如何走路。DeepMind的AlphaGo程序也是强化学习的一个很好的示例：它在2017年5月的围棋比赛中打败了当时世界排名第一的柯洁。它通过分析数百万场比赛，然后与自己进行多次对弈，从而习得了获胜策略。请注意，在与人类冠军的比赛中学习过程是被关闭的，AlphaGo只是在应用它已经学到的策略。正如你将在下一节中看到的，这称为离线学习



## 批量（离线）学习与在线（增量）学习

对机器学习系统进行分类的另一个标准是系统能否从输入数据流中进行增量学习。



### 批量学习

在批量学习中，系统无法进行增量学习：它必须使用所有可用的数据进行训练。这通常会占用大量的时间和计算资源，因此通常需要离线完成。首先对系统进行训练，然后将其投入生产环境运行，就不再学习了。它只是应用它学到的东西。这称为离线学习。不幸的是，模型的性能往往会随着时间的推移而慢慢变差，因为世界在不断演进发展，而模型却保持不变。这种现象通常称为模型腐烂或数据漂移。解决方案是定期根据最新的数据重新训练模型。你需要多久做一次取决于用例：如果模型对猫和狗的图片进行分类，它的性能会衰减得很慢，但如果模型处理快速变化的系统，例如对金融市场进行预测，那么它很可能会衰减得相当快。

即使是经过训练可以对猫和狗的图片进行分类的模型也可能需要定期重新训练，这不是因为猫和狗会在一夜之间发生变化，而是因为相机会不断变化，图像格式、清晰度、亮度和大小比例也在不断变化。

如果想让批量学习系统理解新数据（比如新型垃圾邮件），需要在完整数据集（不仅是新数据，还有旧数据）上从头开始训练新版系统，然后用新模型替换旧模型。幸运的是，训练、评估和启动机器学习系统的整个过程可以相当容易地自动化，因此即使是批量学习系统也可以适应变化。只需要经常更新数据并从头开始训练新版系统。

![自动化训练评估和启动](./images/机器学习介绍8.png)

这个解决方案很简单，而且通常效果很好，但使用完整数据集进行训练可能需要花费很多小时，因此通常每24小时甚至每周训练一次新系统。如果你的系统需要适应快速变化的数据（例如，预测股票价格），那么你需要一个更具反应性的解决方案。

此外，在完整的数据集上进行训练需要大量的计算资源（CPU、内存空间、磁盘空间、磁盘I/O、网络I/O等）。如果你有大量的数据，并且你让系统每天从头开始训练，那么最终会花费你很多钱。如果数据量巨大，那么甚至可能无法使用批量学习算法。最后，如果系统需要能够自动学习并且它的资源有限（例如，智能手机应用程序或火星上的漫游机器人），那么携带大量训练数据并占用大量资源来每天训练数小时是不太可能的。在这些情况下，更好的选择是使用能够增量学习的算法。


### 增量学习/在线学习


在在线学习中，通过以单独的数据或小批量的小组数据方式循序地向系统提供数据实例来对系统进行增量训练。每个学习步骤都既快速又便宜，因此系统可以即时学习新数据

![增量学习在生产环境不断学习](./images/机器学习介绍10.png)






在线学习对于需要快速适应变化的系统（例如，检测股票市场中的新模式）很有用。如果你的计算资源有限，例如，模型是在移动设备上训练的，那么在线学习是一个不错的选择。

此外，对于超大数据集——超出一台计算机的主存储器所能容纳的数据，在线学习算法也同样适用［这称为核外(out-of-core)学习］。该算法加载部分数据，在该数据上运行一个训练步骤，然后重复该过程，直到它在所有数据上运行完。

![增量学习应对超大量数据](./images/机器学习介绍11.png)

在线学习系统的一个重要参数是它们适应不断变化的数据的速度：这称为学习率。如果设置的学习率很高，那么系统会快速适应新数据，但它也会很快忘记旧数据（并且你也不希望垃圾邮件过滤器只标记最新类型的垃圾邮件）。反之，如果设置的学习率很低，那么系统会有更多的惰性，也就是说，它会学习得更慢，但它对新数据中的噪声或非典型数据点（异常值）序列的敏感度也会降低。

核外学习通常是离线（即不在实时系统上）完成的，因此在线学习可能是一个容易混淆的名字。将其视为增量学习会更合适。在线学习的一大挑战是，如果将不良数据输入系统，系统的性能可能会迅速下降（取决于数据的质量和学习率）。如果它是实时系统，那么你的客户会注意到这个现象。不良数据的来源可能是机器人的传感器故障，或者有人对搜索引擎恶意刷屏以提高搜索结果排名等。为降低这种风险，你需要密切监控系统，并在检测到性能下降时立即关闭学习（并尽量恢复到之前的工作状态）。你可能还想监控输入数据并对异常数据做出反应。例如，使用异常检测算法



## 基于实例的学习与基于模型的学习


对机器学习系统进行分类的另一种方法是根据它们的泛化方式。大多数机器学习任务都与做出预测有关。这意味着在给定大量训练样例的情况下，系统需要能够对它以前未见到过的样例做出良好的预测（泛化）。在训练数据上有很好的性能是好的，但还不够，真正的目标是在新实例上表现良好。泛化方法主要有两种：基于实例的学习和基于模型的学习。


### 基于实例的学习


最司空见惯的学习方式可能就是简单地背诵。如果你以这种方式来创建垃圾邮件过滤器，那么它只会标记所有与用户已标记的电子邮件相同的电子邮件——这虽然不是最坏的解决方案，但肯定不是最好的。垃圾邮件过滤器不仅可以标记与已知垃圾邮件相同的电子邮件，还可以对其进行编程来标　记与已知垃圾邮件非常相似的电子邮件。这需要衡量两封电子邮件之间的相似性。两封电子邮件之间的（非常基本的）相似性度量可以是计算它们共有的单词数。如果一封电子邮件与已知的垃圾邮件有很多相同单词，那么系统会将其标记为垃圾邮件。这称为基于实例的学习：系统学习样例，然后通过使用相似性度量将它们与学习到的样例（或它们的子集）进行比较来泛化到新实例。例如，在图中，新实例被归类为三角形，因为大多数最相似的实例都属于该类。

![基于实例的学习](./images/机器学习介绍12.png)

### 基于模型的学习和典型的机器学习工作流程


对一组样例进行泛化的另一种方法是为这些样例构建一个模型，然后使用该模型进行预测。这称为基于模型的学习

- 研究数据
- 选择一个模型
- 使用训练数据进行训练（即通过算法搜索最小化代价函数的模型参数值)
- 最后，应用该模型对新实例进行预测（这称为推断/泛化），希望该模型能够被很好地泛化。

这就是一个典型的机器学习项目！


## 机器学习的主要挑战
简而言之，由于你的主要任务是选择一个模型并在一些数据上对其进行训练，因此可能出现问题的不外乎是“不良模型”和“不良数据”。让我们先从不良数据开始。


### 训练数据量不足


要让孩子了解什么是苹果，你只需指着一个苹果说“苹果”（可能需要重复此过程很多次）就行了，然后孩子就能够识别出各种颜色和形状的苹果，简直是天才！机器学习还没达到这一步，大多数机器学习算法需要大量数据才能正常工作。即使对于非常简单的问题，你通常也需要成千上万个样例，而对于图像或语音识别等复杂问题，你可能需要数百万个样例（除非你可以重用现有模型的某些部分）。

- 数据的重要性：
给定足够的数据，截然不同的机器学习算法（包括相当简单的算法）在自然语言消歧（上下文区分to/two/too）的复杂问题上表现几乎完全相同

![数据对算法的重要性](./images/机器学习介绍13.png)

- 对复杂问题而言数据比算法更重要,需要注意的是，中小型数据集仍然很普遍，获得额外的训练数据并不总是一件轻而易举或物美价廉的事情，所以暂时不要抛弃算法。


### 训练数据不具备代表性


为了很好地实现泛化，训练数据必须能代表你想要泛化的新样例，这一点至关重要。无论你使用基于实例的学习还是基于模型的学习，都是如此。例如，训练一个GDP对应幸福指数的线性模型，国家数据集并不完全具有代表性，它不包含任何人均GDP低于23500美元或高于62500美元的国家。
下图展示了添加这些国家信息之后的数据表现。

![更具代表性的训练样本](./images/机器学习介绍14.png)

如果你在这个数据上训练一个线性模型，你会得到上图中的实线，旧模型用虚线表示。如你所见，添加了一些缺失的国家信息不仅显著地改变了模型，而且清楚地表明这种简单的线性模型可能永远不会那么准确。似乎非常富裕的国家并不比中等富裕国家更幸福（事实上，似乎稍微有点不幸福！），相反，一些贫穷国家似乎比许多富裕国家更幸福。

通过使用不具代表性的训练集，你训练了一个不太可能做出准确预测的模型，尤其是对于那些非常贫穷和非常富裕的国家。

使用代表你要泛化到的实例的训练集至关重要。不过说起来容易，做起来难：如果样本太小，那么会出现采样噪声（即非代表性数据被选中），但如果采样方法有缺陷，即使是非常大的样本也可能不具有代表性。这称为采样偏差。


### 低质量数据


显然，如果你的训练数据充满错误、异常值和噪声（例如，低质量的测量产生的数据），系统将更难检测到底层模式，也就更不太可能表现良好。花时间清洗训练数据通常是非常值得的。事实上，大多数数据科学家都会花费大量时间做这项工作。例如：·

- 如果某些实例明显异常，则简单地丢弃它们或尝试手动修复错误可能会有所帮助。·
- 如果某些实例缺少一些特征（例如，5%的客户没有说明自己的年龄），你必须决定是完全忽略此属性、忽略这些实例、将缺失值补充完整（例如，填写年龄的中位数），还是训练一个具有该特征的模型，再训练一个没有该特征的模型。


### 无关特征


垃圾进，垃圾出。只有当训练数据包含足够多的相关特征并且没有太多无关特征时，系统才能够进行学习。机器学习项目成功的一个关键部分是提取出好的特征集来进行训练。这个过程称为特征工程，包括以下步骤：

- 特征选择（在现有特征中选择最有用的特征进行训练）。
- 特征提取（结合现有特征产生更有用的特征，正如我们之前看到的，降维算法可以提供帮助）
- 通过收集新数据创建新特征

训练数据不具备代表性，低质量数据和无关特征是不良数据的实例，以下讨论不良模型的示例



### 过拟合训练数据


假设你正在别的城市旅游，被出租车司机拒载。你可能会说那个城市的所有出租车司机都是地域歧视。以偏概全是我们人类常做的事情。机器也会落入同样的陷阱。在机器学习中，这称为过拟合，也就是指该模型在训练数据上表现良好，但泛化效果不佳。

当模型相对于训练数据的数量和噪声过于复杂时，就会发生过拟合。以下是可能的解决方案：
- 通过选择参数较少的模型（例如，线性模型而不是高阶多项式模型）、减少训练数据中的属性数量或约束模型来简化模型。
- 收集更多训练数据。
- 减少训练数据中的噪声（例如，修复数据错误并移除异常值）。

通过约束模型使它更简单，并降低过拟合的风险，这个过程称为正则化。学习期间应用的正则化程度可以由超参数控制。超参数是学习算法（而非模型）的参数。因此，它不受学习算法本身的影响，必须在训练前设置并在训练期间保持不变。如果将正则化超参数设置得非常大，你将得到一个几乎平坦的模型（斜率接近于零）。学习算法虽然肯定不会过拟合训练数据，但也不太可能找到好的解决方案。调整超参数是构建机器学习系统的重要部分


### 欠拟合训练数据


欠拟合与过拟合正好相反：当模型太简单而无法学习数据的底层结构时，就会发生欠拟合。例如，生活满意度的线性模型容易出现欠拟合。因为现实情况总是比模型更复杂，所以它的预测必然是不准确的，即使是在训练样例上也是如此。

以下是解决此问题的主要方式：
- 选择具有更多参数的更强大的模型。
- 为学习算法提供更好的特征（特征工程）。
- 减少对模型的约束（例如通过减少正则化超参数）。


### 术语总结
- 机器学习是关于如何让机器更好地完成某些任务的理论，它从数据中学习而无须清晰地编写规则。
- 机器学习系统有许多类型：有监督和无监督，半监督自监督，强化学习，批量的和在线的，基于实例的和基于模型的。
- 在机器学习项目中，你从训练集中收集数据，然后将训练集提供给学习算法。如果该算法是基于模型的，它会调整一些参数以使模型拟合训练集（对训练集本身做出良好的预测），然后希望它也能够对新实例做出良好的预测。如果该算法是基于实例的，那么它会记住样例，并根据相似性度量将它们与学习过的实例进行比较，从而泛化到新实例。
- 如果训练集太小，或者数据不具有代表性、有噪声或被不相关的特征（垃圾进、垃圾出）污染，那么系统的表现不会很好。最后，你的模型既不能太简单（这种情况会导致欠拟合）也不能太复杂（这种情况会导致过拟合）。

下一个主题：一旦训练了一个模型，你就不能只是期待它泛化到新实例，你还需要具体的步骤，评估它是否具备泛化能力，并在必要时对其进行微调。让我们看看如何做到这一点。


## 测试和验证


了解模型对新实例的泛化能力的唯一方法是在新实例上进行实际尝试。一种方法是将模型部署到生产环境并监控其性能。这种方法很有效，但如果模型非常糟糕，用户就会投诉/流失，所以这显然不是最好的方法。

更好的选择是将数据分成两组：训练集和测试集。顾名思义，你可以使用训练集训练模型，并使用测试集对其进行测试。新实例的错误率称为泛化误差（或样本外误差），通过在测试集上评估模型，你可以获得误差的估计值。这个值能告诉你模型在处理以前从未见过的实例时的表现。如果训练误差很低（即模型在训练集上犯的错误很少）但是泛化误差很高，这意味着你的模型过拟合了训练数据。

通常使用80%的数据进行训练，保留20%的数据进行测试。然而，这取决于数据集的大小：如果它包含1000万个实例，那么保留1%意味着你的测试集将包含10万个实例，这可能足以很好地估计泛化误差。


### 超参数调整和模型选择

评估模型非常简单：只需使用一个测试集。现在假设你在两种类型的模型（比如线性模型和多项式模型）之间犹豫不决，如何做出判断？一种选择是同时训练这两个模型，并使用测试集比较它们的泛化能力。

现在假设多项式模型拟合得更好，但你想应用一些正则化来避免过拟合。问题是，如何选择正则化超参数的值？一种选择是对于这个超参数，使用100个不同的值来训练100个不同的模型。假设你找到了最佳超参数值，它生成的模型泛化误差最小，比如只有5%的误差。你将此模型部署到生产环境，但遗憾的是它的性能并不如预期，产生了15%的误差。到底发生了什么？

问题是你多次测量了测试集上的泛化误差，并且调整了模型和超参数来生成拟合那个测试集的最佳模型。这意味着该模型不太可能在新数据上表现良好。 （对测试集过拟合了，真实环境的数据表现得不好）

这个问题的一个常见解决方案称为保持验证（见下图）：你只需保持训练集的一部分，以评估几个候选模型并选择最佳模型。新的保留集称为验证集（或开发集）。更具体地说，你可以在简化的训练集（即完整训练集减去验证集）上训练具有各种超参数的多个模型，然后选择在验证集上表现最佳的模型。在此保持验证过程之后，你在完整训练集（包括验证集）上训练最佳模型，这就是你的最终模型。最后，你在测试集上评估这个最终模型以获得泛化误差的估计。

![使用验证集评估的模型](./images/机器学习介绍15.png)




这个解决方案通常效果很好。但是，如果验证集太小，则模型评估就不精确，你最终可能会错误地选择一个次优模型。

相反，如果验证集太大，那么剩余的训练集会比完整的训练集小得多。由于最终模型是在完整的训练集上进行训练，因此在小得多的训练集上训练的候选模型并不理想。这就像选择最快的短跑运动员去参加马拉松比赛。

解决这个问题的一种方法是使用许多小的验证集执行重复的交叉验证。每个模型在对其余数据进行训练后，每个验证集都会评估一次。通过对模型的所有评估求平均，可以更准确地衡量其性能。然而，这有一个缺点：训练时间是验证集数量的倍数。


### 数据不匹配


  在某些情况下，很容易获得大量数据用于训练，但这些数据可能无法完全代表将在生产环境中使用的数据。

例如，假设你想创建一个移动应用程序来拍摄花朵照片并自动确定它们的种类。你可以轻松地在网络上下载数百万张花卉图片，但它们并不能完全代表在移动设备上使用该应用程序实际拍摄的照片。也许你只有1000张有代表性的照片（即实际使用该应用程序拍摄的）。在这种情况下，要记住的最重要的规则是验证集和测试集都必须尽可能地代表你希望在生产环境中使用的数据，因此它们应该完全由有代表性的图片组成：你可以将它们打乱并将一半放在验证集中，一半放在测试集中（确保两个集中都没有重复或接近重复）。

在网络图片上训练了模型后，如果你观察到模型在验证集上的表现令人失望，那么你将不知道这是因为模型对训练集过拟合了，还是仅仅因为网络图片和移动应用程序图片之间的不匹配。一种解决方案是将一些训练图片（来自网络）放在train-dev（训练开发）集（见下图）的另一个集合中。

模型训练完成后（在训练集上，而不是在train-dev集上），你可以在train-dev集上对其进行评估。如果模型表现不佳，那么它一定是对训练集过拟合了，所以应该尽量简化或正则化该模型、获取更多的训练数据，以及清洗训练数据。但是如果模型在train-dev集上表现良好，那么你可以在开发集上评估模型。如果模型表现不佳，那么问题一定来自数据不匹配。你可以尝试通过预处理网络图片来解决这个问题，使它们看起来更像移动应用程序拍摄的照片，然后重新训练模型。一旦你拥有在train-dev集和开发集上都表现良好的模型，就可以在测试集上最后一次评估它，以了解它在生产环境中的表现。

![train-dev集](./images/机器学习介绍16.png)

当真实数据稀缺时（右），可以使用类似丰富的数据（左）进行训练，并在train-dev集中保留一些数据以评估过拟合。然后使用真实数据评估数据不匹配（开发集）和最终模型的性能（测试集）


模型是数据的简化表示。简化旨在丢弃不太可能泛化到新实例的多余细节。当你选择特定类型的模型时，你是在隐含地对数据做出假设。例如，如果你选择线性模型，则隐含地假设数据基本上是线性的，并且实例与直线之间的距离只是噪声，可以安全地忽略它。

如果你完全不对数据做出任何假设，那么就没有理由偏爱某个模型。对于一些数据集，最好的模型是线性模型，而对于其他数据集，最好的模型是神经网络模型。不存在一个先验模型可以保证一定能更好地工作（数学上不能推导出一个模型，这个模型最能代表当前数据）

保证确定哪种模型最好的唯一方法是对所有模型进行评估。由于这是不可能的，因此在实践中你对数据做出了一些合理的假设并仅评估了几个合理的模型。例如，对于简单的任务，你可能只会评估几个具有不同正则化水平的线性模型，而对于复杂的问题，你可能会评估多个神经网络模型。


b## 练习题

这一部分讨论的是机器学习最重要的概念，贯穿于整个机器学习话题。请确保完成以下练习

```markdown

1. 如何定义机器学习？    T （任务） E（经验） P（性能）
答：就是让机器更好的进行某些任务的理论，是在数据中获得的，而不是透过清晰的规则编写获得
2. 机器学习在哪些问题上表现突出，你能说出四类应用吗？        复杂任务（人类智能级别的任务），无需明确规则，问题有大量数据
现有的解决方案需要大量的微调或一长串规则进行解决，使用传统方法可能无法解决的问题，变化的环境，定义明确但是十分复杂但又大量数据
3. 什么是被标记的训练集？   √
特征具有对应的标签
4. 最常见的两种监督学习任务是什么？   √
分类与回归
5. 你能说出四种常见的无监督学习任务吗？
聚类问题，可视化，异常检测，关联规则   ，#降维
6. 你会使用什么类型的算法让机器人在各种未知地形中行走？
强化学习算法
7. 你会使用什么类型的算法将客户分成多个组？
无监督学习的聚类算法
8. 你会把垃圾邮件检测问题定义为监督学习问题还是无监督学习问题？   √
监督学习问题
9. 什么是在线学习系统？
通过单独的数据或者小批量数据循环的向系统提供数据来对系统进行增量训练，
可以学习用户的数据
10. 什么是核外学习？
当数据集超过电脑储存时可以通过读取部分数据进行训练，并重复这一过程的方法就是核外学习
11. 什么类型的算法依赖于相似性度量来进行预测？
基于实例的学习算法
12. 模型参数和模型超参数有什么区别？     √
模型参数就是在机器学习中需要不断进行更新迭代的参数值，模型超参数就是通过设定在学习过程中一直保持不变的参数
13. 基于模型的算法搜索什么？它们最常用的成功策略是什么？它们如何做出预测？   √
成本和最优化，通过梯度下降的策略，通过最优化后的模型，对预测数据进行相同的处理
14. 你能说出机器学习中的四个主要挑战吗？       √
训练数据不足，数据不具备代表性，低质量数据，无关特征
15. 如果你的模型在训练数据上表现很好，但对新实例的泛化能力很差，这是怎么回事？你能说出三种可能的解决方案吗？            √
可能对原数据存在过拟合的问题，搜集更多数据，对特征进行降维，对模型进行正则化
16. 什么是测试集？为什么要使用它？           √
就是将原数据分成两组一组用于 训练模型的训练集，而测试集则是用来进行模型测试，测试机可以很好的      评估模型训练成果，看一下模型的泛化能力
17. 验证集的目的是什么？               √
将测试即和训练集彻底分开
18. 什么是train-dev集？什么时候需要它？如何使用？
将原本的训练集进行拆分，分成训练集与训练-开发集，测试集不能很好的代表训练集的特性时考虑将训练集进行拆分，用训练集进行机器学习，用训练-开发集进行检测，当检测结果不理想时，证明存在过拟合问题，如果检测结果理想，再将测试集喂给模型，此时如果不理想则证明数据存在不匹配问题
19. 如果使用测试集来调整超参数会出现什么问题？             √
测试集调整超参数会污染测试集，此时测试集相当于参与了训练，再将测试集进行测试起不到测试的效果，不能成为泛化指标，也就不能解决过拟合问题
```



## 机器学习基础与概念

### 回归模型
![线性回归示意图](./images/监督学习-回归-1.png)
- 回归模型预测数值，线性回归是回归模型的一种

| 特征 | 标签 |
| ---- | ---- |
| 2104 | 400  |
| 1416 | 232  |
| 1534 | 315  |
| ...  | ...  |
| 3210 | 870  |


### 机器学习术语
- 训练集（training set): 用来训练模型的数据
- x：特征（feature）/输入变量
- y：输出变量/目标变量
- m：训练集的数量
- (x,y): 单个训练数据
- $x^{(i)}$, $y^{(i)}$： 第i个训练数据


==监督学习：==

训练集 -- 学习算法 -- 得到一个 f（数学模型， 函数）

x（特征）-- f（模型，函数） -- 预测（预测的y）

问题：这个f怎么表示？

f(x) = wx + b,  这是一个最简单的回归模型：线性回归。 但它可以作为其他回归模型的理解基础
在示例的数据中，它是单变量的线性回归，特征只有一个变量

```python
import numpy as np
import matplotlib.pyplot as plt
```

```python
# x_train 输入变量
# y_train 目标变量
x_train = np.array([1.0, 2.0])
y_train = np.array([300.0, 500.0])
print(f"x_train = {x_train}")
print(f"y_train = {y_train}")

# 使用m 表示训练数据的数量
print(f"x_train.shape: {x_train.shape}")
m = x_train.shape[0]
# m = len(x_train)
print(f"训练数据的数量: {m}")

# 画数据的散点图
# Plot the data points
plt.scatter(x_train, y_train, marker='x', c='r')
# Set the title
plt.title("Data may be anything")
# Set the y-axis label
plt.ylabel('Income ')
# Set the x-axis label
plt.xlabel('level ')
plt.show()

# 用直线（线性模型）去匹配训练数据，定义线性模型本身的参数 w，b （f(x)=wx+b)
w = 200
b = 100
print(f"w: {w}")
print(f"b: {b}")

# 可以对每个训练数据的输入变量，计算 f 预测的输出
def compute_model_output(x, w, b):
    """
    计算线性模型的预测
    Args:
      x (ndarray (m,)): Data, m 个训练数据的输入变量
      w,b (scalar)    : 模型参数
    Returns
      f_wb (ndarray (m,)): 模型对输入的预测
    """
    return w*x + b


# 调用compute_model_output, 画出输出

tmp_f_wb = compute_model_output(x_train, w, b)
tmp_f_wb
# 画预测的线
plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')
#
# 画训练数据
plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')
#
# 设置标题
plt.title("Data may be anything")
#
# 设置y轴标签
plt.ylabel('Income ')
#
# 设置x轴标签
plt.xlabel('Level ')
plt.legend()  # 显示图例
plt.show()
```

```python
# 随堂练习：
# 尝试去为线性模型设置不同w和b， w，b是多少的时候可以最好地拟合训练数据
# w,b = 200 , 100
# 预测：现在有了具体的模型（得出了模型的参数，在线性模型里是w和b），可以拿它用来预测没有见过的输入
x_new = 1.2
income_pred = w * x_new + b
print(f"对x=1.2预测的数据: {income_pred:.2f}")
```


### 损失函数
![单个损失](./images/监督学习-回归-2.png)
- 衡量模型的表现，损失函数的值越小说明模型表现越好

- 最小化损失函数的值也被称作训练目标

- f(x) = wx+b, x是固定的，由输入的数据决定，w和b是可以调整的，w，b也被称作模型的权重

- 训练数据的实际的y和f(x)预测的y有差距，因此可以将把每个训练数据的误差都考虑到，定义如下损失函数：均方误差损失（MSE）：

==预测值定义 ：==
$$
\hat y^{(i)} \;=\; w^{\top}x^{(i)} + b
$$

==损失函数：==
$$
J(w,b) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\!\bigl(\,\hat y^{(i)} - y^{(i)}\bigr)^2
$$
- 1/2m为了让这个损失值不会因为训练集变大，使误差变得很大；也为了后续计算方便
- 注意：不同的w，b决定了不同的直线模型，不同的直线模型在训练数据上有不同的损失 -> J(w,b)是关于w，b的函数！w，b是自变量。
- 找到w，b使得J(w,b)最小化，这样的w，b也能最贴合训练数据，只要J(w,b)的定义合理

总结： 模型训练的四要素
- 模型的假设： 对模型数学公式的假设，直线模型
- 参数：数学公式自己的参数，wx+b， 参数 w和b
- 损失函数: 关于参数的函数，输入：模型的参数 返回的数字
- 目标： 找到w和b，可以最小化损失函数


![损失函数图像](./images/监督学习-回归-3.png)


### 达到目标/最小化损失函数的方式 -- 梯度下降
- 一个函数关于每个变量的偏导数，每个变量的偏导数组织在一起是一个向量，这个向量的方向是函数增长最快的方向
- 每个具体w，b计算梯度,得到具体的向量，当前w，b沿着梯度相反的方向走，可以是J(w,b)变小
- 梯度下降算法在机器学习中很常用，不只是线性回归
- 步骤：
-   1. 初始化w和b  2. 不断改变w，b使得J(w,b)变小  3. 直到在最小值附近
- 损失函数不一定只有一个最小值，可能有多个最小值, 所以不同的初始w，b可能会走向不同的最小值；但平方差之和的损失函数肯定只有一个最小值：


![多个最小值的损失函数](./images/监督学习-回归4.png)




### 梯度下降：同步更新 vs. 不同步更新

| 步骤               | **同步更新（Simultaneous / Synchronous）**——最常用，保证同一轮梯度基于“旧参数” | **不同步更新（Sequential / Asynchronous）**——概念说明用，实际较少单独使用 |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1 计算梯度**     | $(g_w = \frac{\partial J}{\partial w}(w,b),\; g_b = \frac{\partial J}{\partial b}(w,b))$ | **先算 $w$梯度**$(g_w = \frac{\partial J}{\partial w}(w,b))$ |
| **2 生成暂存变量** | $(\tilde w = w - \alpha g_w)\\(\tilde b = b - \alpha g_b)$   | （无暂存，直接改）$(w \leftarrow w - \alpha g_w)$            |
| **3 赋值**         | $(w \leftarrow \tilde w)\\(b \leftarrow \tilde b)$**同时生效** | **再用已更新的 \\(w\\)** 重新计算$(g_b = \frac{\partial J}{\partial b}(w,b))$然后$(b \leftarrow b - \alpha g_b)$ |
| **4 迭代下一轮**   | 全部参数都基于**上一轮旧值**算梯度，收敛更平稳               | 后计算的参数用到了**前面已更新的新值**；和同步更新的性质不一样 |

---

- 同步更新公式（常用写法）：
  $$
  \begin{aligned}
  g_w &= \frac{1}{m}\sum_{i=1}^{m}(\hat y^{(i)}-y^{(i)})\,x^{(i)},\\[4pt]
  g_b &= \frac{1}{m}\sum_{i=1}^{m}(\hat y^{(i)}-y^{(i)}),\\[8pt]
  w &\leftarrow w - \alpha\,g_w,\\
  b &\leftarrow b - \alpha\,g_b.
  \end{aligned}
  $$
  
- 不同步更新示例：
  $$
  \begin{aligned}
  w &\leftarrow w - \alpha\,\frac{\partial J}{\partial w}(w,b)\\[6pt]
  b &\leftarrow b - \alpha\,\frac{\partial J}{\partial b}\bigl(\underbrace{w}_{\text{已更新}},\,b\bigr).
  \end{aligned}
  $$


- 梯度下降直觉：
1. 可以把参数b去掉，J(w)是一个变量的函数，J(w)在某个w0的导数是正数，说明w0应该往左，J(w)会变低；导数是负数，说明w0应该往右，J(w)会变低，所以即使单个变量的函数，把导数的正负看成方向，导数的方向也是函数增长的最快方向。
2. 所以参数的更新公式是 w = w - learning_rate(学习率) * (dJ/dw), dJ/dw是损失函数的导数； 才可能保证J(w)变低

#%% md
### 学习率α
- 如果学习率太小，梯度下降可以正常进行，但会很慢

![学习率小](./images/监督学习-回归5.png)

- 如果学习率太大：
    1. 更新可能会一直跨过达到最小值的参数，一直到不了最小值
    2. 损失函数值可能不会收敛到一个值，会发散。

![学习率大](./images/监督学习-回归6.png)

- 如果初始化的参数正好落在了局部最小值，梯度下降不会更新参数
- 如果学习率正合适，每次用梯度下降更新参数，不需要改变学习率，固定学习率也可以使得J(w,b)到达局部最小值
    1. 在局部最小值附近，导数变小了，每次更新的幅度自动变小了
    2. 所以不需要降低学习率去到达局部最小

![学习率合适](./images/监督学习-回归7.png)



## 机器学习代码实现--线性回归



### 线性回归模型的梯度下降
- 线性回归模型：
$$
f(x) \;=\; w*x + b
$$
- 损失函数：
$$
J(w,b) \;=\; \frac{1}{2m}\sum_{i=1}^{m}\!\bigl(\,f(x^{(i)}) - y^{(i)}\bigr)^2
$$
- 优化（最小化）损失函数的方式：
  $$
  \begin{aligned}
  g_w &= \frac{1}{m}\sum_{i=1}^{m}(f(x^{(i)})-y^{(i)})\,x^{(i)},\\[4pt]
  g_b &= \frac{1}{m}\sum_{i=1}^{m}(f(x^{(i)})-y^{(i)}),\\[8pt]
  w &\leftarrow w - \alpha\,g_w,\\
  b &\leftarrow b - \alpha\,g_b.
  \end{aligned}
  $$
  
- 导数公式的推导过程（不理解也没关系，有最终的梯度公式就能写代码了）：
  $$
  - \begin{aligned}
    \frac{\partial J}{\partial w}
     &=\frac{\partial}{\partial w}\;
       \frac1{2m}\sum_{i=1}^{m}\Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr)^2 \\[4pt]
     &=\frac1{2m}\sum_{i=1}^{m}2\bigl(wx^{(i)}+b-y^{(i)}\bigr)\,x^{(i)} \\[4pt]
     &=\frac1{m}\sum_{i=1}^{m}\Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr)\,x^{(i)}
    \end{aligned}
  
  \begin{aligned}
  \frac{\partial J}{\partial b}
     &=\frac{\partial}{\partial b}\;
       \frac1{2m}\sum_{i=1}^{m}\Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr)^2 \\[4pt]
     &=\frac1{2m}\sum_{i=1}^{m}2\bigl(wx^{(i)}+b-y^{(i)}\bigr) \\[4pt]
     &=\frac1{m}\sum_{i=1}^{m}\Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr)
  \end{aligned}
  $$
  
- 梯度下降最终算法：
  重复执行以下逻辑，直到J的值收敛了，或者直到w和b的梯度接近于0；注意：**同时更新 \(w\) 和 \(b\)**
  $$
  \begin{aligned}
  w &\;\leftarrow\; w - \alpha \,\frac{1}{m}\sum_{i=1}^{m}
      \Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr)\,x^{(i)},\\[6pt]
  b &\;\leftarrow\; b - \alpha \,\frac{1}{m}\sum_{i=1}^{m}
      \Bigl(f_{w,b}\bigl(x^{(i)}\bigr)-y^{(i)}\Bigr).
  \end{aligned}
  $$
  
- 线性回归的平方差之和的损失函数是凹函数（碗状函数），只有一个全局最小值，没有局部最小值
- 这个算法梯度下降的每一步用了所有的训练数据，每一步用了所有训练数据的梯度下降也叫 批量梯度下降（Batch Gradient Descent)； 有一步梯度下降用训练数据的子集参与计算的算法

b### 线性回归梯度下降代码实验

```python
import numpy as np
import matplotlib.pyplot as plt

# TODO 1. 自定义训练集数据, 并用散点图可视化
noise = np.random.randn(10)
y = np.arange(1,11)
x_train = np.arange(1,11)  # 输入数据
y_train = 3* y + 1.1*noise +3 # 输入数据对应的标签

plt.scatter(x_train, y_train, marker = "^" , c="r")
plt.show()
y_train
# 示例图片：

# 2. 完成损失函数的实现
def compute_cost(x, y, w, b):
    """
    x: ndarray (m,) 训练数据的特征
    y: ndarray (m,) 训练数据的标签
    w: 浮点数，直线模型的参数
    b: 浮点数，直线模型的参数
    return: 浮点数，当前w，b下的损失值
    """
    # TODO
    m = x.shape[0]
    med = np.power(w*x+b-y , 2)
    med = med.sum()
    return med/2/m
# 3. 完成计算梯度函数的实现
def compute_gradient(x, y, w, b):
    """
    计算线性回归的梯度
    参数:
      x (ndarray (m,)): 输入数据，m 个样本
      y (ndarray (m,)): 目标值
      w, b (浮点数)   : 模型参数
    返回:
      dj_dw (浮点数): 损失函数对参数 w 的梯度
      dj_db (浮点数): 损失函数对参数 b 的梯度
    """
    # TODO
    m = x.shape[0]
    med= w*x+b-y
    med_w = x * med
    med_w = med_w.sum()/m
    dj_dw = med_w
    dj_db = med.sum()/m
    return dj_dw, dj_db


```


- 检查梯度计算实现是否正确，本质是根据导数的定义来计算：
| 概念                              | 说明                                                         |
| --------------------------------- | ------------------------------------------------------------ |
| **梯度检查（Gradient Checking）** | 在机器学习中会遇到复杂的梯度公式。为了防止推导或代码实现出错，可用 **数值差分**（finite difference）近似梯度，再与解析梯度对比。若两者误差极小，就说明实现是可信的。 |
| **中心差分**                      | 数值梯度近似公式：<br>\\(\displaystyle \frac{\partial J}{\partial \theta}\approx\frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}\\)；误差 \\(O(\varepsilon^2)\\)，比单边差分更精确。 |
| **适用场景**                      | - 调试早期代码<br>- 单元测试（确保未来改动不破坏梯度）<br>   |
| **限制**                          | 数值差分需要多次计算损失函数，**速度慢**。真实深度神经网络只在开发阶段做小批量检查。 |


- 中心差分更准确的数学扩展（了解即可）：

为什么中心差分更精确？

$$
\begin{aligned}
J(\theta + \varepsilon)
&= J(\theta) + \varepsilon J'(\theta) + \frac{\varepsilon^{2}}{2}J''(\theta) + \mathcal{O}\!\bigl(\varepsilon^{3}\bigr),\\[6pt]
J(\theta - \varepsilon)
&= J(\theta) - \varepsilon J'(\theta) + \frac{\varepsilon^{2}}{2}J''(\theta) + \mathcal{O}\!\bigl(\varepsilon^{3}\bigr).
\end{aligned}
$$
上述公式相减，误差只剩下极小数epsilon的立方项

```python
def gradient_check(x, y, w, b, eps=1e-7, tol=1e-4, verbose=True):
    """
    使用数值差分验证 compute_gradient 是否正确
    Args:
        x, y : 数据集
        w, b : 当前参数（float）
        eps  : 微小扰动 ε，用于近似数值梯度
        tol  : 容差阈值，|解析梯度 - 数值梯度| < tol 视为通过
        verbose: 打印详细差异
    Returns:
        passed (bool) : 是否通过梯度检查
    """
    # 1) 解析梯度
    dj_dw, dj_db = compute_gradient(x, y, w, b)

    # 2) 数值梯度——中心差分
    num_dj_dw = (compute_cost(x, y, w + eps, b) -
                 compute_cost(x, y, w - eps, b)) / (2 * eps)
    num_dj_db = (compute_cost(x, y, w, b + eps) -
                 compute_cost(x, y, w, b - eps)) / (2 * eps)

    # 3) 误差
    diff_w = abs(dj_dw - num_dj_dw)
    diff_b = abs(dj_db - num_dj_db)
    passed = diff_w < tol and diff_b < tol

    if verbose:
        print(f"你实现的 dj_dw = {dj_dw: .6e}, 数值模拟 = {num_dj_dw: .6e}, 误差 = {diff_w: .2e}")
        print(f"你实现的 dj_db = {dj_db: .6e}, 数值模拟 = {num_dj_db: .6e}, 误差 = {diff_b: .2e}")
        print("OK! Gradient check PASSED" if passed else "Gradient check FAILED")

    return passed

test_w = 2
test_b = 1
assert gradient_check(x_train,y_train,test_w,test_b), "梯度检查失败 梯度的实现有问题！！！"


import math
def gradient_descent(x, y, w_in, b_in, alpha, num_iters,
                     cost_function, gradient_function):
    """
    执行梯度下降以拟合线性模型参数 w、b
    将进行 num_iters 次参数更新，学习率为 alpha。

    参数:
      x (ndarray (m,))  : 输入数据，共 m 个样本
      y (ndarray (m,))  : 目标值
      w_in, b_in (float): 参数初始值
      alpha (float)     : 学习率
      num_iters (int)   : 梯度下降迭代次数
      cost_function     : 计算成本 J 的函数
      gradient_function : 计算梯度 (dj_dw, dj_db) 的函数

    返回:
      w (float)        : 迭代结束后的 w
      b (float)        : 迭代结束后的 b
      J_history (list) : 每次迭代的损失值
      p_history (list) : 每次迭代的参数 [w, b]
    """

    # 用于保存损失值和参数历史，便于后续绘图
    J_history = []
    p_history = []

    w, b = w_in, b_in
    dj_dw, dj_db = 0, 0

    for i in range(num_iters):
        # Todo：1. 利用形式参数 gradient_function 计算梯度
        dj_dw, dj_db = gradient_function(x, y, w, b)

        # Todo：2.  按梯度下降公式同步更新参数 w，b
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # 保存成本与参数
        if i < 100_000:            # 防止存储资源消耗过大
            J_history.append(cost_function(x, y, w, b))
            p_history.append([w, b])

        # 每进行总迭代次数的 1/10（或不足 10 次时每次）打印一次训练的信息
        if i % math.ceil(num_iters / 10) == 0:
            print(f"迭代 {i:4d}:  成本 {J_history[-1]:0.2e}  "
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  "
                  f"w: {w: 0.3e}, b: {b: 0.5e}")

    # 返回最终参数及历史记录
    return w, b, J_history, p_history



# 初始化直线的参数
w_init = 0
b_init = 0

# 梯度下降算法 设置迭代次数 和 学习率
iterations = 10000
tmp_alpha = 1.0e-2

# 运行梯度下降
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)
print(f"梯度下降找到的(w,b) : ({w_final:8.4f},{b_final:8.4f})")
```

```python
# TODO： 画 对训练数据拟合的直线 和 训练数据的散点图，画到一张图里
plt.scatter(x_train, y_train, marker="x", c='r',label = "Actual Values")
plt.plot(x_train, w_final*x_train + b_final, c='y',label='Our Prediction')
plt.legend()

plt.show()
# 示例图片：
```


- 损失值和梯度下降迭代次数的关系

损失值与迭代次数的关系图是衡量梯度下降进展的有效指标。在成功的运行中，成本应该始终下降。由于初始成本的变化非常迅速，因此将初始下降过程与最终下降过程以不同的比例绘制是很有用的。在下图中，请注意坐标轴上的损失比例和迭代步长。

```python
# 画损失值 和 迭代次数的关系
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))
ax1.plot(J_hist[:100])
ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])
ax1.set_title("Cost vs. iteration(start)")
ax2.set_title("Cost vs. iteration (end)")
ax1.set_ylabel('Cost')
ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')
ax2.set_xlabel('iteration step')
plt.show()
```


- 预测

现在您已经找到了参数 w 和 b的最优值，现在可以使用模型根据我们学习到的参数来预测没有见过的数据。

```python
# TODO： 自己任意定义不在训练集里的x，用学习到的参数w和b预测x对应的y
x_not_in_train = np.random.randint(11,50,size=10)
y_pre = compute_model_output(x_not_in_train,w_final,b_final)
# plt.scatter(x_not_in_train,y_pre,marker="^",c="b",label="predict")
# # plt.scatter(x_train, y_train, marker="x", c='r',label = "Actual Values")
# # plt.plot(x_train, w_final*x_train + b_final, c='y',label='Our Prediction')
# plt.legend()
#
# plt.show()
# y_pre
for i in range(len(y_pre)):
    print(f"x:{x_not_in_train[i]},预测的y为{y_pre[i]:0.2f}")
```


- 增加学习率alpha

我们讨论了梯度下降中学习率 alpha 的合理取值。𝛼 越大，梯度下降收敛速度越快。但是，如果 𝛼 过大，梯度下降就会发散。上面有一个收敛效果良好的解的例子。
我们试着增加 𝛼 的值，看看会发生什么：

```python
# 初始化参数
w_init = 0
b_init = 0

# 设置学习率
iterations = 10
tmp_alpha = 0.9 # TODO： 设置一个相对较大的alpha （0.8-2之间）

# 运行梯度下降
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,
                                                    iterations, compute_cost, compute_gradient)
```

增加学习率会使得损失越来愈高，即每次学习都不是往梯度下降方向。









### 多个输入值（特征）和一个标签的回归

| 训练样本 $i$ | $x_1$<br/>房屋面积 (ft²) | $x_2$<br/>卧室数 | $x_3$<br/>楼层数 | $x_4$<br/>房龄 (年) | $y$<br/>价格 (× \$1 000) |
| ------------ | ------------------------ | ---------------- | ---------------- | ------------------- | ------------------------ |
| 1            | 2104                     | 5                | 1                | 45                  | 460                      |
| 2            | **1416**                 | **3**            | **2**            | **40**              | 232                      |
| 3            | 1534                     | 3                | 2                | 30                  | 315                      |
| 4            | 852                      | 2                | 1                | 36                  | 178                      |
| …            | …                        | …                | …                | …                   | …                        |

**例子（i = 2）：**
$\vec{x}^{(2)} = [\,1416,\;3,\;2,\;40\,]$
$x^{(2)}_3 = 2$

---

1. 记号说明

| 记号            | 含义                                 |
| --------------- | ------------------------------------ |
| $x_j$           | 第 $j$ 个特征（列）                  |
| $n$             | 特征数量，本例 $n = 4$（$j = 1…4$）  |
| $\vec{x}^{(i)}$ | 第 $i$ 个训练样本的**特征向量**      |
| $x^{(i)}_j$     | 第 $i$ 个样本在第 $j$ 个特征上的取值 |
| $y^{(i)}$       | 第 $i$ 个样本的目标值（房价）        |

---

* 对于样本 $i = 2$：

  * 特征向量 $\vec{x}^{(2)} = [1416,\;3,\;2,\;40]$
  * “楼层数”对应的取值 $x^{(2)}_3 = 2$

2. 多特征线性回归模型

在单变量回归中，我们只有一个特征 $x$。
对 **多特征（多变量）** 情况，假设函数拓展为：

$$
h_\theta(\vec{x}) \;=\; \theta_0 \;+\; \theta_1 x_1 \;+\; \theta_2 x_2 \;+\; \theta_3 x_3 \;+\; \theta_4 x_4
$$
直觉理解：
1. 每一个特征都将影响标签y，参数是负的，说明随着这个特征和标签y是负相关；参数是正的，说明这个特征和标签y是正相关，参数绝对值接近0说明这个特征和标签关系不大
2. 当所有特征都是0时，也有个基础标签值

* 令 $x_0 = 1$（偏置项），可写成向量形式

  $$
  h_\theta(\vec{x}) \;=\; \theta^\top \vec{x}
  $$
  $$
  \begin{aligned}
  \theta^\top \vec{x}
  &= [\,\theta_0,\;\theta_1,\;\theta_2,\;\dots,\;\theta_n\,]
   \begin{bmatrix}
     1 \\ x_1 \\ x_2 \\ \vdots \\ x_n
   \end{bmatrix} \\[4pt]
  &= \theta_0\cdot 1 \;+\; \theta_1x_1 \;+\; \theta_2x_2 \;+\; \dots \;+\; \theta_nx_n
  \end{aligned}
  $$

  

向量点乘是更紧凑的写法，与逐项相加完全等价

* **第一项** $\theta_0\cdot 1$ 就是偏置（intercept）。
* 剩余各项 $\theta_jx_j$（$j=1…n$）对应每个特征的“权重×取值”。
* $\theta = [\,\theta_0,\theta_1,\dots,\theta_n]^\top$ 为模型参数。
---

快速问答，$x^{(4)}_1$指的在示例的数据里，指的指是多少？

答案：852


### 向量化
好处：
1. 代码简洁
2. 能利用线性代数计算的库（numpy）的底层代码（C语言），CPU并行计算，甚至GPU参与运算  -> 代码运行更快

- 参数与特征

* **权重向量** $\mathbf{w} = [\,w_1,\;w_2,\;w_3\,]$ （此处 $n = 3$）

* **偏置（截距）** $b$ — 单个数字

* **特征向量** $\mathbf{x} = [\,x_1,\;x_2,\;x_3\,]$

  **索引小提示**

  * **线性代数**：下标从 1 开始（$w_1,w_2,\dots$）
  * **NumPy / Python**：下标从 0 开始（`w[0]`, `w[1]`, `w[2]`）

```python
# NumPy 表示
w = np.array([1.0,  2.5, -3.3])
b = 4
x = np.array([10, 20, 30])
```

---

- 未向量化的实现

数学公式

$$
f_{\mathbf{w},b}(\mathbf{x}) \;=\; \sum_{j=1}^{n} w_j\,x_j \;+\; b
$$




Python 循环写法（代码冗长且较慢）

```python
f = 0
for j in range(n):      # j = 0 … n‑1
    f += w[j] * x[j]
f += b
```

\-  当 $n = 100{,}000$ 时，这种写法会十分低效。

---

- 向量化的实现

数学公式
$$
f_{\mathbf{w},b}(\mathbf{x}) \;=\; \mathbf{w}\,\cdot\,\mathbf{x} \;+\; b
$$
NumPy 一行代码（简洁且高效）

```python
f = np.dot(w, x) + b
```

通过一次向量点乘即可完成所有乘加运算，强烈建议始终使用向量化写法。

实现也可以往x内添个1， 然后偏置b也放到权重向量里面
| 步骤                         | 说明                                                         |
| ---------------------------- | ------------------------------------------------------------ |
| 1 在特征向量最前面加上常数 1 | 得到 **增广特征向量** <br> $\tilde{\mathbf{x}} = [\,1,\;x_1,\;x_2,\;x_3\,]$ |
| 2 把偏置并入权重向量         | 得到 **增广权重向量** <br> $\tilde{\mathbf{w}} = [\,b,\;w_1,\;w_2,\;w_3\,]^\top$ |
| 3 预测公式                   | $\displaystyle \hat{y} = \tilde{\mathbf{w}} \,\cdot\, \tilde{\mathbf{x}}$ —— 无需再写 `+ b` |

---

- 循环 VS. 向量化 — 为什么 `np.dot` 更快的底层原因

1. 逐项循环在做什么？

* **时间片 $t_0$**：先算 `w[0] * x[0]` 并累加到 `f`
* **时间片 $t_1$**：再算 `w[1] * x[1]` 并累加……
* ……
* **时间片 $t_{15}$**：最后算 `w[15] * x[15]` 并累加
* **缺点**：一次只做一个乘法 + 加法，16 维向量就要跑 16 次循环，对更高维数据或上百万样本更是灾难。


2. 向量化背后发生了什么？

1. **并行乘法**

   * NumPy/底层 BLAS 库会把整段内存中的 16 个 `w[j]` 与 `x[j]` **同时** 送进 CPU 的向量化指令（SIMD）或多核线程。
2. **流水线累加**

   * 乘完后的 16 个结果再通过高效的归约（reduction）操作一次性求和。
3. **结果回写**

   * 整个过程几乎没有 Python 层面的循环开销，而且充分利用了 CPU 缓存和并行度。

==**一句话总结**：==
==把 *“多次循环 + 小标量运算”* 换成 *“一次大规模矩阵/向量运算”*，让底层数值库接管并行化与内存优化，**才能真正扩展到大数据集**。==
![循环和向量化对比](./images/监督学习-回归8.png)


- 注意：梯度下降也要向量化
$\mathbf{w} = [\,b,\;w_1,\;w_2,\;w_3, ... w_{16}\,]$
$\mathbf{d} = [\,d_b,\;d_1,\;d_2,\;d_3\, ... d_{16}]$

w和d都是numpy数组后，直接w - 0.1*d即可


- 快速问答：

  下面的代码在实现，计算线性回归模型的预测值，哪一个实现是向量化的？
1. f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b
2. ```python
   f = 0
   for j in range(n):
       f = f + w[j] * x[j]
   f = f + b
   ```
3. f = np.dot(w,x) + b

答案：3




### 多特征线性回归实验
- 这次实验是之前线性回归实验的扩展，扩展到支持多个输入特征
- 使用矩阵表示训练数据的输入特征
- 重写了预测，损失函数以及计算梯度去支持多个特征
- 使用了向量化的技巧

```python
import copy, math
import numpy as np
import matplotlib.pyplot as plt
np.set_printoptions(precision=2)   # 减少数组里 浮点数显示出来的精度
```

#%% md
下面给出一份更完整、格式一致的 **多特征线性回归记号速查表**，补全了所有 Python 示例变量，并在括号中给出了常见的形状（行 × 列）说明，方便对照代码实现。

| 记号                                 | 含义                                                         | Python 变量名 (示例)               | 典型形状          |
| :----------------------------------- | :----------------------------------------------------------- | :--------------------------------- | :---------------- |
| **通用线性代数记号**                 |                                                              |                                    |                   |
| $a$                                  | 标量（非粗体）                                               | `a`                                | $()$              |
| $\mathbf{a}$                         | 向量（粗体小写）                                             | `a_vec`                            | $(n,)$ 或 $(n,1)$ |
| $\mathbf{A}$                         | 矩阵（粗体大写）                                             | `A_mat`                            | $(m,n)$           |
| **回归数据**                         |                                                              |                                    |                   |
| $\mathbf{X}$                         | 训练样本特征矩阵                                             | `X_train`                          | $(m,n)$           |
| $\mathbf{y}$                         | 训练样本目标向量                                             | `y_train`                          | $(m,)$ 或 $(m,1)$ |
| $\mathbf{x}^{(i)}$                   | 第 $i$ 条样本的特征向量                                      | `X_train[i]`                       | $(n,)$            |
| $y^{(i)}$                            | 第 $i$ 条样本的目标值                                        | `y_train[i]`                       | $()$              |
| $m$                                  | 样本数量                                                     | `m = X_train.shape[0]`             | —                 |
| $n$                                  | 每条样本的特征数                                             | `n = X_train.shape[1]`             | —                 |
| **模型参数**                         |                                                              |                                    |                   |
| $\mathbf{w}$                         | 权重向量                                                     | `w`                                | $(n,)$            |
| $b$                                  | 偏置（截距）                                                 | `b`                                | $()$              |
| **模型与预测**                       |                                                              |                                    |                   |
| $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ | 在样本 $\mathbf{x}^{(i)}$ 上的模型输出：<br>$\displaystyle f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w}\cdot\mathbf{x}^{(i)} + b$ | `f_wb = np.dot(w, X_train[i]) + b` | $()$              |

 **提示**

- 在 NumPy 中，你可以用 `X @ w + b` 一次性计算 **全部** 样本的预测，其中 `X` 形状为 $(m,n)$，`w` 为 $(n,)$。
- 若采用 **增广向量技巧**（在 `X` 前拼接一列 1，并把 `b` 并入权重向量），则预测可写成 `X_tilde @ w_tilde`，再也不用显式写 `+ b`。


- 问题背景
使用 **房价预测** 作为示例。训练数据集现在包含 **7 个样本、4 个特征**（面积、卧室数、楼层数、房龄），数据见下表。
训练好线性回归模型后，就能预测其他房屋的价格，例如「面积 1200 sqft、3 间卧室、1 层、房龄 40 年」的房屋。



- 训练数据集

| 样本 | 面积 (sqft) | 卧室数 | 楼层数 | 房龄 (年) | 价格 (千美元) |
| :--: | :---------: | :----: | :----: | :-------: | :-----------: |
|  1   |    2104     |   5    |   1    |    45     |      460      |
|  2   |    1416     |   3    |   2    |    40     |      232      |
|  3   |     852     |   2    |   1    |    35     |      178      |
|  4   |    1260     |   3    |   1    |    20     |      240      |
|  5   |    3000     |   4    |   2    |     5     |      540      |
|  6   |    1980     |   4    |   1    |    15     |      350      |
|  7   |     890     |   2    |   1    |    60     |      155      |

```python
## 构造 `X_train` 和 `y_train`
import numpy as np

# 特征矩阵：每行一个样本，每列一个特征
X_train = np.array([
    [2104, 5, 1, 45],
    [1416, 3, 2, 40],
    [ 852, 2, 1, 35],
    [1260, 3, 1, 20],
    [3000, 4, 2,  5],
    [1980, 4, 1, 15],
    [ 890, 2, 1, 60]
], dtype=float)

# 目标向量：对应的房价，单位千美元
y_train = np.array([460, 232, 178, 240, 540, 350, 155], dtype=float)

m, n = X_train.shape
print(f"样本数 m = {m},  特征数 n = {n}")
```


所有训练样本会存放在 NumPy 的矩阵 `X_train` 中。矩阵的 **每一行代表一条训练样本**。当你拥有 $m$ 条训练样本（在本例中 $m = 7$），且每条样本包含 $n$ 个特征（本例中 $n = 4$）时，$\mathbf{X}$ 将是一个维度为 $(m,\,n)$ 的矩阵——也就是 **$m$ 行、$n$ 列**：
$$
\mathbf{X} =
\begin{pmatrix}
 x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\
 x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
 \vdots & \vdots & \ddots & \vdots \\
 x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1}
\end{pmatrix}
$$

- 记号说明

* **$\mathbf{x}^{(i)}$**
  第 $i$ 条训练样本对应的**特征向量**：
$$
    \mathbf{x}^{(i)}
    = \bigl(x^{(i)}_0,\;x^{(i)}_1,\;\dots,\;x^{(i)}_{\,n-1}\bigr)
$$
* **$x^{(i)}_j$**
  第 $i$ 条样本中，第 $j$ 个特征的取值。

  * 括号中的上标 $(i)$ 表示**样本编号**
  * 下标 $j$ 表示**该样本中的第 $j$ 个元素**



- 参数向量 $\mathbf{w}$ 与偏置 $b$

* $\mathbf{w}$ 是一个包含 $n$ 个元素的向量

  * 每个元素对应于一个特征的模型参数
  * 在本数据集中，$n = 4$
  * 通常将其写成列向量：
$$
\mathbf{w} = \begin{pmatrix}
w_0 \\
w_1 \\
\vdots \\
w_{n-1}
\end{pmatrix}
$$
* $b$ 是一个标量（单一数字）参数，用作偏置项。



```python
# 假设b_init 和 w_init 就是训练好的参数
b_init =785.1811367994083
w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])
print(f"w_init shape: {w_init.shape}, b_init 类型: {type(b_init)}")
```

多变量线性回归模型的预测公式为
$$
f_{\mathbf{w},b}(\mathbf{x})
= w_0 x_0 + w_1 x_1 + \dots + w_{n-1} x_{\,n-1} + b
\tag{1}
$$
也可以写成向量形式
$$
f_{\mathbf{w},b}(\mathbf{x})
= \mathbf{w} \cdot \mathbf{x} + b
\tag{2}
$$
其中 “$\cdot$” 表示**向量点积**。

```python
def predict(x, w, b):
    """
    线性回归的单样本预测函数

    参数
    ----
    x : ndarray, 形状 (n,)
        输入样本，包含 n 个特征
    w : ndarray, 形状 (n,)
        模型权重参数
    b : 标量
        模型偏置参数

    返回
    ----
    p : 标量
        预测结果
    """
    p = np.dot(x, w) + b
    return p
```

```python
x_vec = X_train[0,:]
print(f"x_vec shape {x_vec.shape}, x_vec 值: {x_vec}")

f_wb = predict(x_vec,w_init, b_init)
print(f"f_wb shape {f_wb.shape}, 预测: {f_wb}")	
```

- 多变量线性回归的代价函数

多特征情况下的代价函数 $J(\mathbf{w},b)$ 定义为
$$
J(\mathbf{w},b)=\frac{1}{2m}\sum_{i=0}^{m-1}
\Bigl(f_{\mathbf{w},b}\bigl(\mathbf{x}^{(i)}\bigr)-y^{(i)}\Bigr)^{2}
\tag{3}
$$
其中
$$
f_{\mathbf{w},b}\bigl(\mathbf{x}^{(i)}\bigr)=
\mathbf{w}\,\cdot\,\mathbf{x}^{(i)}+b
\tag{4}
$$
与之前的实验不同，这里 $\mathbf{w}$ 和 $\mathbf{x}^{(i)}$ 都是 **向量**（而非单个标量），以支持包含多个特征的输入。



```python
def compute_cost(X, y, w, b):
    """
    计算损失
    Args:
      X (ndarray (m,n)): 训练集的输入, m 样本 with n 特征
      y (ndarray (m,)) : 训练集目标
      w (ndarray (n,)) : 模型参数 w
      b (scalar)       : 模型参数 b

    Returns:
      cost (scalar): 损失值
    """
    m = X.shape[0]
    cost = 0.0

    # TODO： 根据公式实现损失函数
    return cost

w_test = np.ones((4,))
b_test = 10
cost_answer = 1117012.2142857143
assert np.isclose(compute_cost(X_train,y_train,w_test,b_test), cost_answer), "compute cost实现有问题"
```

- 多变量的梯度下降法

梯度下降更新规则：
$$
\begin{align*}
\text{重复执行，直到收敛：}\;\bigl\{ \\
& w_j \;=\; w_j \;-\; \alpha \;\frac{\partial J(\mathbf{w},b)}{\partial w_j}
\qquad\text{对 } j = 0 \dots n-1 \tag{5}\\[4pt]
& b \;=\; b \;-\; \alpha \;\frac{\partial J(\mathbf{w},b)}{\partial b} \\
\bigr\}
\end{align*}
$$
其中 $n$ 为特征数量，参数 $w_j$ 与 $b$ **同时** 更新。各偏导数为
$$
\begin{align}
\frac{\partial J(\mathbf{w},b)}{\partial w_j}
&=\frac{1}{m}\sum_{i=0}^{m-1}
\bigl(f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)}\bigr)\,
x_{j}^{(i)} \tag{6}\\[6pt]
\frac{\partial J(\mathbf{w},b)}{\partial b}
&=\frac{1}{m}\sum_{i=0}^{m-1}
\bigl(f_{\mathbf{w},b}(\mathbf{x}^{(i)})-y^{(i)}\bigr) \tag{7}
\end{align}
$$
* $m$：训练样本数
* $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$：模型对第 $i$ 条样本的预测值
* $y^{(i)}$：第 $i$ 条样本的真实目标值



```python
def compute_gradient(X, y, w, b):
    """
    Computes the gradient for linear regression
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters
      b (scalar)       : model parameter

    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.
    """
    m,n = X.shape           #(number of examples, number of features)

    # TODO： 根据公式实现梯度
    err = X@w + b - y
    dj_dw = X.T @ err / m
    dj_db = np.sum(err) / m
    return dj_dw, dj_db
```

```python
def gradient_check(x, y, w, b, eps=1e-7, tol=1e-2, verbose=True):
    """
    使用数值差分验证 compute_gradient 是否正确
    Args:
        x, y : 数据集
        w, b : 当前参数, w是ndarray，b是float
        eps  : 微小扰动 ε，用于近似数值梯度
        tol  : 容差阈值，|解析梯度 - 数值梯度| < tol 视为通过
        verbose: 打印详细差异
    Returns:
        passed (bool) : 是否通过梯度检查
    """
    # 1) 解析梯度
    dj_dw, dj_db = compute_gradient(x, y, w, b)

    # 2) 数值梯度——中心差分
    n = x.shape[1]

    num_dj_dw = np.zeros((n,))

    for i in range(n):
        eps_vec = np.zeros((n,))
        eps_vec[i] = eps
        num_dj_dw[i] =(compute_cost(x, y, w + eps_vec, b) -
                 compute_cost(x, y, w - eps_vec, b)) / (2 * eps)

    num_dj_db = (compute_cost(x, y, w, b + eps) -
                 compute_cost(x, y, w, b - eps)) / (2 * eps)

    # 3) 误差
    diff_w = np.abs(dj_dw - num_dj_dw)
    diff_b = abs(dj_db - num_dj_db)
    passed = np.all(diff_w < tol) and diff_b < tol

    if verbose:
        print(f"你实现的 dj_dw = {dj_dw}, 数值模拟 = {num_dj_dw}, 误差 = diff_w: {diff_w}")
        print(f"你实现的 dj_db = {dj_db: .6e}, 数值模拟 = {num_dj_db: .6e}, 误差 = {diff_b: .2e}")
        print("OK! Gradient check PASSED" if passed else "Gradient check FAILED")

    return passed

# 梯度检查
w_test = np.ones((4,))
b_test = 10
assert gradient_check(X_train,y_train,w_test,b_test), "梯度检查失败 梯度的实现有问题！！！"
```

```python
# 使用梯度下降算法
def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function,
                     alpha, num_iters):
    """
    批量梯度下降（Batch Gradient Descent）
    --------------------------------------
    循环执行 num_iters 轮参数更新，通过梯度下降学习权重 w 与偏置 b。

    参数
    ----
    X : ndarray (m, n)
        训练数据，m 行样本、n 列特征
    y : ndarray (m,)
        目标值（标签）
    w_in : ndarray (n,)
        权重向量的初始值
    b_in : 标量
        偏置的初始值
    cost_function : callable
        计算代价 J 的函数
    gradient_function : callable
        计算代价函数梯度的函数
    alpha : float
        学习率
    num_iters : int
        梯度下降的迭代次数

    返回值
    ------
    w : ndarray (n,)
        训练完毕后的权重向量
    b : 标量
        训练完毕后的偏置
    J_history : list
        每一次迭代计算得到的代价，用于绘图或分析
    """

    # 用于记录每次迭代的 J 以及 w，方便后续绘图
    J_history = []

    # 深拷贝，避免在函数内部修改外部变量
    w = copy.deepcopy(w_in)
    b = b_in

    for i in range(num_iters):

        # 计算梯度，并更新参数
        dj_dw, dj_db = gradient_function(X, y, w, b)

        # 梯度下降：按照学习率 alpha 更新 w 和 b
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # 记录当前代价（防止资源耗尽，只保存前 100000 步）
        if i < 100000:
            J_history.append(cost_function(X, y, w, b))

        # 每迭代总次数的 1/10 打印一次代价
        if i % math.ceil(num_iters / 10) == 0:
            print(f"迭代 {i:4d} 次：代价 {J_history[-1]:8.2f}")

    # 返回最终参数和代价历史，便于绘图
    return w, b, J_history
```

```python
# initialize parameters
initial_w = np.zeros_like(w_init)
initial_b = 0.
# some gradient descent settings
iterations = 1000
alpha = 5.0e-7
# run gradient descente)
w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,
                                                    compute_cost, compute_gradient,
                                                    alpha, iterations)

print(f"通过梯度下降找到的b,w : {b_final:0.2f},{w_final} ")
m,_ = X_train.shape
for i in range(m):
    print(f"预测值: {np.dot(X_train[i], w_final) + b_final:0.2f}, 目标值: {y_train[i]}")
```

```python
# plot cost vs 迭代次数
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
ax1.plot(J_hist)
ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])
ax1.set_title("Cost vs. iteration")
ax2.set_title("Cost vs. iteration (tail)")
ax1.set_ylabel('Cost')
ax2.set_ylabel('Cost')
ax1.set_xlabel('iteration step')
ax2.set_xlabel('iteration step')
```

上述实验的模型预测的不太准确，损失值还是比较大，后面要讨论如何继续优化它





## 机器学习之数据处理

### 特征缩放

- 问题：为什么特征的大小尺度很不一样，会导致梯度下降算法变慢？ 为什么把特征缩放到同一个范围内，比如[-1,1],[0,1]会解决梯度下降算法变慢的问题？（从直觉上理解即可）

 1. 线性模型与贡献项

对于多特征线性模型
$$
\hat{y}= w_1x_1+w_2x_2+\dots+w_nx_n+b,
$$
* **每一项 $w_jx_j$** 都在为预测值贡献“加权分数”。
* 如果 $x_j$ 的取值范围比其他特征大得多（如 0 – 3000），那么 **同样大小的权重 $w_j$** 会让这一项产生 **远超其他项** 的数值贡献。

  * 例：面积 $x_1=2000$、卧室数 $x_2=3$。即使 $w_1=w_2=1$，也会有 $w_1x_1 =2000$ 与 $w_2x_2 =3$ 的悬殊差距。
  * 这样会导致预测值几乎只由面积控制，梯度也会优先生长或压缩 $w_1$，其余权重更新“存在感”微弱。


2. 损失函数对“大贡献项”的“惩罚”

以平方损失为例
$$
J(\mathbf{w},b)=\frac1{2m}\sum_{i}( \hat{y}^{(i)}-y^{(i)})^2.
$$
* 如果某个特征项 $w_jx_j$ 过大，$\hat{y}$ 很容易被它**推离**真值 $y$，损失急剧上升。
* 因为损失对误差 $e$ **平方放大**，优化器为了迅速降低 $J$，会对引起大误差的那一项**优先下手**：
$$
  \frac{\partial J}{\partial w_j}=\frac1m\sum_i e^{(i)}x_j^{(i)}.
$$
  * 这里的梯度与 $x_j$ **成正比**，数值范围大的特征让梯度也变大，从而导致 **更激进的权重更新**。
* 结果就是：

  * **陡峭方向（大特征）** → 梯度巨大 → 权重调整幅度大，为了抑制贡献而被“拉回”。
  * **平缓方向（小特征）** → 梯度较小 → 权重缓慢更新。
* 在相同学习率 $\alpha$ 下，参数向量会先在陡峭方向来回“过冲—回拉”，同时沿平缓方向慢慢滑动——蜿蜒的轨迹。


3. 为什么缩放能“抚平”这种差距？

    **统一量级后**，所有 $x_j$ 大约落在 $[-1,1]$ 或 $[0,1]$：

    * $\partial J / \partial w_j$ 的幅度不会因为“谁更大”而相差几百倍。
     * 各方向步长相近 → 更新轨迹趋向**径向直线**，收敛速度显著提升。

4. 小结

* **大的特征 $\bigl(x_j \gg x_k\bigr)$**：

  * 同样的权重会导致 **预测贡献极大**，损失函数随之陡峭。
  * 为了快速降损失，优化器会对该权重 **过度惩罚/大幅摆动**——看似把 $w_j$ “限制” 在较小值。
* **梯度 zig‑zag** 本质：

  * 在陡峭（大特征）方向剧烈跳跃，在平缓（小特征）方向慢慢爬坡。
* **解决方式**：
   特征缩放 / 标准化



==**总结**：==
==线性模型里，特征值越大，权重微小变动就能放大误差；损失函数因此“惩罚”它、梯度猛拉它，形成陡峭与平缓方向的极端差异，导致梯度下降之字形前进——缩放后地形变圆，更新才会顺畅。==



#### 特征缩放的实现
1. 除以最大值
2. 均值归一化（Mean normalization）
3. Z-score标准化（Z-score normalization）

```python
import numpy as np

# 原始特征矩阵：每行一个样本，列 0=面积，列 1=卧室数
X = np.array([
    [2104, 5],
    [1416, 3],
    [ 852, 2]
], dtype=float)
print("原始 X:\n", X)

# 1. 按最大值缩放
X_max = X / X.max(axis=0)        # 按列除以各自的最大值
print("\n按最大值缩放:\n", X_max)
# 做法：每个特征值除以该列最大值。
# 范围：[0,1] 若所有值 ≥ 0。
# 特点实现最简单，但如果后续出现大于训练集最大值的样本，缩放后值会超出 1。

# 2. 均值归一化
X_mean = (X - X.mean(axis=0)) / (X.max(axis=0) - X.min(axis=0))
print("\nMean normalization:\n", X_mean)
# 做法：先减去列均值，再除以列范围（max-min）
# 范围：大约落在[-1,1]
# 特点：居中到0，且保留相对大小；若分布极端偏斜，效果可能一般。

# 3. Z‑Score 标准化
x_zscore = (X - X.mean(axis=0)) / X.std(axis=0)
print("Z-score 标准化:", x_zscore)
# 做法：减去均值，再除以标准差。
# 范围：均值0，标准差1；数值可正可负，通常在[-3,3]区间
# 特点：对正态分布型（或近似）数据最友好，常用的缩放方法
```

- 什么时候考虑缩放数据？
| 原始特征范围               | 评估         | 操作建议          |
| -------------------------- | ------------ | ----------------- |
| $0 \le x_1 \le 3$          | ✅ 范围小     | **不用缩放**      |
| $-2 \le x_2 \le 0.5$       | ✅ 居中且适中 | **不用缩放**      |
| $-100 \le x_3 \le 100$     | ⚠️ 过大       | **需缩放**        |
| $-0.001 \le x_4 \le 0.001$ | ⚠️ 过小       | **需放大/标准化** |
| $98.6 \le x_5 \le 105$     | ⚠️ 偏大       | **需缩放**        |

### 确保梯度下降在**正常工作**

| 目标         | 说明                                         |
| ------------ | -------------------------------------------- |
| **优化目标** | 最小化代价函数 $J(\mathbf{w}, b)$            |
| **学习曲线** | 每一次迭代都应使 $J(\mathbf{w}, b)$ **递减** |

> 迭代次数的需求差异很大：30、1 000 甚至 100 000 步都可能出现，取决于学习率和问题规模。



#### 自动收敛判定（伪代码）

```text
设 ε = 1e-3  (收敛阈值)

若  J(t−1) − J(t)  ≤ ε:
    判定为收敛
    -> 已找到使 J 接近全局最小的 (w, b)
```

* **核心思想**：若一次迭代带来的代价下降幅度不超过 ε（例如 0.001），说明模型已基本到达最低点附近，可停止训练。



**实践提醒**

* 学习率过大：$J$ 可能震荡甚至上升。
* 学习率过小：$J$ 虽下降，但收敛速度慢，曲线“拖尾”。
* 建议在训练中实时绘制 **学习曲线**，以及设置 ε‑收敛检验，以便动态调整超参数。




### 选择学习率
1. 如何判断梯度下降是否“出问题”  — 学习率与 Bug 的快速排查

| 现象                                        | 可能原因                                                     | 应对策略                                                  |
| ------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------- |
| **锯齿状上下波动**<br/>（但总体仍缓慢下降） | - 学习率 α 偏大，更新步长过猛，在谷底两侧来回跳 ↔            | ↓ **减小 α**，直至曲线平滑单调降低                        |
| **J 指数级上升**                            | 1. **符号写反**：<br/> `w = w + α · ∂J` ❌<br/> 应写 `w = w − α · ∂J` ✅<br/>2. **α 远大于最优值** | - 先检查代码是否忘记减号<br/>- 若逻辑无误，显著**调小 α** |
| **J 单调下降，但非常慢**                    | 学习率 **过小**，每步只挪一点点                              | ↑ **逐步增大 α**，观察到平滑但更陡的下降即可              |


- 总结：
    1. **先看学习曲线**：若 $J(\mathbf{w},b)$ 每次迭代都应下降；出现震荡或上升即有问题。
    2. **排 Bug**：确认更新公式是 **“减去”** 梯度，而非加。
    3. **调学习率**：

   * 震荡/爆炸 → **减小 α**
   * 缓慢爬坡 → **增大 α**
    4. **经验法则**：找到一个让 $J$ 在每步稳步下降、且下降速度尽可能快的 α；再配合 ε 收敛检测，既高效又安全。


2. 学习率选择简单策略

<img alt="学习率选择" height="600" src="./images/监督学习-回归9.png" />




### 特征工程

- 使用直觉，和对数据+业务的理解，设计新的特征
- 新的特征通过对原始特征转换，或者组合原始的特征得到
- 特征工程 + 线性回归 可以得出不仅仅对数据拟合直线，可以拟合多项式曲线

![特征工程后的多项式回归](./images/监督学习-回归10.png)



```python
import numpy as np
import matplotlib.pyplot as plt
import math
np.set_printoptions(precision=2)
```

```python
# 使用梯度下降算法
def gradient_descent(X, y,  alpha, iterations):
    """
    批量梯度下降（Batch Gradient Descent）
    --------------------------------------
    循环执行 iterations 轮参数更新，通过梯度下降学习权重 w 与偏置 b。

    参数
    ----
    X : ndarray (m, n)
        训练数据，m 行样本、n 列特征
    y : ndarray (m,)
        目标值（标签）
    alpha : float
        学习率
    iterations : int
        梯度下降的迭代次数

    返回值
    ------
    w : ndarray (n,)
        训练完毕后的权重向量
    b : 标量
        训练完毕后的偏置
    J_history : list
        每一次迭代计算得到的代价，用于绘图或分析
    """

    def cost_function(X, y, w, b):
        m = X.shape[0]
        err = np.dot(X, w) + b - y
        return np.sum(err**2) / (2*m)

    def gradient_function(X, y, w, b):
        m,n = X.shape
        err = np.dot(X,w) + b - y
        dj_dw = X.T @ err / m
        dj_db = np.sum(err) / m
        return dj_dw, dj_db

    # 用于记录每次迭代的 J 以及 w，方便后续绘图
    _, n = X.shape
    J_history = []
    w = np.zeros((n,))
    b = 0

    for i in range(iterations):

        # 计算梯度，并更新参数
        dj_dw, dj_db = gradient_function(X, y, w, b)

        # 梯度下降：按照学习率 alpha 更新 w 和 b
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # 记录当前代价（防止资源耗尽，只保存前 100000 步）
        if i < 100000:
            J_history.append(cost_function(X, y, w, b))

        # 每迭代总次数的 1/10 打印一次代价
        if i % math.ceil(iterations / 10) == 0:
            print(f"迭代 {i:4d} 次：代价 {J_history[-1]:8.2f}")

    # 返回最终参数和代价历史，便于绘图
    return w, b, J_history
```

```python
# 构造一个简单的多项式 y = 1+x**2

x = np.arange(0, 20, 1)
y = 1 + x**2
X = x.reshape(-1, 1)

model_w,model_b, _= gradient_descent(X,y,iterations=1000, alpha = 1e-2)

plt.scatter(x, y, marker='x', c='r', label="Actual Value")
plt.title("no feature engineering")
plt.plot(x,X@model_w + model_b, label="Predicted Value")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.show()
```


如您所料，当前模型拟合效果并不好。我们需要的其实是类似
$$
y = w_0\,x_0^{\,2} + b
$$
这样的 **多项式特征**。
实现方式是对**输入数据进行特征工程**：把原始特征 $x$ 替换为 $x^{2}$。也就是说，只要把训练数据中的 `X` 换成 `X**2`，就能让模型学习到
$$
y = w_0\,x_0^{\,2} + b
$$
的关系。
下面尝试一下：在代码里把 `X` 替换为 `X**2`。

```python
# 随堂练习：把上面的X换成X**2， 然后重新执行上个代码的流程：跑梯度下降得出参数，画图，得出如下图片：

```


在上面的例子里，我们知道需要加入 $x^2$ 项，但在实际问题中，并不总能一眼看出该添加哪些特征。通常可以一次性构造出多种候选特征，再通过训练筛选出最有用的。

举个例子：如果我们尝试这样的模型会怎样？
$$
y = w_0x_0 \;+\; w_1x_1^{\,2} \;+\; w_2x_2^{\,3} \;+\; b
$$
```python
x = np.arange(0, 20, 1)
y = x**2

X = np.column_stack([x, x**2, x**3])  # np.c_[x, x**2, x**3]，np.vstack([x, x**2, x**3]).T可以达到一样的效果
print(X.shape)
```

```python
model_w,model_b,_ = gradient_descent(X, y, iterations=10000, alpha=1e-7)

plt.scatter(x, y, marker='x', c='r', label="Actual Value")
plt.title("x, x**2, x**3 features")
plt.plot(x, X@model_w + model_b, label="Predicted Value")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()

print(f"梯度下降找到的w，b分别是：w:{model_w}, b:{model_b}")
```


注意到训练后参数 $\mathbf{w} = [0.08,\; 0.54,\; 0.03]$，偏置 $b = 0.0106$。
这说明最终模型为
$$
0.08\,x \;+\; 0.54\,x^{2} \;+\; 0.03\,x^{3} \;+\; 0.0106
$$
在梯度下降过程中，算法通过**增大 $w_1$**、相对**减小 $w_0$ 与 $w_2$**，把重点放在最能拟合数据的 $x^{2}$ 项上。如果训练足够久，其余权重还会继续减小。

**梯度下降会自动“挑选”合适的特征** —— 通过放大对应的参数、压低无用特征的参数。

**要点**

* 权重越小 → 该特征越不重要；若权重趋近于 0，则该特征几乎对模型无用。
* 在本例中，拟合后与 $x^{2}$ 对应的权重（0.54）远大于 $x$ 和 $x^{3}$ 的权重，说明 $x^{2}$ 最能帮助模型贴合数据。



- 另一种视角

在上文中，我们根据多项式特征是否能很好地拟合目标数据来挑选它们。换个角度看，一旦我们构造出新的特征，本质上仍在使用**线性回归**。既然如此，最优特征应当与目标变量呈“线性关系”。用一个具体示例来理解

```python
#%%
x = np.arange(0, 20, 1)
y = x**2

# engineer features .
X = np.c_[x, x**2, x**3]   #<-- added engineered feature
X_features = ['x','x^2','x^3']
```

```python
fig,ax=plt.subplots(1, 3, figsize=(12, 3), sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X[:,i],y)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel("y")
plt.show()
```

从上面的结果可以清楚地看出，$\,x^{2}$ 特征与目标值 $y$ 呈线性关系，因此线性回归可以轻松利用这一特征来生成模型。

- 特征缩放

当数据集中各特征的数值范围差异很大时，应该进行特征缩放，以加快梯度下降的收敛速度。在前面的例子中，存在 $x$、$x^{2}$ 和 $x^{3}$ 三个特征，它们的量级天生相差悬殊。下面，我们就对这些特征应用 **Z‑score 标准化** 来做演示。

```python
def zscore_normalize_features(X):
    return (X - X.mean(axis=0)) / X.std(axis=0)
```

```python
x = np.arange(0,20,1)
y = x**2
X = np.c_[x, x**2, x**3]
print(f"列和列之间的峰值差（peak to peak）   原始的X:{np.ptp(X,axis=0)}")

# add mean_normalization
X = zscore_normalize_features(X)
print(f"列和列之间的峰值差（peak to peak），缩放后的 X:{np.ptp(X,axis=0)}")
```

```python
# 现在梯度基本都一样小，可以用更大的学习率
model_w, model_b, _ = gradient_descent(X, y, iterations=100000, alpha=1e-1)

plt.scatter(x, y, marker='x', c='r', label="Actual Value")
plt.title("Normalized x x**2, x**3 feature")
plt.plot(x,X@model_w + model_b, label="Predicted Value")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.show()

print(f"梯度下降找到的w，b分别是：w:{model_w}, b:{model_b}")
```

特征缩放使模型收敛速度显著提升。
再看一次参数向量 $\mathbf{w}$ 的数值：对应于 $x^{2}$ 项的 $w_1$ 最为突出，而梯度下降几乎将 $x^{3}$ 项的权重压到趋近于零。


- 复杂函数也能“线性”建模 —— 以 cos 函数的泰勒逼近为例

在特征工程的帮助下，即使原本高度非线性的目标函数，也可以转化为**线性回归**可处理的问题。方法就是：

1. **展开函数**（泰勒级数）；
2. **把展开式中的每一项当作一个特征**；
3. 用线性回归去学习这些特征的权重。
$$
\cos x \;=\; 1 \;-\; \frac{x^{2}}{2!} \;+\; \frac{x^{4}}{4!} \;-\; \frac{x^{6}}{6!} \;+\; \cdots
$$


```python
x = np.arange(0,20,1)
y = np.cos(x/2)

# 随堂练习：对x做特征工程，比如可以组合x，x**2，...到 x**15，利用梯度下降训练这些特征的参数，训练好后，看拟合的曲线是否接近np.cos(x/2）， （画散点+折线图）
```



## logistic回归学习

### 二元分类介绍
| 模块             | 关键内容                                                     | 讲解要点                                                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **什么是分类？** | 用模型回答“是 / 否”或“属于哪一类”的问题                      | 结果 **$y$** 不是连续数，而是离散类别                        |
| **示例问题**     | - 这封邮件是垃圾邮件吗？<br>- 这笔交易是欺诈吗？<br>- 这枚肿瘤是恶性的吗？ | 每个问题的答案只可能是 **“是 / 否”**                         |
| **输出标签 $y$** | 只能取 **两种值** → 称为 **二元分类**                        | 通常用数字表示：<br>- **0** → False / “否”<br>- **1** → True  / “是” |
| **正负类别**     | - **正类 (positive class)**：用 **1** 表示，含义是“该现象存在”<br>- **负类 (negative class)**：用 **0** 表示，含义是“该现象缺失” | “正 / 负”与“好 / 坏”无必然对应，只代表有无                   |
| **为何用 0/1？** | - 便于模型计算（如逻辑回归的 Sigmoid 输出介于 0–1）<br>- 损失函数、评估指标都依赖数值编码 |                                                              |


- 为什么线性回归模型在二元分类问题上很糟糕？

1. 线性模型不会把输出限制到 0-1
2. 分类问题需要决策边界：f(x)<0.5 -> x对应的分类是0； f(x)>=0.5 -> x对应的分类是1
3. 由于分类问题的y只有0/1， 很容易来一个新的数据后，对线性模型的参数有大调整，导致决策边界出现很大变化，导致分类的表现更差，而实际情况是这个新数据的出现不应该去改变决策边界

线性回归模型在分类问题上的糟糕表现，导致需要其他的数学模型处理分类问题




### logistic回归

需要最终输出0-1的值

**Sigmoid / Logistic 函数**
$$
g(z) = \frac{1}{1 + e^{-z}}, \qquad 0 < g(z) < 1
$$
**线性组合**
$$
z = \mathbf{w}\cdot\mathbf{x} + b
$$
**逻辑回归输出**
$$
f_{\,\mathbf{w},\,b}(\mathbf{x})
  = g\!\bigl(\mathbf{w}\cdot\mathbf{x} + b\bigr)
  = \frac{1}{1 + e^{-\bigl(\mathbf{w}\cdot\mathbf{x} + b\bigr)}}
$$
- 逻辑回归输出值的解读

| 线性组合 → Sigmoid                                           | 输出含义                                                     |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $f_{\,\mathbf{w},\,b}(\mathbf{x}) \;=\; \frac{1}{1+e^{-(\mathbf{w}\!\cdot\!\mathbf{x}+b)}}$ | 这就是 **“预测为 1 的概率”**，记作  $\displaystyle P\bigl(y=1 \mid \mathbf{x};\mathbf{w},b\bigr)$ |

* 记号：

  * $\mathbf{x}$：输入特征向量
  * $y$：标签，0 或 1
  * $\mathbf{w}, b$：模型参数



- 概率视角
$$
  f_{\,\mathbf{w},\,b}(\mathbf{x})
  = P(y=1 \mid \mathbf{x};\mathbf{w},b)
$$
* 由于 Sigmoid 输出范围 $0\!<\!g(z)\!<\!1$，自然可解释为概率。
* 有 概率完备性：4
$$
  P(y=0) + P(y=1) = 1
$$
- 例子

  **例子：肿瘤直径预测恶性**

  * 特征 $x$：肿瘤直径（cm）
  * 标签 $y$：

    * 0 ➔ 良性 (not malignant)
    * 1 ➔ 恶性 (malignant)



若模型给出
$$
f_{\,\mathbf{w},\,b}(\mathbf{x}) = 0.7
$$
则可解读为：**该肿瘤为恶性的概率为 70%**。

* 若设阈值 0.5，则判定 $y=1$（恶性）。




### 决策边界


给定逻辑回归模型
$$
f_{\,\mathbf{w},\,b}(\mathbf{x}) = g\!\bigl(\mathbf{w}\!\cdot\!\mathbf{x}+b\bigr)
                                  = \frac{1}{1 + e^{-\bigl(\mathbf{w}\!\cdot\!\mathbf{x}+b\bigr)}} ,
$$
它可解释为
$$
f_{\,\mathbf{w},\,b}(\mathbf{x})
     \;=\;
     P\!\bigl(y = 1 \mid \mathbf{x}; \mathbf{w}, b\bigr),
$$
即“在输入 $\mathbf{x}$ 下，$y$ 为 1 的概率”。


- 0 或 1？—— 使用阈值 (threshold)

**选择阈值 0.5**：
$$
    \hat{y} \;=\;
    \begin{cases}
      1, & f_{\,\mathbf{w},\,b}(\mathbf{x}) \,\ge\, 0.5 \\
      0, & f_{\,\mathbf{w},\,b}(\mathbf{x}) \,<\, 0.5
    \end{cases}
$$
- 进一步推导何时 $f_{\,\mathbf{w},\,b}(\mathbf{x}) \ge 0.5$：
$$
  \begin{aligned}
    f_{\,\mathbf{w},\,b}(\mathbf{x}) \,\ge\, 0.5
      \ &\Longleftrightarrow\;
      g(z) \,\ge\, 0.5 \\
      \ &\Longleftrightarrow\;
      z = \mathbf{w}\!\cdot\!\mathbf{x} + b \,\ge\, 0 ,
  \end{aligned}
$$
  于是有
$$
    \hat{y} \;=\;
    \begin{cases}
      1, & \mathbf{w}\!\cdot\!\mathbf{x} + b \ge 0 \\
      0, & \mathbf{w}\!\cdot\!\mathbf{x} + b < 0 .
    \end{cases}
$$
- **结论**：
  决策边界由超平面 $\mathbf{w}\!\cdot\!\mathbf{x} + b = 0$ 给出；Sigmoid + 阈值 0.5 等价于判断该线性函数的符号。



- 决策边界的例子

![决策边界图](./images/监督学习-分类1.png)
![决策边界图-非线性](./images/监督学习-分类2.png)



![决策边界图-非线性2](./images/监督学习-分类3.png)

- 快速问答：假设你正在开发一款肿瘤检测算法，目的是先行筛查出可能的肿瘤并提交给专家进一步检查。那么，该把判定阈值设置为多少才合适？
1. 较高，比如阈值设置为0.9
2. 较低，比如阈值设置为0.2  

答案：2


### 代码绘制决策边界

```python
import numpy as np
import matplotlib.pyplot as plt
```

* **数据集**

假设你有如下训练数据集：

* 输入变量 `X` 是一个 NumPy 数组，包含 6 条训练样本，每条样本有两个特征
* 输出变量 `y` 也是一个 NumPy 数组，同样包含 6 条样本，其取值仅为 `0` 或 `1`

```python
X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y = np.array([0, 0, 0, 1, 1, 1]).reshape(-1,1)
```

```python
# 随堂练习：把X里的每个样本（x0,x1)看成坐标绘制散点图，要求样本对应的分类是0画圆，样本对应的分类是1画叉，绘图如下：
fig,ax = plt.subplots(1,1,figsize=(4,4))

# TODO: 在ax上绘图
ax.scatter(X[y[:,0]==0 , 0],X[y[:,0] == 0 , 1],marker = 'o' , color = 'b')
ax.scatter(X[y[:,0]==1 , 0],X[y[:,0] == 1 , 1],marker = 'x' , color='r')
# ------- 这里不用修改 ---------------
# 设置坐标刻度，x：0-4， y：0-3.5
ax.axis([0,4,0,3.5])
plt.show()
```

```python
# 随堂练习：把X里的每个样本（x0,x1)看成坐标绘制散点图，要求样本对应的分类是0画圆，样本对应的分类是1画叉，绘图如下：
fig,ax = plt.subplots(1,1,figsize=(4,4))

# TODO: 在ax上绘图
ax.scatter(X[y[:,0]==0 , 0],X[y[:,0] == 0 , 1],marker = 'o' , color = 'b')
ax.scatter(X[y[:,0]==1 , 0],X[y[:,0] == 1 , 1],marker = 'x' , color='r')
# ------- 这里不用修改 ---------------
# 设置坐标刻度，x：0-4， y：0-3.5
ax.axis([0,4,0,3.5])
plt.show()
```

* **绘制决策边界**

现在看逻辑回归模型是如何进行预测的。

* 我们的逻辑回归模型为
$$
  f(\mathbf{x}) \;=\; g\!\bigl(-3 + x_0 + x_1\bigr)
$$
* 根据前面的推导，当
$$
  -3 + x_0 + x_1 \;\ge\; 0
$$
  时，模型预测 $y = 1$。

下面用图形方式展示这一点。首先绘制方程
$$
-3 + x_0 + x_1 = 0
$$
这等价于
$$
x_1 = 3 - x_0.
$$
```python

fig,ax = plt.subplots(1,1,figsize=(5,4))
# 随堂练习：
# 1. 把刚才 的把X里的数据看成坐标 画成根据y分类绘制的散点图的代码，定义到一个函数里 ，直接调用函数绘制数据. 函数参数接收 X,y 和 ax（子图对象）
# TODO
def plotData(X,y,ax):
    y = y.reshape(-1,1)
    ax.scatter(X[y[:,0] == 0 ,0], X[y[:,0] == 0 ,1],marker = 'o', color = 'b',label = 'y=0')
    ax.scatter(X[y[:,0] == 1 ,0], X[y[:,0] == 1 ,1],marker = 'x', color = 'r',label = 'y=1')
    plt.legend()
    plt.xlabel('x1')
    plt.ylabel('x2')

    return None

x0 = np.arange(0, 6)
# 2. 根据公式 x1 = 3-x0，画决策边界
# TODO
x1 = 3 - x0
ax.plot(x0, x1, color = "orange")

plotData(X,y,ax)
# ------- 这里不用修改 ---------------
ax.axis([0, 4, 0, 3.5])
ax.fill_between(x0,x1, alpha=0.2) # 填充在线下面的区域
plt.show()

# 两个练习实现正确，会得到如下图片：
```


### 损失函数

逻辑回归不适用线性回归的损失函数：$$
J(\mathbf{w},b)=\frac{1}{2m}\sum_{i=0}^{m-1}
\Bigl(f_{\mathbf{w},b}\bigl(\mathbf{x}^{(i)}\bigr)-y^{(i)}\Bigr)^{2}
\tag{3}$$

1. **损失函数形状**

   * 线性回归的损失函数是碗形（convex）  —— 只有一个最低点。往下滚一定能滚到谷底。
   * 逻辑回归也应用平方误差做损失函数的话，则不是碗形，会有很多局部最小值，梯度下降可能陷入局部最小值

2. **用途不同**

   * 线性回归要预测一个连续数值，用“距离差的平方”来衡量误差最直观。
   * 逻辑回归要预测 0‑1 分类／概率，不需要把误差平方；而是关注分类的概率

所以：**同样的平方误差损失函数，在线性回归是好用的碗形函数；放到逻辑回归就变成很多局部最小值，找不到最好解**——这就是为什么逻辑回归通常改用其他函数


- 单个样本的损失定义：
$$
L\!\bigl(f_{\vec w,b}(\vec x^{(i)}),\,y^{(i)}\bigr)=
\begin{cases}
-\;\log\!\bigl(f_{\vec w,b}(\vec x^{(i)})\bigr), & \text{if } y^{(i)} = 1 \\[6pt]
-\;\log\!\bigl(1-f_{\vec w,b}(\vec x^{(i)})\bigr), & \text{if } y^{(i)} = 0
\end{cases}
$$

- 为什么选择 $-\log f(\mathbf{x})$ 作为 $y=1$ 的损失？

1. **单调惩罚**：$-\log z$ 在 $0<z<1$ 内严格递减，概率越接近 1 损失越小；概率越偏离 1 损失急速增大。
2. **零‑极大极限**：

   * $f\rightarrow 1$ ⇒ 损失 $\rightarrow 0$，最好情况零惩罚；
   * $f\rightarrow 0$ ⇒ 损失 $\rightarrow +\infty$，最坏情况无限惩罚。

---
* **为什么选择 $-\log\bigl(1-f(\mathbf{x})\bigr)$ 作为 $y=0$ 的损失？**

1. **单调惩罚**
   $-\log(1-z)$ 在 $0<z<1$ 区间严格 **递增**：

   * 当模型把正类概率 $f(\mathbf{x})$ 维持在 **接近 0** 时，$(1-f)$ 接近 1，损失最小；
   * 一旦模型把 $f(\mathbf{x})$ 提高（误判为正类的倾向越强），$(1-f)$ 变小，$-\log(1-f)$ 急速上升，惩罚越大。

2. **零‑极大极限**

   * $f\rightarrow 0$ ⇒ $1-f\rightarrow 1$ ⇒ $-\log 1 = 0$ → **最佳情况零惩罚**；
   * $f\rightarrow 1$ ⇒ $1-f\rightarrow 0$ ⇒ $-\log 0 \to +\infty$ → **最坏情况无限惩罚**。



==**总结**  预测值越远离标签值，损失越大==

**逻辑回归：成本函数（Cost）与单样本损失（Loss）**



1. 成本函数
$$
J(\,\vec w,\,b\,)\;=\;\frac{1}{m}\sum_{i=1}^{m}
   L\bigl(f_{\vec w,b}\!\bigl(\vec x^{(i)}\bigr),\,y^{(i)}\bigr)
$$
* 其中 $m$ 为训练样本数。
* $f_{\vec w,b}(\vec x)=\dfrac{1}{1+e^{-(\vec w\cdot\vec x+b)}}$ 为模型输出的“正类概率”。
* **目标**：寻找参数 $\vec w, b$ 使成本 $J$ 最小。


2. 单样本损失
$$
L\bigl(f_{\vec w,b}(\vec x^{(i)}),\,y^{(i)}\bigr)=
\begin{cases}
-\;\log\!\bigl(f_{\vec w,b}(\vec x^{(i)})\bigr), & \text{若 } y^{(i)} = 1 \\[6pt]
-\;\log\!\bigl(1-f_{\vec w,b}(\vec x^{(i)})\bigr), & \text{若 } y^{(i)} = 0
\end{cases}
$$


 3. 说明

| 词条                                 | 含义                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| **loss（单样本损失）**               | 单个样本对整体成本的贡献。                                   |
| **cost（全部样本损失，代价，成本）** | 全部样本的平均损失，用于评估当前 $\vec w,b$。                |
| **凸性 (convex，碗状)**              | 当使用上述对数损失时，成本 $J$ 对 $\vec w,b$ 是凸的 ⇒ 梯度下降可达全局最小值。 |

 \-   用对数损失把每个样本的“错得有多离谱”量化，再取平均构成成本函数，随后通过优化（如梯度下降）寻找成本最低的参数。

#%% md
- 成本函数的简化写法

1.  用“指示系数”把两行合并

令 $y\in\{0,1\}$ 本身就作为 **指示器**（indicator）：
$$
L\bigl(f(\mathbf{x}),\,y\bigr)\;=\;
-\;y\,\log f(\mathbf{x})\;-\;(1-y)\,\log\!\bigl(1-f(\mathbf{x})\bigr)
$$
* 如果 **$y=1$**
$$
  L = -1\cdot\log f(\mathbf{x}) \;-\;(1-1)\cdot\log(1-f)= -\log f(\mathbf{x})
$$
  第二项被 $0$ 抹掉，只剩正类损失。

* 如果 **$y=0$**
$$
  L = -0\cdot\log f(\mathbf{x}) \;-\;(1-0)\cdot\log(1-f)= -\log\!\bigl(1-f(\mathbf{x})\bigr)
$$
  第一项消失，只剩负类损失。


2. 合并公式的意义

| 优点             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| **简洁**         | 不必在代码或推导中写两套 if–else 逻辑。                      |
| **向量化**       | 可以一次性对整批样本计算：$-\mathbf y^\top\!\log\mathbf f-\! (1-\mathbf y)^\top\!\log(1-\mathbf f)$。 |
| **梯度推导统一** | 对 $\mathbf w,b$ 求偏导时，只需处理这一行公式即可，简化推导与实现。 |

==**总结**==
==把 $y$ 当作 0‑1 指示器乘到对应项上，自动“关掉”不该出现的对数项，因此两种情况自然合并为一条公式，既简洁又便于计算。==

- 最终的成本函数

1. 单样本损失（Loss）
$$
\boxed{
\displaystyle
L\!\bigl(f_{\vec w,b}(\vec x^{(i)}),\,y^{(i)}\bigr)
  = -\,y^{(i)}\,\log\!\bigl(f_{\vec w,b}(\vec x^{(i)})\bigr)
    \;-\;\bigl(1-y^{(i)}\bigr)\,
       \log\!\Bigl(1-f_{\vec w,b}(\vec x^{(i)})\Bigr)
}
$$


2. 总体成本（Cost）
$$
\boxed{
\displaystyle
J(\vec w,b)
   = \frac{1}{m}\sum_{i=1}^{m}
       L\!\bigl(f_{\vec w,b}(\vec x^{(i)}),\,y^{(i)}\bigr)
   = -\,\frac{1}{m}\sum_{i=1}^{m}
       \Bigl[
            y^{(i)}\,\log\!\bigl(f_{\vec w,b}(\vec x^{(i)})\bigr)
            \;+\;
            \bigl(1-y^{(i)}\bigr)\,\log\!\Bigl(1-f_{\vec w,b}(\vec x^{(i)})\Bigr)
       \Bigr]
}
$$
**性质**

* 对参数 $(\vec w,b)$ 来说，$J$ 是 **凸函数 (convex)**；
* 因此在优化过程中存在 **唯一的全局最小值 (single global minimum)**。



```python
X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)
y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)


fig,ax = plt.subplots(1,1,figsize=(4,4))
plotData(X_train, y_train, ax)

ax.axis([0, 4, 0, 3.5])
ax.set_ylabel('$x_1$')
ax.set_xlabel('$x_0$')
plt.show()
```

```python
def compute_cost_logistic(X, y, w, b):
    """
    Computes cost

    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters
      b (scalar)       : model parameter

    Returns:
      cost (scalar): cost
    """
    cost = 0.0
    # TODO： 实现成本函数，返回损失值
    w = w.reshape(-1,1)
    # b = b.reshape(-1,1)
    y = y.reshape(-1,1)
    m = X.shape[0]
    f_wb = 1/(1+np.exp(-X@w - b) ) # (n,1)
    # log_fwb = np.log(f_wb)        # (n,1)
    cost = np.sum((y*np.log(f_wb) + (1-y)*np.log(1-f_wb)) , axis = 0)
    return -cost/m

w_test = np.array([1,1])
b_test = -3
assert np.isclose(compute_cost_logistic(X_train, y_train, w_test, b_test), 0.36686678640551745), "损失函数实现有问题"
```

```python
x0 = np.arange(0,6)

# 画两个决策边界
x1 = 3 - x0
x1_other = 4 - x0

fig,ax = plt.subplots(1, 1, figsize=(4,4))
# Plot the decision boundary
ax.plot(x0,x1, c="blue", label="$b$=-3")
ax.plot(x0,x1_other, c="magenta", label="$b$=-4")
ax.axis([0, 4, 0, 4])

# Plot the original data
plotData(X_train,y_train,ax)
ax.axis([0, 4, 0, 4])
ax.set_ylabel('$x_1$', fontsize=12)
ax.set_xlabel('$x_0$', fontsize=12)
plt.legend(loc="upper right")
plt.title("Decision Boundary")

plt.show()
```

```python
# 可以看出来b=-4，w=[1,1]比 b=-3， w=[1,1]的决策边界更糟糕，通过调用实现的损失函数，也可以反映这一点

w_array1 = np.array([1,1])
b_1 = -3
w_array2 = np.array([1,1])
b_2 = -4

print("损失值  b = -3 : ", compute_cost_logistic(X_train, y_train, w_array1, b_1))
print("损失值  b = -4 : ", compute_cost_logistic(X_train, y_train, w_array2, b_2))
```


- 从概率角度解释损失函数 （可选）
最大似然估计
1. 建模假设

* 每个样本 $\bigl(\mathbf x^{(i)},y^{(i)}\bigr)$ **独立同分布 (i.i.d.)**
* 标签 $y^{(i)}\in\{0,1\}$ 遵循 **伯努利分布**，其成功概率由模型给出：
$$
    P\!\bigl(y^{(i)}=1\mid \mathbf x^{(i)}\bigr)=
      f_{\mathbf w,b}\!\bigl(\mathbf x^{(i)}\bigr)=
      \sigma\!\bigl(\mathbf w^\top\mathbf x^{(i)}+b\bigr)
$$
  其中 $\sigma(z)=\dfrac{1}{1+e^{-z}}$。



2. 写出**似然函数** 本质上是最大化概率

对全部 $m$ 个样本，联合概率（似然）为
$$
\mathcal L(\mathbf w,b)
  =\prod_{i=1}^{m}
     \bigl[f_{\mathbf w,b}(\mathbf x^{(i)})\bigr]^{y^{(i)}}
     \bigl[1-f_{\mathbf w,b}(\mathbf x^{(i)})\bigr]^{1-y^{(i)}}
$$

3. 取对数 → **对数似然**
$$
\log\mathcal L(\mathbf w,b)
  =\sum_{i=1}^{m}
     \Bigl[
        y^{(i)}\log f_{\mathbf w,b}(\mathbf x^{(i)})
        +\bigl(1-y^{(i)}\bigr)\log\!\bigl(1-f_{\mathbf w,b}(\mathbf x^{(i)})\bigr)
     \Bigr]
$$

4. 取负平均 → 得到**损失/成本函数**

最大似然 → **最大化** $\log\mathcal L$。
等价于 **最小化** 其 **负** 值，再除以样本数 $m$ 得平均损失：
$$
J(\mathbf w,b)
  =-\frac{1}{m}\log\mathcal L(\mathbf w,b)
  =-\frac{1}{m}\sum_{i=1}^{m}
     \Bigl[
        y^{(i)}\log f_{\mathbf w,b}(\mathbf x^{(i)})
        +\bigl(1-y^{(i)}\bigr)\log\!\bigl(1-f_{\mathbf w,b}(\mathbf x^{(i)})\bigr)
     \Bigr]
$$
注意括号里的那一项 **正是单样本损失**
$$
L\bigl(f(\mathbf x^{(i)}),y^{(i)}\bigr)
  =-\,y^{(i)}\log f(\mathbf x^{(i)})
   -\bigl(1-y^{(i)}\bigr)\log\!\bigl(1-f(\mathbf x^{(i)})\bigr)
$$

5. 结果与意义

    1. **统计一致**：损失函数不是随意挑的，而是伯努利模型在 MLE 框架下自然得到的 **负对数似然**（即交叉熵）。
    2. **可解释性**：最小化 $J$ ⇔ 最大化训练数据出现的概率；因此学到的 $\hat f(\mathbf x)$ 可以直接解释为“在当前参数下，输入 $\mathbf x$ 属于正类的最大似然概率”。

**总结**：逻辑回归把每个标签视为一次伯努利试验，最大似然估计要求“让观测到的数据最可能发生”。




- 逻辑回归的梯度下降

**Logistic 回归：成本函数、梯度及批量梯度下降更新公式**

1. 成本函数（Cost）
$$
\boxed{
\displaystyle
J(\vec w,b)
  = -\,\frac{1}{m}\sum_{i=1}^{m}
     \Bigl[
        y^{(i)}\;\log f_{\vec w,b}\!\bigl(\vec x^{(i)}\bigr)
        \;+\;
        \bigl(1-y^{(i)}\bigr)\,
        \log\!\Bigl(1-f_{\vec w,b}\!\bigl(\vec x^{(i)}\bigr)\Bigr)
     \Bigr]
}
$$
其中
$$
f_{\vec w,b}(\vec x)=\frac{1}{1+e^{-(\vec w\cdot\vec x+b)}}\in(0,1)
$$

2. 批量梯度（对全部 $m$ 个样本求平均）
$$
\begin{aligned}
\frac{\partial J}{\partial w_j}
  &= \frac{1}{m}\sum_{i=1}^{m}
        \bigl(f_{\vec w,b}(\vec x^{(i)})-y^{(i)}\bigr)\,
        x^{(i)}_j \\[6pt]
\frac{\partial J}{\partial b}
  &= \frac{1}{m}\sum_{i=1}^{m}
        \bigl(f_{\vec w,b}(\vec x^{(i)})-y^{(i)}\bigr)
\end{aligned}
$$

3. 批量梯度下降（同时更新）
$$
\textbf{repeat\;\{}\quad
  \begin{cases}
    w_j \;=\;
      w_j
      \;-\;
      \alpha\,
      \dfrac{\partial J}{\partial w_j}\,,
      & j = 1,\dots,n \\[8pt]
    b   \;=\;
      b
      \;-\;
      \alpha\,
      \dfrac{\partial J}{\partial b}
  \end{cases}
\quad\textbf{\}}
$$
* $\alpha$ 为学习率 (learning rate)。
* 所有参数 **同时更新**（simultaneous updates）以利用同一轮次计算出的梯度。

**说明**

* 梯度项 $f_{\vec w,b}(\vec x^{(i)})-y^{(i)}$ 表示“预测概率与真实标签的偏差”；
* 成本函数对 $(\vec w,b)$ 是**凸**的，故批量梯度下降可收敛到全局最小值。
* 线性回归的一些技巧在逻辑回归仍然适用：选择学习率，监控迭代次数和损失值，向量化，特征缩放



```python
X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y_train = np.array([0, 0, 0, 1, 1, 1])
```

```python
def compute_gradient_logistic(X, y, w, b):
    """
    计算逻辑回归的梯度

    Args:
      X (ndarray (m,n): Data, m examples with n features
      y (ndarray (m,)): target values
      w (ndarray (n,)): model parameters
      b (scalar)      : model parameter
    Returns
      dj_dw (ndarray (n,)): 关于w的梯度
      dj_db (scalar)      : 关于b的导数
    """
    y = y.reshape(-1,1)
    w = w.reshape(-1,1)
    m,n = X.shape
    # dj_dw = np.zeros((n,))
    # dj_db = 0.
    dz = X@w + b                  # (m,1)
    fwb_y = sigmoid(dz) -y        # (m,1)
    # dj_dw_tmp = (fwb_y ) * X    # (m,1) broadcast ->(m,n) * (m,n)
    dj_dw = (np.sum((fwb_y ) * X , axis = 0) / m).flatten()
    dj_db = np.sum(fwb_y, axis = 0) / m
    # TODO: 随堂练习：实现梯度的计算
    return dj_dw, dj_db


# 简单检查下梯度实现是否正确
X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y_tmp = np.array([0, 0, 0, 1, 1, 1])
w_tmp = np.array([2.,3.])
b_tmp = 1.
dj_dw_tmp, dj_db_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)
assert np.allclose(dj_dw_tmp, np.array([0.498333393278696, 0.49883942983996693])) and np.isclose(dj_db_tmp, 0.49861806546328574), "梯度实现不正确"
```

```python
import math,copy
def gradient_descent(X, y, w_in, b_in, alpha, num_iters):
    """
    执行 **批量梯度下降（Batch Gradient Descent）**

    参数:
      X (ndarray (m, n)): 训练数据，m 行 n 列 —— m个样本、每个样本 n个特征
      y (ndarray (m, )): 目标值（标签），长度为 m
      w_in (ndarray (n,)): 模型参数 w 的初始值
      b_in (scalar)      : 模型参数 b 的初始值
      alpha (float)      : 学习率
      num_iters (scalar) : 迭代次数

    返回:
      w (ndarray (n,)): 更新后的参数 w
      b (scalar)       : 更新后的参数 b
    """
    # 用于保存每次迭代的成本 J 以及 w，主要方便后续绘图
    J_history = []
    w = copy.deepcopy(w_in)   # 避免在函数内部修改外部的 w
    b = b_in

    for i in range(num_iters):
        # 计算梯度，再根据梯度更新参数
        dj_dw, dj_db = compute_gradient_logistic(X, y, w, b)

        # 根据梯度更新参数 w 与 b
        w = w - alpha * dj_dw
        b = b - alpha * dj_db

        # 记录每次迭代的成本值（防止资源耗尽，仅记录前 100000 次）
        if i < 100000:
            J_history.append(compute_cost_logistic(X, y, w, b))

        # 每进行 num_iters/10 次迭代就打印一次当前成本
        if i % math.ceil(num_iters / 10) == 0:
            print(f"第 {i:6d} 次迭代:  成本 {J_history[-1]}")

    # 返回最终的 w、b 以及成本历史，便于绘图
    return w, b, J_history
```

```python
# 对训练数据做梯度下降
w_tmp  = np.zeros_like(X_train[0])
b_tmp  = 0.
alph = 0.1
iters = 100000

w_out, b_out, J = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters)
print(f"\n最终的参数: w:{w_out}, b:{b_out}")
```

```python
# def plt_prob(ax, w_out, b_out):


fig,ax = plt.subplots(1,1,figsize=(5,4))
# Plot the original data
ax.set_ylabel(r'$x_1$')
ax.set_xlabel(r'$x_0$')
ax.axis([0, 4, 0, 3.5])
plotData(X_train,y_train,ax)
z = -b_out/w_out
# TODO：随堂练习，根据学习到的 w_out, b_out画决策边界；  提示：利用公式 wx+b =0 -> [w1,w2]*[x1,x2]+b=0，实现正确会得到如下图像
plt.plot([0,z[0]],[z[1],0], c = "b")
plt.show()
```

```python

# 更多绘图：绘制预测y=1的概率，概率越高越蓝，概率越低，越白

fig,ax = plt.subplots(1,1,figsize=(5,4))

# 利用已训练好的参数，计算概率
def predict_proba(x, y, w0, w1, b):
    z = b + w0*x + w1*y     # 线性部分
    return 1/(1+np.exp(-z))


def plt_prob(ax, w_out,b_out):

    # 生成网格
    x0_space  = np.linspace(0, 4 , 100)
    x1_space  = np.linspace(0, 4 , 100)

    tmp_x0,tmp_x1 = np.meshgrid(x0_space,x1_space)
    z = predict_proba(tmp_x0,tmp_x1,w_out[0], w_out[1],b_out)

    cmap = plt.get_cmap('Blues')
    pcm = ax.pcolormesh(tmp_x0, tmp_x1, z,
                   cmap=cmap, shading='nearest', alpha = 0.9)
    ax.figure.colorbar(pcm, ax=ax)

plt_prob(ax, w_out,b_out)
ax.set_ylabel(r'$x_1$')
ax.set_xlabel(r'$x_0$')
ax.axis([0, 4, 0, 3.5])
plotData(X_train,y_train,ax)

plt.show()
```




### 逻辑回归成本函数的梯度推导 （可选）


#### 1. 记号与目标

| 记号                                                         | 含义                                   |
| ------------------------------------------------------------ | -------------------------------------- |
| $\mathbf x^{(i)}\in\mathbb R^{n}$                            | 第 $i$ 个样本（含偏置前不加 1）        |
| $y^{(i)}\in\{0,1\}$                                          | 真实标签                               |
| $\mathbf w\in\mathbb R^{n},\;b\in\mathbb R$                  | 参数                                   |
| $f^{(i)}\equiv f_{\mathbf w,b}(\mathbf x^{(i)}) = \sigma(z^{(i)})$ | 预测概率，$\sigma(z)=\frac1{1+e^{-z}}$ |
| $m$                                                          | 样本总数                               |

$$
J(\mathbf w,b)
  = -\frac1m\sum_{i=1}^{m}
      \Bigl[
        y^{(i)}\log f^{(i)}
        + (1-y^{(i)})\log(1-f^{(i)})
      \Bigr]
$$

目标：求 $\displaystyle\frac{\partial J}{\partial w_j}$ 与 $\displaystyle\frac{\partial J}{\partial b}$。



#### 2. 单样本损失对 $z$ 的导数
$$
L^{(i)}
  = -\bigl[y^{(i)}\log f^{(i)} + (1-y^{(i)})\log(1-f^{(i)})\bigr]
$$
   a. **对预测 $f^{(i)}$ 求导**
$$
   \frac{\partial L^{(i)}}{\partial f^{(i)}}
     = -\frac{y^{(i)}}{f^{(i)}} + \frac{1-y^{(i)}}{1-f^{(i)}}
     = \frac{f^{(i)}-y^{(i)}}{f^{(i)}(1-f^{(i)})}
$$
   b. **sigmoid 的导数**
$$
   \frac{\partial f^{(i)}}{\partial z^{(i)}} = f^{(i)}\bigl(1-f^{(i)}\bigr)
$$
   c. **链式法则**
$$
   \frac{\partial L^{(i)}}{\partial z^{(i)}}
     = \frac{\partial L^{(i)}}{\partial f^{(i)}}\;
       \frac{\partial f^{(i)}}{\partial z^{(i)}}
     = \bigl(f^{(i)}-y^{(i)}\bigr)
$$
#### 3. 对权重 $w_j$ 的梯度
$$
\begin{aligned}
\frac{\partial J}{\partial w_j}
  &= \frac1m\sum_{i=1}^{m}
     \frac{\partial L^{(i)}}{\partial z^{(i)}}\;
     \frac{\partial z^{(i)}}{\partial w_j}  \\
  &= \frac1m\sum_{i=1}^{m}
     \bigl(f^{(i)}-y^{(i)}\bigr)\;
     x^{(i)}_j
\end{aligned}
$$
#### 4. 对偏置 $b$ 的梯度
$$
\frac{\partial J}{\partial b}
  = \frac1m\sum_{i=1}^{m}
     \frac{\partial L^{(i)}}{\partial z^{(i)}}\;
     \frac{\partial z^{(i)}}{\partial b}
  = \frac1m\sum_{i=1}^{m}
     \bigl(f^{(i)}-y^{(i)}\bigr)
$$
#### 5. 向量化表示（推荐在代码中使用）
$$
\nabla_{\mathbf w}J = \frac1m \, \mathbf X^\top\!\bigl(\mathbf f - \mathbf y\bigr)
\quad\quad
\frac{\partial J}{\partial b} = \frac1m\sum_{i=1}^{m}(f^{(i)}-y^{(i)})
$$
* $\mathbf X\in\mathbb R^{m\times n}$ 为特征矩阵。
* $\mathbf f=[f^{(1)},\dots,f^{(m)}]^\top$。
* $\mathbf y=[y^{(1)},\dots,y^{(m)}]^\top$。



#### 6. 批量梯度下降更新
$$
\boxed{
\begin{aligned}
\mathbf w &\gets \mathbf w - \alpha\,\nabla_{\mathbf w}J \\[4pt]
b &\gets b - \alpha\,\dfrac{\partial J}{\partial b}
\end{aligned}
}
$$
（$\alpha$ 为学习率，全部参数 **同时** 更新）



- 记忆方式




**“预测减真实，前乘特征”** —— $f-y$ 乘 $x_j$ 就是对 $w_j$ 的梯度；对偏置把 $x_j$ 看成 1 即可。


$$
\frac{\partial J}{\partial w_j}\;=\;\overbrace{\bigl(f-y\bigr)}^{\text{误差}}\;\times\;x_j
$$



### 使用机器学习库sklearn跑逻辑回归和线性回归

```python
X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])
y = np.array([0, 0, 0, 1, 1, 1])

from sklearn.linear_model import LogisticRegression

# 训练
lr_model = LogisticRegression()
lr_model.fit(X, y)
```

```python
# 预测
y_pred = lr_model.predict(X)

print("训练集的预测:", y_pred)
```

```python
# 看预测的正确率
print("训练集预测的正确率:", lr_model.score(X, y))
```

```python
def load_house_data():
    data = np.loadtxt("./datasets/houses.txt", delimiter=',', skiprows=1)
    X = data[:,:4]
    y = data[:,4]
    return X, y
```

```python
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler

# 1. 加载数据
X_train, y_train = load_house_data()
```

```python
# 2. 特征缩放
scaler = StandardScaler()
X_norm = scaler.fit_transform(X_train)
print(f"原始数据峰值差        X:{np.ptp(X_train,axis=0)}")
print(f"缩放后的峰值差       X_norm:{np.ptp(X_norm,axis=0)}")
```

```python
# 3. 创建并拟合线性模型
sgdr = SGDRegressor(max_iter=1000)
sgdr.fit(X_norm, y_train)
print(sgdr)
print(f"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}")
```

```python
# 4. 看训练后的参数
b_norm = sgdr.intercept_
w_norm = sgdr.coef_
print(f"model parameters:                   w: {w_norm}, b:{b_norm}")
```

```python
# 5. 预测结果

y_pred_sgd = sgdr.predict(X_norm)
y_pred = np.dot(X_norm, w_norm) + b_norm
print(f"机器学习库的预测和手动预测是否匹配: {(y_pred == y_pred_sgd).all()}")

print(f"训练集的预测:\n{y_pred[:4]}" )
print(f"目标值 \n{y_train[:4]}")
```



# 机器学习项目主要步骤

- 机器学习项目主要步骤：
1. 放眼大局
2. 获取数据
3. 探索和可视化数据以获取见解
4. 为机器学习算法准备数据
5. 选择一个模型并训练它
6. 微调模型
7. 展示解决方案
8. 发布，监控和维护系统


## 放眼大局
利用加州的一份住房与人口统计数据来构建房价预测模型。数据中汇总了各个最小统计单元的指标，如人口数量、家庭收入中位数、房价中位数等。这里我们把这些最小统计单元统称为“地区” （一个地区通常有600-3000人）。你的模型需要基于这些特征学习，在给定其他指标的情况下预测任一地区的房价中位数。


拿出你的机器学习项目清单， 对于多数机器学习项目，它可以工作得很好；但确保根据需求进行调整。

### 框定问题

你问老板的第一个问题应该是业务目标到底是什么。建立模型可能不是最终目标。公司期望如何使用该模型并从中受益？了解目标很重要，因为它将决定你如何框定问题、你将选择哪些算法、你将使用哪种性能指标来评估你的模型，以及你将花费多少精力来调整它。

老板回答说，你的模型的输出（对一个地区房价中位数的预测）将连同许多其他信息一起输入到另一个机器学习系统（见下图）。这个下游系统将决定在给定的区域是否值得投资。做到这一点至关重要，因为它直接影响收入。

![机器学习流水线](./images/end_to_end_ml_proj/p1.png)






- 流水线：

系列数据处理组件称为数据流水线。流水线在机器学习系统中非常常见，因为有大量数据需要操作并且需要应用很多的数据转换。

组件通常是异步运行的。每个组件都会拉取大量数据，对其进行处理，然后将结果输出到另一个数据存储器中。然后，一段时间后，流水线中的下一个组件拉取此数据并给出自己的输出。每个组件都是相当独立的：组件之间的接口就是数据存储。这使得系统易于掌握（借助于数据流图），不同的团队可以专注于不同的组件。此外，如果一个组件发生故障，下游组件通常可以仅使用损坏组件的最后输出继续正常运行（至少一段时间）。这使得架构非常健壮。

另外，如果没有实施适当的监控，损坏的组件可能会在一段时间内被忽视。数据变得陈旧，整个系统的性能会下降。



- 在开始训练机器学习系统之前，请你先思考并尝试回答以下问题：

<details>
<summary> 你认为这是什么类型的机器学习任务？</summary>
    **这是一个监督学习任务**。
因为我们拥有带有标签的数据：每条记录都包括模型的输入（如人口、收入中位数等）和目标输出（该地区的房价中位数）。模型可以利用这些已标注的样本进行训练。

<details>
<summary> 它是分类任务还是回归任务？</summary>
    **这是一个回归任务**，因为我们希望预测的是一个连续数值（即房价中位数），而不是分类标签。
进一步来说，这是一个 **单预测值回归** 问题：我们只预测一个输出值。
不过，它也是一个 **多元回归**（multiple regression）问题，因为我们使用多个输入特征来预测这个输出值。

<details>
<summary>应该使用批量学习还是在线学习？</summary>
    **推荐使用批量学习（Batch Learning）**。<br>
    原因如下：<br>
	- 数据集大小适中，足以完全载入内存。<br>
	- 没有实时数据流不断涌入系统。<br>
	- 环境相对稳定，不需要模型频繁调整以应对数据分布漂移。<br>
因此，传统的批量学习方法即可胜任，不需要在线学习技术。<br>
如果数据量很大，可以将批量学习的工作拆分到多个服务器，或者使用在线学习技术



### 选择性能指标


下一步是选择性能指标。回归问题的典型性能度量是均方根误差(Root Mean Square Error，RMSE)。它给出了系统在其预测中通常会产生多大误差，并为较大的误差赋予较高的权重

- RMSE 公式及符号说明

我们使用下列公式来计算预测结果的均方根误差（Root Mean Square Error, RMSE）：

$$
\text{RMSE}(X, h) = \sqrt{ \frac{1}{m} \sum_{i=1}^{m} \left(h\left(x^{(i)}\right) - y^{(i)} \right)^2 }
$$



- 符号说明

这个公式引入了几个在机器学习中非常常见的符号，以下是各个符号的含义：

- **m** 是你测量 RMSE 的数据集中的**实例数**。
  例如，如果你在 2000 个地区的验证集上评估 RMSE，则有：
  `m = 2000`

- **x⁽ⁱ⁾** 是数据集中第 *i* 个实例的所有**特征值**（不包括标签）的向量，
  **y⁽ⁱ⁾** 是它的**标签**（该实例的期望输出值）。

  例如，如果数据集中的第一个地区位于经度 -118.29°，纬度 33.91°，居住有 1416 人，收入中位数为 38372 美元，房屋价值中位数为 156400 美元（暂时忽略其他特征），那么：

  $$
  x^{(1)} = \begin{bmatrix}
  -118.29 \\
  33.91 \\
  1416 \\
  38372
  \end{bmatrix}
  \quad \text{和} \quad
  y^{(1)} = 156400
  $$

- **X** 是一个矩阵，包含数据集中所有实例的所有特征值（不包括标签）。
  每个实例为一行，第 *i* 行等于 \(x^{(i)}\) 的转置，记作 $$(x^{(i)})^T$$

表示特征矩阵 **X**：

$$
X =
\begin{pmatrix}
(x^{(1)})^\mathrm{T} \\
(x^{(2)})^\mathrm{T} \\
\vdots \\
(x^{(1999)})^\mathrm{T} \\
(x^{(2000)})^\mathrm{T}
\end{pmatrix}
=
\begin{pmatrix}
-118.29 & 33.91 & 1416 & 38372 \\
\vdots & \vdots & \vdots & \vdots
\end{pmatrix}
$$

- **h** 是系统的**预测函数**，也称为假设。
  当给系统一个实例的特征向量 $ x^{(i)} $ 时，它会输出该实例的预测值：

  $$
  \hat{y}^{(i)} = h(x^{(i)})
  $$

---

- 平均绝对误差（MAE）

$$
\text{MAE}(X, h) = \frac{1}{m} \sum_{i=1}^{m} \left| h(x^{(i)}) - y^{(i)} \right|
$$

RMSE 和 MAE 都是衡量两个向量（预测向量和目标向量）之间距离的方法。各种距离度量或范数是可能的：

- 计算平方和的根（RMSE）对应于欧几里得范数：这是我们都熟悉的距离概念。
  它也被称为 $\ell_2$ 范数，记为 $\| \cdot \|_2$（或简写为 $\| \cdot \|$）。

- 计算绝对值之和（MAE）对应于 $\ell_1$ 范数，记为 $\| \cdot \|_1$。
  这有时被称为曼哈顿范数，因为如果你只能沿着正交的城市街区行动，那么它会测量城市中两点之间的距离。

- 一般而言，包含 $n$ 个元素的向量 $v$ 的 $\ell_k$ 范数定义为：

$$
\|v\|_k = (|v_1|^k + |v_2|^k + \cdots + |v_n|^k)^{1/k}
$$

- $\ell_0$ 给出向量中的非零元素的数量，$\ell_\infty$ 给出向量中的最大绝对值。


范数指数越高，它就越关注**大值**而忽略小值。这就是 RMSE 比 MAE 对异常值更敏感的原因。

但是当异常值呈指数级减少时（例如在钟形曲线中），RMSE 表现非常好，并且通常是首选。



### 检查假设

最后，**列出并验证已有假设**是非常重要的一步，有助于及早发现潜在问题。
例如，你假设系统输出的房价将被下游系统原样使用。但如果下游其实将其转化为“便宜”“中等”“昂贵”等类别并用于分类模型，那你当前的回归建模将毫无意义。

因此，**在明确需求之前，不应盲目假设问题类型**。幸运的是，在与下游团队沟通后，你确认他们确实需要实际价格值。



## 获取数据



```python
import sys

assert sys.version_info >= (3, 7)   # 检查python版本

from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")  # 检查sklearn版本
```

```python
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
# TODO:进行数据读取
def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")			# 指定文件路径
    if not tarball_path.is_file():						# 判断路径下是否存在housing.tgz文件，如果没有进行if语句
        Path("datasets").mkdir(parents=True, exist_ok=True)			# 如果没有文件则递归形式的船舰文件夹
        url = "https://github.com/ageron/data/raw/main/housing.tgz"	# 指定文件下载的网址
        urllib.request.urlretrieve(url, tarball_path)				# 将url上的文件下载到指定文件夹
    with tarfile.open(tarball_path) as housing_tarball:				# 类似于file的open，但是归档压缩文件需要tarfile.open(文件路径)
            housing_tarball.extractall(path="datasets")				# 将归档压缩文件里的内容物进行提取，path参数指定路径
    return pd.read_csv(Path("datasets/housing/housing.csv"))		# pandas的read_csv进行文件读取

housing = load_housing_data()										# 将读取到的内容用housing变量进行接收

housing.head()  		# 进行文件查看，粗看一下文件内容的情况以便后续清洗
housing.info()
housing["ocean_proximity"].value_counts()
housing.describe()

```



调用load_housing_data()时，它会查找datasets/housing.tgz文件。如果找不到，它会在当前目录中创建datasets目录，从ageron/data GitHub存储库下载housing.tgz文件，并将其内容提取到datasets目录中。这将创建datasets/housing目录，其中包含housing.csv文件。最后，该函数将此CSV文件加载到包含所有数据的Pandas DataFrame对象中，并返回它。



```python
import matplotlib.pyplot as plt

# 定义默认字体大小
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)

housing.hist(bins=50, figsize=(12, 8))
```



### 创建测试集

```python
import numpy as np
#TODO：自己模拟测试集与训练集
def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))		# np.random.permutatuion(n),会将0-n随机排序，得到一个数组
    test_set_size = int(len(data) * test_ratio)				# 得到测试集的数量
    test_indices = shuffled_indices[:test_set_size]			# 通过切片得到测试集
    train_indices = shuffled_indices[test_set_size:]		# 余下的生成训练集
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)		# 16512
len(test_set)		# 4128
```


上述实现的弊端：数据更新后，或者反复多次运行分离训练和测试数据后，还是会看到整个数据集

解决方案：通过标识符来决定它是否应该进入测试集（假设实例的标识符是唯一且不变的）。比如，可以计算每个实例标识符的哈希值（相当于id），如果哈希值低于或等于最大哈希值的20%，则将该实例放入测试集中。这可以确保测试集在多次运行中保持一致，即使刷新数据集也是如此。



#### 简单随机采样进行测试集与训练集划分

1. 通过自定义函数实现

```python
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):   # crc32会将id转成均匀分布的类似哈希值的数字，这些数字在0-2**32均匀分布
    return crc32(np.int64(identifier)) < test_ratio * (2**32)

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```

通过简单的索引来进行采样可能会对于数据的采样结果有差异，对于此模型而言加州的房子在各个地区的分布情况可能有所差异，临海房子多，数据多，依据索引进行划分可能造成训练集沿海数据量过多，内地数据量少而训练后的参数不能在测试集不能达到很好的效果


```python
housing_with_id = housing.reset_index()  # 加一个索引列
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")
```

增加有经纬度生成的id列，此列id很好的代表了位置差异，在训练效果上可能更好


```python
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]  # 通过经纬度创建id
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")
```


Scikit-Learn提供了一些函数以各种方式将数据集拆分为多个子集。最简单的函数是train_test_split()，它所做的事情与我们之前定义的shuffle_and_split_data()函数几乎相同，

但有几个附加功能。首先，有一个random_state参数允许你设置随机生成器种子，保证在数据不变的前提下，训练和测试的拆分是一致的。

其次，你可以将多个具有相同行数的数据集传递给它，并且它会在相同的索引上拆分它们（这非常有用，例如，如果你有一个单独标签的DataFrame）

==用过机器学习库进行简单随机划分==

`from sklearn.model_selection import train_test_split`

`train_test_split(housing, test_size=0.2, random_state=42)`: housing，需要拆分的数据，`test_size`测试集占比，`random_state` 种子数，方便复现

```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
```

#### 分层采样

- 分层采样：
数据集足够大，纯随机采样没问题。但如果不够大，会引入显著采样偏差。

向1000个人做问卷调查，尝试保证1000个人足够代表全体人口。例如人口中女性占51.1%， 男性占48.9%，因此对1000个人问卷调查尝试保证这样一个比例：511名女性和489名男性，这个就是分层采样。

定义：将总体分为称做层的同质子组，并从每个层中抽取正确数量的实例以保证测试集能代表总体。

如果进行调查的人使用纯随机采样，则大约有10.7%的机会会抽取到女性参与者少于48.5%或超过53.5%的偏差测试集，可以因为采样偏差，得到错误的结论



```python
# 利用二项分布 N次独立随机实验，每次结果为1的概率为p，x次为1的概率 （x<=N)

from scipy.stats import binom            # 导入二项分布
sample_size = 1000
ratio_female = 0.511
proba_too_small = binom(sample_size, ratio_female).cdf(485 - 1)
proba_too_large = 1 - binom(sample_size, ratio_female).cdf(535)
print(proba_too_small + proba_too_large)
```

```python
# 纯数值模拟采样偏差的概率
samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)
((samples < 485) | (samples > 535)).mean()			# 0.10723
```

分层采样需要输入`dtype = category`的列，那么pandas的qcut和cut方法都可以将数据变成`category`属性，同时也可以`astype`方法进行转换，详见pandas的分类类型

```python
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])
housing["income_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.xlabel("Income category")
plt.ylabel("Number of districts")
plt.show()
```



==用机器学习库进行分层采样==

`from sklearn.model_selection import StratifiedShuffleSplit`

`StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)`  `n_splits`表示会有10次采样结果，`test_size=0.2`表示采样的测试集比例，`random_state=42`生成种子数，方便复现

```python
from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []		# 存储采样结果，n_splits = 10 会有10次采样结果

# splitter.split（df，能够代表分层采样的序列）
for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])
```

```python
strat_train_set, strat_test_set = train_test_split(
    housing, test_size=0.2, stratify=housing["income_cat"], random_state=42)  # 分层采样 单词拆分
```

```python
strat_test_set["income_cat"].value_counts() / len(strat_test_set)  # 看分层后测试集，和原始的完整数据集一直
# income_cat
# 	3    0.350533
# 	2    0.318798
# 	4    0.176357
#	5    0.114341
#	1    0.039971
#	Name: count, dtype: float64
```

```python
def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)  # 看给定数据里，各个收入分类的占比

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) # 随机采样

compare_props = pd.DataFrame({
    "Overall %": income_cat_proportions(housing),
    "Stratified %": income_cat_proportions(strat_test_set),
    "Random %": income_cat_proportions(test_set),
}).sort_index()
compare_props.index.name = "Income Category"
compare_props["Strat. Error %"] = (compare_props["Stratified %"] /
                                   compare_props["Overall %"] - 1)
compare_props["Rand. Error %"] = (compare_props["Random %"] /
                                  compare_props["Overall %"] - 1)
(compare_props * 100).round(2)
```

```python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
housing = strat_train_set.copy()
housing.plot(kind="scatter", x="longitude", y="latitude", grid=True)
plt.show()

housing.plot(kind="scatter", x="longitude", y="latitude", grid=True, alpha=0.2)
plt.show()

housing.plot(kind="scatter", x="longitude", y="latitude", grid=True,
             s=housing["population"] / 100, label="population",
             c="median_house_value", cmap="jet", colorbar=True,
             legend=True, sharex=False, figsize=(10, 7))
plt.show()
```

数据对于房价中位数的影响程度有大有小，具体大小可以通过计算相关系数(pandas的corr方法)来看各组数据对与房价中位数的影响程度

```python
corr_matrix = housing.corr(numeric_only=True)						# 这相当于一个方法器，表示要求housing中只有数字的列与其他列的相关系数，此时并未传入其他列，所以没有任何数据
corr_matrix["median_house_value"].sort_values(ascending=False)		# 此时传入"median_house_value"列，即计算"median_house_value"列与其他所有列vde相关系数，并将结果进行降序排列

from pandas.plotting import scatter_matrix				# 散点矩阵图

attributes = ["median_house_value", "median_income", "total_rooms",
              "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))	# 散点矩阵生成的图，此时相当于attributes的列向量@attributes的行向量，attributes一样的生成直方图，不一样的生成散点图
plt.show()
housing.plot(kind="scatter", x="median_income", y="median_house_value",
             alpha=0.1, grid=True)
plt.show()
```

特征融合初窥

```python
housing["rooms_per_house"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_ratio"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["people_per_house"] = housing["population"] / housing["households"]
# 进行特征融合，再看其与房价中位数的相关系数
corr_matrix = housing.corr(numeric_only=True)
corr_matrix["median_house_value"].sort_values(ascending=False)
```


为机器学习算法准备数据。这里应该为这个目的编写函数，而不是手动执行此操作
原因如下：

1. 可以在任何数据集上轻松地重现这些转换（例如，下次你获得新的数据集时)。
2. 可以逐步构建一个可在未来项目中重复使用的转换函数库。
3. 可以在实时系统中使用这些函数来转换新数据，再将其提供给你的算法



### 缺失值处理

```python
housing = strat_train_set.drop("median_house_value", axis=1)	
housing_labels = strat_train_set["median_house_value"].copy()		# 分离特征与标签

null_rows_idx = housing.isnull().any(axis=1)			# columns进行聚合留下行索引
housing.loc[null_rows_idx].head()

# 选项1：去掉相应的行
housing_option1 = housing.copy()

housing_option1.dropna(subset=["total_bedrooms"], inplace=True)  # 去掉相应的地区

housing_option1.loc[null_rows_idx].head()

# 选项2：去除含有缺失值的列
housing_option2 = housing.copy()

housing_option2.drop("total_bedrooms", axis=1, inplace=True)  # 去掉整个属性

housing_option2.loc[null_rows_idx].head()

# 选项3：数据填充，将缺失值填充为某个数（此处为中位数）
housing_option3 = housing.copy()

median = housing["total_bedrooms"].median()
housing_option3["total_bedrooms"].fillna(median, inplace=True)  # 将缺失值填为某个值 （零，均值，中位数等）归责

housing_option3.loc[null_rows_idx].head()
```



使用一个方便的Scikit-Learn类：SimpleImputer，而不是前面的代码。它的好处是将存储每个特征的中位数值：这不仅可以在训练集上估算缺失值，而且可以在验证集、测试集和任何提供给模型的新数据上估算缺失值。要使用它，首先需要创建一个SimpleImputer实例，指定要用该属性的中位数替换每个属性的缺失值：



```python
from sklearn.impute import SimpleImputer  # 归责（把缺失值，填充成某个具体值）

imputer = SimpleImputer(strategy="median")              # 策略 strategy   # 还没有被喂过任何具体数据，根据中位数填充缺失值的策略
#  df.select_dtypes(include=类型列表)    只保留   列的类型在这个类型列表中的那些列

housing_num = housing.select_dtypes(include=[np.number])  # 只能根据数值属性计算中位数，创建仅包含数值属性的数据副本，排除文本属性
print(type(housing_num))

imputer.fit(housing_num)  # 使用fit()方法将imputer实例拟合到训练数据中：
```

```python
imputer.statistics_  # imputer简单地计算了每个属性的中位数并将结果存储在它的statistics_实例变量中
housing_num.median().values

# imputer：转换器  ->
# fit ： 根据传进来的数据 该对象内部的属性
# transform： 传进来的数据 -》 numpy数组
X = imputer.transform(housing_num)
X
# imputer.get_feature_names_out()
temp = pd.DataFrame(X, columns=imputer.get_feature_names_out())
temp.columns
```


缺失值也可以替换为平均值(strategy="mean")，或替换为最频繁的值(strategy="most_frequent")，或替换为常数值(strategy="constant"，fill_value=...)。最后两种策略支持非数值数据。




Scikit-Learn的API(类，类里的方法/属性)设计得非常好，以下是一些设计原则：

- 一致性： 所有对象共享一个一致且简单的接口。
- 估计器：任何可以根据数据集估计某些参数的对象都称为估计器（例如，SimpleImputer是估计器）。估计本身由fit()方法执行，它将一个数据集作为参数（或两个数据集用于监督学习算法，第二个数据集包含标签）。指导估计过程所需的任何其他参数都被视为超参数（例如SimpleImputer的strategy），并且必须将其设置为实例变量（通常通过构造函数参数）。
- 转换器：一些估计器（例如SimpleImputer）也可以转换数据集，这些被称为转换器。同样，方法名很简单：转换由transform()方法执行，将要转换的数据集作为参数。它返回转换后的数据集。这种转换通常依赖于学习到的参数，就像SimpleImputer的情况一样。所有的转换器还有一个名为fit_transform()的便捷方法，相当于先调用fit()再调用transform()（但有时fit_transform()会经过优化，运行速度更快）。
- 预测器：一些估计器在给定数据集的情况下能够进行预测，这些被称为预测器。例如，LinearRegression模型是一个预测器，给定一个国家的人均GDP，预测生活满意度。预测器有一个predict()方法，它获取新实例的数据集并返回相应预测的数据集。它还有一个score()方法，可以在给定测试集（以及，在监督学习算法中对应的标签）的情况下测量预测的质量。
- 检查：所有估计器的超参数都可以通过公开实例变量（例如，imputer.strategy）直接访问，并且所有估计器的学习参数可以通过带有下划线后缀的公共实例变量访问（例如，imputer.statistics_）。
- 防止类扩散：数据集表示为NumPy数组，或pandas DataFrame，而不是自定义类。超参数只是常规的Python字符串或数字。
- 构成：尽可能重用现有的构建块，很容易从任意序列的转换器来创建一个Pipeline估计器，然后是最终估计器。



```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing_num.index)
housing_tr.loc[null_rows_idx].head()

# from sklearn import set_config
#
# set_config(transform_output="pandas")  # scikit-learn >= 1.2   # 设置pandas dataframe输入，输出pandas dataframe
```



### 异常值处理



```python
# 丢弃异常值

from sklearn.ensemble import IsolationForest    # 异常检测模型

isolation_forest = IsolationForest(random_state=42)
outlier_pred = isolation_forest.fit_predict(X)   # 异常特征预测为-1，非异常特征预测为1
np.sum(outlier_pred==-1)

# 丢弃异常值的逻辑在下面的代码
# housing = housing.iloc[outlier_pred == 1]
# housing_labels = housing_labels.iloc[outlier_pred == 1]

housing_cat = housing[["ocean_proximity"]]   # 保持为dataframe，防止降维
housing_cat.head(8)
```

```python
from sklearn.preprocessing import OrdinalEncoder        # 分类编码器

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)

housing_cat_encoded[:8]

ordinal_encoder.categories_
```

```python
# 上述分类表示的一个问题：算法会假设两个距离较近的值比两个距离较远的值更相似

# 另一种分类表示方式
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot # type是scipy的稀疏矩阵
```

```python
cat_encoder.categories_
# [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
#        dtype=object)]

df_test = pd.DataFrame({"ocean_proximity": ["INLAND", "NEAR BAY"]})
pd.get_dummies(df_test)
```


OneHotEncoder的优势在于它会记住经过了哪些类别的训练。这非常重要，因为一旦你的模型投入生产，它应该被提供与训练期间完全相同的特征：不多也不少

```python
cat_encoder = OneHotEncoder(sparse_output=False)
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
cat_encoder.transform(df_test)  # pd.get_dummies和OneHotEncoder的区别（OneHotEncoder会记住经过了哪些类别的训练）


cat_encoder.categories_

df_test_unknown = pd.DataFrame({"ocean_proximity": ["<2H OCEAN", "ISLAND"]})
pd.get_dummies(df_test_unknown)  # get_dummies()只看到两个类别，因此它输出两列，而OneHotEncoder以正确的顺序为每个学习到的类别输出一列
cat_encoder.handle_unknown = "ignore"
cat_encoder.transform(df_test_unknown)#.toarray()

cat_encoder.feature_names_in_  # 估计器会将列名称存储在feature_names_in属性中

cat_encoder.feature_names_in_
cat_encoder.get_feature_names_out()
df_output = pd.DataFrame(cat_encoder.transform(df_test_unknown), columns=cat_encoder.get_feature_names_out(), index=df_test_unknown.index)
df_output
```



### 特征缩放和转换


- 特征缩放和转换

需要应用于数据的最重要的转换之一是特征缩放。除了少数例外，机器学习算法在输入数值属性具有非常不同的尺度时表现不佳。房屋数据就是这种情况：房间总数大约在6～39320之间，而收入中位数仅在0～15之间。如果不进行任何缩放，大多数模型将偏向于忽略收入中位数并更多地关注于房间的数量。

与所有估计器一样，重要的是仅把缩放器拟合到训练数据：永远不要对训练集以外的任何其他对象使用fit()或fit_transform()。一旦你有了一个训练好的缩放器，你就可以用它来transform()任何其他集合，包括验证集、测试集和新数据


最小—最大缩放（很多人称之为归一化）是最简单的：对于每个属性，值被移动和重新缩放，这样它们最终值在0～1之间。这是通过减去最小值并除以最小值和最大值之间的差值来执行的。Scikit-Learn为此提供了一个名为MinMaxScaler的转换器。它有一个feature_range超参数，如果出于某种原因你不想要0～1，则允许你更改范围

```python
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)

np.max(housing_num_min_max_scaled,axis=0) - np.min(housing_num_min_max_scaled,axis=0)
```


与最小—最大缩放不同，标准化不会将值限制在特定范围内。但是，标准化受异常值的影响要小多。例如，假设一个地区的收入中位数等于100（错误数据），而不是通常的0～15。最小—最大缩放到0～1范围会将此异常值映射到1，并将所有其他值压缩到0～0.15，而标准化不会受到太大影响。Scikit-Learn提供了一个名为StandardScaler的转换器用于标准化：

```python
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_num)

np.mean(housing_num_std_scaled , axis=0)        # 接近0
np.std(housing_num_std_scaled , axis=0)         # 接近1
```

### 重尾处理

当一个特征的分布有一个重尾(heavy tail)时（即当远离平均值的值不是指数级稀有时），最小—最大缩放和标准化都会将大多数值压缩到一个小范围内。机器学习模型通常不喜欢这样。因此，在缩放特征之前，你应该首先对其进行变换来缩小重尾，并尽可能使分布大致对称

```python
# 重尾
fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)
housing["population"].hist(ax=axs[0], bins=50)
housing["population"].apply(np.log).hist(ax=axs[1], bins=50)
axs[0].set_xlabel("Population")
axs[1].set_xlabel("Log of population")
axs[0].set_ylabel("Number of districts")
plt.show()
```

```python
# 把数值替换为分位数，会得到均匀分布
percentiles = [np.percentile(housing["median_income"], p)
               for p in range(1, 100)]				# 这是按照百分比生成空间，每1%的数据归为一组
flattened_median_income = pd.cut(housing["median_income"],
                                 bins=[-np.inf] + percentiles + [np.inf],
                                 labels=range(1, 100 + 1))
flattened_median_income.hist(bins=50)   # 标签打的1-100， 横坐标是1-100
plt.xlabel("Median income percentile")
plt.ylabel("Number of districts")
plt.show()
housing.info()
```



### 多峰处理

当一个特征具有多峰分布（即具有两个或更多清晰的峰，称为模式）时，例如housing_median_age特征， 可以为每个模式添加一个特征，表示房屋年龄中位数与该特定模式之间的相似性。相似性度量通常使用径向基函数(Radial Basis Function，RBF)计算——任何一个仅取决于输入值和固定点之间距离的函数。最常用的RBF是高斯RBF，其输出值随着输入值远离固定点而呈指数衰减。例如，房屋年龄x和35之间的高斯RBF相似性由方程exp(-γ(x-35)2)给出。超参数γ(gamma)决定了当x远离35时相似性度量衰减的速度。使用Scikit-Learn的rbf_kernel()函数，可以创建一个新的高斯RBF特征来测量房屋年龄中位数与35之间的相似性：

`gamma` 代表的是数据对于设定值的相似性度量衰减的速度。越大衰减速度越快，越小衰减速度越慢

另外，rbf_kernel()的第二个参数，两层列表，外层代表rbf_kernel可以传如多个位置，第二个列表指的是，可能是计算距二维上的点的距离

```python
from sklearn.metrics.pairwise import rbf_kernel

# 与房龄35的相似度
age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)

ages = np.linspace(housing["housing_median_age"].min(),
                   housing["housing_median_age"].max(),
                   500).reshape(-1, 1)
gamma1 = 0.1
gamma2 = 0.03
rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)   
rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)

fig, ax1 = plt.subplots()

ax1.set_xlabel("Housing median age")
ax1.set_ylabel("Number of districts")
ax1.hist(housing["housing_median_age"], bins=50)

ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis
color = "blue"
ax2.plot(ages, rbf1, color=color, label="gamma = 0.10")
ax2.plot(ages, rbf2, color=color, label="gamma = 0.03", linestyle="--")
ax2.tick_params(axis='y', labelcolor=color)  # 设置刻度的参数
ax2.set_ylabel("Age similarity", color=color)

plt.legend(loc="upper left")
plt.show()
```

如果目标分布有一条重尾，你可以选择用其对数替换目标。但如果这样做，现在回归模型将预测房价中位数的对数，而不是房价中位数本身。如果你想要已预测的房屋中位数，则需要计算模型预测的指数。幸运的是，大多数Scikit-Learn的转换器都有一个inverse_transform()方法，这使得计算它们的逆转换变得容易。

### 线性回归模型

以下代码使用StandardScaler来缩放标签（就像我们对输入所做的那样），然后在生成的缩放标签上训练一个简单的线性回归模型，并使用它对一些新数据进行预测，我们使用经过训练的缩放器的inverse_transform()方法转换回原始尺度。请注意，我们将标签从Pandas Series转换为DataFrame，因为StandardScaler需要2D输入。此外，在此示例中，为简单起见，我们仅在单个原始输入特征（收入中位数）上训练模型：

```python
from sklearn.linear_model import LinearRegression

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())

model = LinearRegression()
model.fit(housing[["median_income"]], scaled_labels)
some_new_data = housing[["median_income"]].iloc[:5]  # 假装这是要预测的新数据

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)
```

更方便的方式：使用TransformedTargetRegressor。我们只需要构造它，给它回归模型和标签转换器，然后使用原始的未缩放标签，将它拟合到训练集上。它将自动使用转换器来缩放标签并在缩放后的标签上训练回归模型，就像我们之前所做的那样。然后，当我们想要进行预测时，它会调用回归模型的predict()方法并使用缩放器的inverse_transform()方法来产生预测：

```python
# 更简单的转换标签后的预测方式，直接使用TransformedTargetRegressor
from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())
model.fit(housing[["median_income"]], housing_labels)
predictions = model.predict(some_new_data)  # 预测时自动调用转换器的 inverse_transform
predictions
```

尽管Scikit-Learn提供了许多有用的转换器，但需要编写自己的转换器来执行自定义转换、清洗操作或组合一些特定的属性等任务。对于不需要任何训练的转换，只需编写一个函数，将NumPy数组作为输入并输出转换后的数组。例如，如上一节所述，通过将重尾分布的特征替换为它们的对数（假设特征为正且尾部在右侧）来转换具有重尾分布的特征通常是个好主意

### 自定义转换器



```python
# 定制转换器
from sklearn.preprocessing import FunctionTransformer

# FunctionTransformer：自定义转换器 用于不需要训练数据的转换器
log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)  # inverse_func 指定反函数
log_pop = log_transformer.transform(housing[["population"]])

rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))  # 不指定反函数，通过kw_args传字典，指定转换函数的超参数
age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])
age_simil_35

# rbf_kernel不单独处理特征，向它传递具有2个特征的数组，它会计算2D距离来测量相似性
sf_coords = 37.7749, -122.41
sf_transformer = FunctionTransformer(rbf_kernel,
                                     kw_args=dict(Y=[sf_coords], gamma=0.1))
sf_simil = sf_transformer.transform(housing[["latitude", "longitude"]])
sf_simil

# 特征组合转换: 计算输入特征0和1之间比率的FunctionTransformer
ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))
```

```python
# 自定义转换器也可以训练：fit, transform, fit_transform
#   get_params(), set_params()  自动超参数调整

# 将TransformerMixin添加为基类即可以得到fit_transform()：默认实现将只调用fit()，然后调用transform()。如果将BaseEstimator添加为基类（避免在构造函数中使用*args和**kwargs），你还将获得两个额外的方法：get_params()和set_params()。这些对于自动超参数调整很有用。
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # 没有 *args, **kwargs, sklearn的规范
        self.with_mean = with_mean

    def fit(self, X, y=None):  # 即使不用y，也需要它
        X = check_array(X)  # 检查X是不是数组
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # 所有估计器会把输入特征的数量存下来
        return self  # 永远返回 self!

    def transform(self, X):
        check_is_fitted(self)  # 检查是否适配过数据 （检查是否有那些下划线结尾的参数)
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_

```



- sklearn.utils.validation包含几个我们可以用来验证输入的函数
- Scikit-Learn流水线要求fit()方法有两个参数X和y，这就是为什么需要y=None参数，即使不使用y。
- 所有Scikit-Learn估计器都在fit()方法中设置n_features_in_，它们确保传递给transform()或predict()的数据具有这个数量的特征。
- fit()方法必须返回self。
- 此实现并没有100%完成：所有估计器在传递DataFrame时都应在fit()方法中设置feature_names_in_。此外，所有的转换器都应该提供一个get_feature_names_out()方法，以及一个inverse_transform()方法，当它们的转换可以被逆转时



随堂练习：todo：模仿StandardScalerClone， 写一个MinMaxScalerClone

```python
class MinMaxScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self,feature_range=(0,1)):
        self.feature_range = feature_range

    def fit(self,X,y=None):
        X = check_array(X)      # 检查是不是array

        self.min_ = X.min(axis=0)
        self.max_ = X.max(axis=0)
        # X_cp /=self.max_ - self.min_
        self.n_features_in_ = X.shape[1]
        return self

    def transform(self,X):
        check_is_fitted(self)  # 检查是否适配过数据 （检查是否有那些下划线结尾的参数)
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        X_cp = X.copy()
        self.scale_ = (X_cp - self.min_) / (self.max_ - self.min_)
        self.scale_ *= self.feature_range[1] - self.feature_range[0]
        self.scale_ += self.feature_range[0]
        self.mean_ = self.scale_.mean(axis=0)
        self.std_ = self.scale_.std(axis=0)
        return self.scale_
```



### 集群聚类（类似）计算集群中心

自定义转换器再实现中使用其他估计器 （经常发生）

```python
# 转换器在实现中使用其他估计器 （经常发生）
# kmeans识别输入数据的集群， rbf_kernel测量每个样本与每个集群中心的相似程度
from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        # KMeans估计器相关参数：集群数量，随机种子，KMeans是一个随机算法，依赖随机性来定位集群
        self.kmeans_ = KMeans(self.n_clusters, n_init=10,
                              random_state=self.random_state)

        # sample_weight可指定样本的相对权重, 属于KMeans算法里的超参数，训练前指定。
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self # 永远返回self

    def transform(self, X):
        # self.kmeans_.cluster_centers_ 集群中心的位置
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)

    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
```

```python
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)
similarities = cluster_simil.fit_transform(housing[["latitude", "longitude"]],
                                           sample_weight=housing_labels)

similarities[:3].round(2)
```

对于``m*2`的数据集而言，`KMeans`会返回一个`m*n_clusters`的矩阵，表示该点距离每个集群中心的聚合程度，数值越大，代表离集群中心点越近。

```python
housing_renamed = housing.rename(columns={
    "latitude": "Latitude", "longitude": "Longitude",
    "population": "Population",
    "median_house_value": "Median house value (USD)"})
housing_renamed["Max cluster similarity"] = similarities.max(axis=1)

housing_renamed.plot(kind="scatter", x="Longitude", y="Latitude", grid=True,
                     s=housing_renamed["Population"] / 100, label="Population",
                     c="Max cluster similarity",
                     cmap="jet", colorbar=True,
                     legend=True, sharex=False, figsize=(10, 7))
# plt.scatter(cluster_simil.kmeans_.cluster_centers_[:, 1],
#          cluster_simil.kmeans_.cluster_centers_[:, 0],
#          color="black", marker="X", s=200,
#          label="Cluster centers")

plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],
         cluster_simil.kmeans_.cluster_centers_[:, 0],
         linestyle="", color="black", marker="X", markersize=20,
         label="Cluster centers")

plt.legend(loc="upper right")
plt.show()
```

```python
# todo：使用ClusterSimilarity，  调整成其他的集群数量和gamma值，训练数据（fit_transform)后，绘图
new_clu_1 = ClusterSimilarity(n_clusters = 5 ,gamma = 1.0 , random_state = 42)
new_clu_2 = ClusterSimilarity(n_clusters = 10 ,gamma = 0.5 , random_state = 42)


```





### 流水线

- 转换流水线

1. 之前的所有操作都是在对部分列做转换，发现需要这些转换来调整训练数据，而且这些转换有一定的顺序依赖！ -> 正确的顺序指定许多数据转换
2. 依赖零散代码太乱，把每个操作使用函数/类实例的方法 很难有顺序关系，需要对这些转换组织管理
3. Scikit-Learn的Pipeline类，把各种数据转换组装成流水线，解决上面的问题！

```python
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])
num_pipeline
```

Pipeline构造函数采用定义一系列步骤的名称/估计器对（二元组）列表。名称可以是任何名称，只要它们是唯一的并且不包含双下划线(__)。

名字在调整转换器的超参数很有用

除了最后一个，估计器必须都是转换器［即它们必须有一个fit_transform()方法］； 最后一个可以是任何东西：转换器、预测器或任何其他类型的估计器。

```python
from sklearn.pipeline import make_pipeline

num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())  # 不想给转换器命名，直接make_pieline,把各个转换器按位置参数传递进去

# 使用转换器类的名称创建流水线，小写且不带下划线（例如"simpleimputer")
num_pipeline
```

```python
sklearn.set_config(display='diagram') # 所有估计器将呈现交互式图表，可以可视化流水线

num_pipeline
```

如果多个转换器具有相同的名称，则在它们的名称后附加一个索引（例如，"foo-1""foo-2"等）。

当调用流水线的fit()方法时，它会在所有转换器上依次调用fit_transform()，将每次调用的输出作为参数传递给下一次调用，直到它到达最终的估计器，它只调用fit()方法。

流水线公开了与最终估计器相同的方法。在这个示例中，最后一个估计器是一个StandardScaler，它是一个转换器，所以流水线也像一个转换器。如果你调用流水线的transform()方法，它将顺序地将所有转换应用于数据。如果最后一个估计器是预测器而不是转换器，那么流水线有一个predict()方法而不是transform()方法。调用它会按顺序将所有转换应用于数据并将结果传递给预测器的predict()方法。

```python
housing_num_prepared = num_pipeline.fit_transform(housing_num)  # todo提问：这一行会发生什么？
housing_num_prepared[:2].round(2)


# 利用pipeline的get_feature_names_out()方法 恢复DataFrame
df_housing_num_prepared = pd.DataFrame(
    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),
    index=housing_num.index)  # 恢复成DataFrame
df_housing_num_prepared
df_housing_num_prepared.head(2)
```

流水线还支持索引；例如，pipeline[1]返回流水线中的第二个估计器，而pipeline[：-1]返回一个Pipeline对象，其中包含除最后一个估计器之外的所有估计器。你还可以通过steps属性（名称/估计器对列表）或通过named_steps字典属性（将名称映射到估计器）访问估计器。例如，num_pipeline["simpleimputer"]返回名为"simpleimputer"的估计器。

流水线内的估计器的具体属性可以获得，也可以修改，通过`实例化的pipeline名字.set_params(估计器名字__属性名 = 值)`来进行修改，这得益于其内部机制，他会把估计器和属性通过`__`来连接，也正因如此估计器命名不能有`__`。

```python
num_pipeline.steps
num_pipeline[1]

num_pipeline[:-1]
num_pipeline.named_steps["simpleimputer"]
num_pipeline.named_steps["simpleimputer"].strategy
num_pipeline.set_params(simpleimputer__strategy="median")
```

到目前为止，已经分别处理了类别列和数值列。拥有一个能够处理所有列的转换器，

实际上，对每一列应用适当的转换会更方便。为此，你可以使用sklearn的ColumnTransformer。例如，以下的ColumnTransformer会将num_pipeline（刚刚定义的）应用于数值属性，将cat_pipeline（下面代码定义的）应用于分类属性：

```python
from sklearn.compose import ColumnTransformer


num_attribs = ["longitude", "latitude", "housing_median_age", "total_rooms",
               "total_bedrooms", "population", "households", "median_income"]
cat_attribs = ["ocean_proximity"]

cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# 不同列用不同的转换器流水线
preprocessing = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])
```



1. 导入ColumnTransformer类
2. 然后定义数字和类别列名称的列表，并为分类属性构建一个简单的流水线。
3. 最后，构造一个ColumnTransformer。它的构造函数需要一个三元组的列表，每个包含一个名称（必须是唯一的并且不包含双下划线）、一个转换器，以及一个转换器应该应用到的列的名称（或索引）列表。

- remainder参数："drop"删除列，"passthrough"保持列不变，默认情况下，未列出的列将被删除，remainder也可以是其他转换器，意味着对其他列按这个转换器处理

手动写列名不方便，且容易出错，sklearn提供了make_column_selector()函数，返回一个选择器（函数），可以使用它自动选择给定类型的所有列（特征），比如数值/object。选择器函数可以取代列名或索引传递给ColumnTransformer。

和make_piepeline类似，不想自己命名列转换器的名称，可以使用make_column_transformer(), 自动命名为"pipeline-1"和"pipeline-2"

```python
from sklearn.compose import make_column_selector, make_column_transformer

preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)
```

预处理流水线，它获取整个训练数据集并将每个转换器应用于适当的列，然后水平连接转换后的列（转换器绝不能更改行数）。这再一次返回一个NumPy数组，可以使用preprocessing.get_feature_names_out()来获取列名，并将数据包装在一个DataFrame中

OneHotEncoder返回一个稀疏矩阵，而num_pipeline返回一个密集矩阵。当稀疏矩阵和密集矩阵混合存在时，ColumnTransformer会估计最终矩阵的非零单元的比率），如果密度高于给定阈值（默认情况下，sparse_threshold=0.3）。返回一个密集矩阵。

```python
housing_prepared = preprocessing.fit_transform(housing)
housing_prepared_fr = pd.DataFrame(
    housing_prepared,
    columns=preprocessing.get_feature_names_out(),
    index=housing.index)
housing_prepared_fr.head(2)
```



- 整合：创建单一的流水线执行之前的所有转换!!
1. 大多数ML算法不期望缺失值, 数字特征中的缺失值将通过用中位数替换它们来估算，。在分类特征中，缺失值将被最常见的类别替换。
2. 大多数ML算法只接受数字输入, 类别特征将被独热编码
3. 计算并添加一些比率特征：bedrooms_ratio、rooms_per_house和people_per_house。希望这些能更好地与房价中位数相关联
4. 添加集群相似性特征。可能比纬度和经度对模型更有用
5. 长尾特征被它们的对数取代，因为大多数模型更喜欢具有大致均匀分布或高斯分布的特征。
6. 大多数ML算法更喜欢所有特征具有大致相同的尺度, 所有数值特征都将被标准化

以下代码整合了全部操作：

```python
def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]  # feature names out   ['输出特征名']

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())

log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),            # 一对一映射
    StandardScaler())

cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)

default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
                                     StandardScaler())

# cat_pipeline = make_pipeline(
#     SimpleImputer(strategy="most_frequent"),
#     OneHotEncoder(handle_unknown="ignore"))

# class ClusterSimilarity(BaseEstimator, TransformerMixin):
#     def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
#         self.n_clusters = n_clusters
#         self.gamma = gamma
#         self.random_state = random_state
#
#     def fit(self, X, y=None, sample_weight=None):
#         # KMeans估计器相关参数：集群数量，随机种子，KMeans是一个随机算法，依赖随机性来定位集群
#         self.kmeans_ = KMeans(self.n_clusters, n_init=10,
#                               random_state=self.random_state)
#
#         # sample_weight可指定样本的相对权重, 属于KMeans算法里的超参数，训练前指定。
#         self.kmeans_.fit(X, sample_weight=sample_weight)
#         return self # 永远返回self
#
#     def transform(self, X):
#         # self.kmeans_.cluster_centers_ 集群中心的位置
#         return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
#
#     def get_feature_names_out(self, names=None):
#         return [f"Cluster {i} similarity" for i in range(self.n_clusters)]

preprocessing = ColumnTransformer([
        ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),
        ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
        ("people_per_house", ratio_pipeline(), ["population", "households"]),
        ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population",
                               "households", "median_income"]),
        ("geo", cluster_simil, ["latitude", "longitude"]),
        ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
    ],
    remainder=default_num_pipeline)  # remainder，剩下的列用什么转换器，现在就剩下housing_median_age
```

```python
# 执行所有转换，输出具有24个特征的numpy数组
housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape

preprocessing.get_feature_names_out()
```


1. 之前完成了：明确问题，获取数据，进行探索，采样训练集和测试集，编写了预处理流水线来自动清洗+转换数据
2. 现在开始关注模型和训练，因为之前的准备工作，现在这个任务变得简单



### 注入模型与参数

#### `LinearRegression`线性回归模型

```python
from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)
```

```python
housing_predictions = lin_reg.predict(housing)
housing_predictions[:5].round(-2)  # -2 = 近似值最近的百位

housing_labels.iloc[:5].values # 对比真实值
```

```python
# 计算误差百分比
error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1
print(", ".join([f"{100 * ratio:.1f}%" for ratio in error_ratios]))
```

#### 性能指标之均方误差


```python
from sklearn.metrics import mean_squared_error
lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)	# squared = False表示所返回的ndarray是均方根，是均方误差的0.5次幂
lin_rmse
```



大多数地区的房价中位数在120000～265000美元之间，因此68628美元的预测误差很糟糕。

这是模型欠拟合训练数据的样例。发生这种情况时，可能意味着这些特征没有提供足够的信息来做出良好的预测，或者模型不够强大。

解决欠拟合的主要方法是选择更强大的模型，为训练算法提供更好的特征，或者减少对模型的约束。该模型未正则化，排除最后一个选项。

首先尝试一个更复杂的模型，尝试DecisionTreeRegressor，这是一个相当强大的模型，能够在数据中发现复杂的非线性关系

#### `DecisionTreeRegressor`决策树模型

```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree_reg.fit(housing, housing_labels)
```

```python
housing_predictions = tree_reg.predict(housing)
tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
tree_rmse
```



训练出来均方误差是0，实际值和预测值一样，可能是模型完美无缺，更有可能是模型对数据过拟合了，如何确定？

使用交叉验证进行评估，k折交叉验证：通过将原始数据集分成K个子集（折），然后进行K次训练和验证，每次选择一个不同的子集作为验证集，其余的K-1个子集合并作为训练集

直接使用sklearn的k折交叉验证功能

#### k折验证

```python
from sklearn.model_selection import cross_val_score


# cv=10， 拆分为10个不重叠子集，意味者有10次训练和验证
tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)  # sklearn的交叉验证功能期望越大越好，而不是代价函数（越低越好），因此评分函数实际上与RMSE相反，需要切换符号

tree_rmses
pd.Series(tree_rmses).describe()

# 为线性回归模型计算相同的指标
lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
pd.Series(lin_rmses).describe()
```

决策树表现和线性回归一样差，注意，交叉验证不仅可以获得模型性能的估计值，还可以测量该估计值的精确程度（即标准差）。决策树的RMSE约为67153，标准差约为2061。如果只使用一个验证集，你就不会获得此信息。但是交叉验证是以多次训练模型为代价的

决策树出现了严重的过拟合：训练误差0，验证误差很高

#### `RandomForestRegressor`随机森林模型

随机森林的工作原理是在特征的随机子集上训练许多决策树，然后对它们的预测进行平均。

由许多其他模型组成的此类模型称为集成：它们能够提高基础模型（在本例中为决策树）的性能

```python
# 预警： 这段代码可能要跑10分钟
from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing,
                           RandomForestRegressor(random_state=42))
forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,
                                scoring="neg_root_mean_squared_error", cv=10)
pd.Series(forest_rmses).describe()

forest_reg.fit(housing, housing_labels)
housing_predictions = forest_reg.predict(housing)
forest_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
forest_rmse
```

训练的RMSE是17547，仍然比验证的47000低很多，意味着仍然存在相当多的过拟合

可能的解决方案是简化模型、对其进行正则化，或者获取更多的训练数据。

然而，在更深入地研究随机森林之前，应该先试下来自不同类别机器学习算法的许多其他模型（例如，支持向量机，可能还有一个神经网络），而不要花太多时间调整超参数。目标是框定一些（2～5个）有前途的模型。

- 训练前调整预处理和算法的超参数，这个过程无需手动组合超参数值，可以使用机器学习库自动搜索！
- 网格搜索：sklearn的GridSearchCV类，指定需要实验哪些超参数以及这些超参数的哪些值，它会使用交叉验证来评估所有可能的超参数值的组合。



#### 网格搜索

```python
# 警告： 这段代码要跑 8分钟
from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])

# param_grid中有两个字典，所以GridSearchCV将首先评估第一个字典中指定的n_clusters和max_features超参数值的所有3×3=9种组合，然后它将尝试第二个字典中所有2×3=6种超参数值的组合。所以总共网格搜索将探索9+6=15种超参数值的组合，并且它将对每个组合训练流水线3次，因为使用的是3折交叉验证。这意味着总共会有15×3=45轮训练
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)
```


- 注意：可以引用流水线中的任何估计器的任何超参数
- 当`sklearn`看到`preprocessing__geo__n_clusters`时，会在双下划线处拆分此字符串，然后在流水线中查找名为"preprocessing“的估计器，从而找到了用于预处理的`ColumnTransformer`, 接下来在此`ColumnTransformer`中查找名为"geo"的转换器，并找到在纬度和经度属性上使用的`ClusterSimilarity`转换器。然后它找到这个转换器的`n_clusters`超参数；`random_forest__max_features`指的是`max_features`名为"random_forest"的估计器的超参数
- sklearn流水线中包装预处理步骤允许你调整预处理超参数以及模型超参数。这很方便，因为可能会有交互。例如，调整预处理可能会引入更多特征，因此可能也需要增加max_features。如果拟合流水线转换器的计算成本很高，可以将流水线的memory超参数设置为缓存目录的路径：当首次拟合流水线时，Scikit-Learn会将拟合的转换器保存到该目录中。如果你随后使用相同的超参数再次拟合流水线，Scikit-Learn将只加载缓存的转换器。

```python
grid_search.best_params_

grid_search.best_estimator_         # 直接返回流水线
```

```python
# 展示网格搜索过程中  超参数组合的评估结果
cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)

# 下面的代码只是在调整dataframe的显示
cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", "split0_test_score",
                 "split1_test_score", "split2_test_score", "mean_test_score"]]
score_cols = ["split0", "split1", "split2", "mean_test_rmse"]
cv_res.columns = ["n_clusters", "max_features"] + score_cols
cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)
cv_res.head()
```



#### 随机搜索

当探索相对较少的组合时，网格搜索方法很好，但RandomizedSearchCV通常更可取，尤其是当超参数搜索空间很大时。
但RandomizedSearchCV通常更可取的使用方式与GridSearchCV类大致相同，但它不是尝试所有可能的组合，而是评估固定数量的组合，在每次迭代中为每个超参数选择一个随机值。这种方法有几个好处：

1. 如果一些超参数是连续的（或离散的，但有许多可能的值），并且让随机搜索运行1000次迭代，那么它会为每个超参数探索1000个不同的值，而网格搜索只会探索你列出的很少几个值。
2. 假设一个超参数实际上并没有太大的区别，但还不确定。如果它有10个可能的值并将其添加到网格搜索中，则训练时间将延长10倍。但是，如果将其添加到随机搜索中，则不会有任何区别。
3. 如果有6个超参数需要探索，每个都有10个可能的值，那么网格搜索除了训练模型一百万次之外别无选择，而随机搜索总是可以运行你选择的任意次数的迭代。

对于每个超参数，必须提供可能的值列表或概率分布


```python
# 警告，这段代码要跑6分钟
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)  # 3折交叉验证，10组超参数探索

rnd_search.fit(housing, housing_labels)
```

```python
# 展示随机搜索过程中，超参数组合的评估结果
cv_res = pd.DataFrame(rnd_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", "split0_test_score",
                 "split1_test_score", "split2_test_score", "mean_test_score"]]
cv_res.columns = ["n_clusters", "max_features"] + score_cols
cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)
cv_res
```

检查最佳模型，得到一些见解：在这个例子里可以从RandomForestRegressor里得出每个属性对于做出准确预测的相对重要性：

```python
final_model = rnd_search.best_estimator_  # 包括预处理
feature_importances = final_model["random_forest"].feature_importances_
feature_importances.round(2)
```


```python
sorted(zip(feature_importances,
           final_model["preprocessing"].get_feature_names_out()),
           reverse=True)
```


有了这些信息，可尝试删除一些不太有用的特征（例如，只有一个ocean_proximity类别真正有用，因此你可以尝试删除其他类别）。

sklearn.feature_selection.SelectFromModel转换器可以自动删除最无用的特征：当你合它时，它会训练一个模型（通常是随机森林），查看它的feature_importances_属性，然后选择最有用的特征。然后，当你调用transform()时，它会丢弃其他特征。

注意：分析最佳模型的时候，也是确保模型不仅在平均水平上运行良好，而且在所有类别的地区（富裕/贫困地区，北方/南方等）运行良好的好时机，如果模型在某个类别的地区预测不佳，应该选择不对那个类别做预测，可能弊大于利

#### 在测试集上评估系统

分析完最佳模型后，后面的步骤就是在测试集上评估最终模型。 只需要将最终模型（预处理+模型的流水线）的预测方法（predict）应用在测试数据的特征上，并拿预测值和测试集的标签对比，评估预测的性能

```python
X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)
```


- t分布简单理解（可选）
1. t 分布的定义（核心思想）

如果有一组服从正态分布的样本，但**总体标准差 σ 不知道**，只能用样本标准差 $s$ 代替，此时构造的标准化统计量为：

$$
T = \frac{\bar{X} - \mu}{s / \sqrt{n}}
$$

这个统计量 $T$ 服从自由度为 $n - 1$ 的 **t 分布**。
其中各个符号含义如下：

* $\bar{X}$：样本均值
* $\mu$：总体均值
* $s$：样本标准差
* $n$：样本容量
* 自由度（degrees of freedom, df） = $n - 1$

2. 置信区间的计算公式（可配合正态分布理解）

当你用样本标准差 $s$ 来估计总体标准差，且样本量不是非常大时，**应该使用 t 分布**：

$$
\text{置信区间} = \bar{x} \pm t^* \cdot \frac{s}{\sqrt{n}}
$$

其中：
* $\bar{x}$：样本均值
* $s$：样本标准差
* $n$：样本容量
* $\frac{s}{\sqrt{n}}$：标准误差（standard error）
* $t^*$：由自由度 $df = n - 1$ 和置信度（比如 95%）查出的 t 分布临界值， 当n很大时，t分布趋近于标准正态分布，可以去查正态分布下的临界值

```python
# 计算泛化误差的95%置信区间
from scipy import stats
confidence = 0.95

squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence,                        # 置信度 95%
                         len(squared_errors) - 1,           # 自由度 （样本数量-1）
                         loc=squared_errors.mean(),         # 样本均值
                         scale=stats.sem(squared_errors))   # 样本标准误差 （标准差 / sqrt（样本数量))
        )
```

测试集上的RMSE还低于验证RMSE，这个结果不错。现实情况中，系统最终经过微调（超参数搜索）在验证集上好，但测试集不行，这种时候要抵制通过调整超参数让性能在测试集上提升的诱惑，这些改进不太可能泛化到测试集之外的数据




#### todo：sklearn.feature.SelectFromModel

`sklearn.feature_selection.SelectFromModel` 是 Scikit-learn 中的一个特征选择工具，它可以基于模型的特征重要性或系数来自动选择重要特征。这对于降维、减少过拟合和提高模型效率非常有用。

==核心功能==

1. **基于模型的特征选择**：可以使用任何提供特征重要性（如随机森林）或系数（如线性模型）的 estimator 作为基础模型。
2. **自定义阈值**：通过设置 `threshold` 参数，可以选择保留重要性高于特定阈值的特征。（例如`threshold = 'dedian'`会将经过fit后的数据中的特征的重要性进行排序，选择重要性大于中位数的特征）
3. **支持交叉验证**：可以与管道（Pipeline）结合使用，在交叉验证中进行特征选择。

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 加载数据
data = load_breast_cancer()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练基础模型
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 使用 SelectFromModel 进行特征选择
selector = SelectFromModel(rf, threshold="median")  # 保留重要性高于中位数的特征
X_train_selected = selector.fit_transform(X_train, y_train)
X_test_selected = selector.transform(X_test)

# 在选择的特征上训练新模型
rf_selected = RandomForestClassifier(n_estimators=100, random_state=42)
rf_selected.fit(X_train_selected, y_train)

# 评估模型性能
y_pred = rf_selected.predict(X_test_selected)
accuracy = accuracy_score(y_test, y_pred)
print(f"Selected features: {X_train_selected.shape[1]} out of {X_train.shape[1]}")
print(f"Accuracy: {accuracy:.4f}")
```



#### 启动，监控和维护系统

在测试集上通过性能评估后，模型可以部署到生产环境了

部署之前，需要在开发机器上再做些准本工作 （例如，完善代码、编写文档和测试等）。

然后，就可以将模型部署到生产环境。最基本的方法就是保存训练好的最佳模型，将文件传输到你的生产环境，然后加载它。要保存模型，可以直接使用joblib库：

```python
import joblib
joblib.dump(final_model, "my_california_housing_model.pkl")
```



一旦你的模型转移到生产环境，你就可以加载并使用它。为此，你必须首先导入模型依赖的任何自定义类和函数（这意味着将代码转移到生产环境），然后使用joblib加载模型并使用它进行预测

```python
# 随堂练习： 模拟把相关代码弄到线上环境（非开发环境），并在线上环境可以加载模型并预测数据
# 新建一个.py文件，假装这个文件

import joblib

# 假设这个单元格的代码不是在这台电脑上，而是在运行模型的服务器
# 下面代码省略完整的

from sklearn.cluster import KMeans
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics.pairwise import rbf_kernel

def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

#class ClusterSimilarity(BaseEstimator, TransformerMixin):
#    [...]

final_model_reloaded = joblib.load("./models/my_california_housing_model.pkl")

new_data = housing.iloc[:5]  # pretend these are new districts
predictions = final_model_reloaded.predict(new_data)
```

#%% md
模型可能会在网站中使用：用户输入有关新地区的一些数据，然后单击“估计价格”按钮。这会将包含数据的查询发送到Web服务器，该服务器会将其转发到你的Web应用程序，最后你的代码将简单地调用模型的predict()方法（你希望在服务器启动时加载模型，而不是在每次使用模型时）。或者，你可以将模型包装在一个专用的Web服务中，你的Web应用程序可以通过REST API查询该服务。这会使在不中断主应用程序的情况下将模型升级到新版本变得更容易。它还简化了扩展，因为你可以根据需要启动尽可能多的Web服务，并在这些Web服务之间对来自你的Web应用程序的请求进行负载平衡。此外，它允许你的Web应用程序使用任何编程语言，而不仅仅是Python。

![web服务部署并由web程序使用的模型](./images/end_to_end_ml_proj/p2.png)



另一种流行的策略是将模型部署到云端，例如在Google的Vertex AI（以前称为Google Cloud AI Platform和Google Cloud ML Engine）上：只需使用joblib保存模型并将其上传到Google Cloud Storage(GCS)，然后交给Vertex AI并创建一个新模型版本，将其指向GCS文件。 这会提供了一个简单的Web服务，可以处理负载平衡和扩展。它接收包含输入数据（例如，一个地区的数据）的JSON请求，并返回包含预测的JSON响应。然后可以在网站（或正在使用的任何生产环境）中使用此Web服务

#### todo：web部署/云端部署



- 模型部署不是终点，而是新阶段的开始 —— MLOps(机器学习监控和运维）

部署完成并不是机器学习工作的终点，而是维护和优化系统的起点。即使模型在验证集上表现良好，它在真实环境中的性能也可能由于多种原因而下降。因此，我们需要建立一整套 **监控与运维机制（MLOps）** 来保障模型的长期稳定运行。

1. 为什么需要监控和维护？

* 模型性能可能**迅速下降**（如基础设施损坏）或**慢慢衰减**（如数据分布变化、模型过时）。
* 如果不监控，就可能在用户和业务受到严重影响后才发现问题。

2. 常见监控策略

    a. **通过业务指标间接评估模型性能**
        如推荐系统可通过推荐商品的销售量来监控效果。

    b. **通过人工评估模型输出**
        尤其在高风险任务中（如缺陷检测），可以将模型不确定的样本交给人工审核员、专家，甚至众包平台上的评估员进行评估。

    c. **用户反馈机制**
       通过调查、互动设计（如验证码）收集用户反馈数据。


3. 自动化和基础设施的关键组成部分：

为了长期维护模型，应该：

* 自动化新数据收集与标注流程
* 自动化训练与调参脚本（如每日/每周训练）
* 自动化新旧模型对比评估与部署
* 按数据子集（如地区、群体）细分评估模型性能
* 监控输入数据质量（如是否缺失特征、统计特征漂移、新类别出现）
* 设置告警系统，第一时间发现性能异常



4. 模型和数据的版本管理：

* 保留**模型历史版本**，以便快速回滚
* 保留**每个数据集的快照**，确保可复现与回测
* 所有更新要可追踪、可审计，方便对问题溯源



5. 总结：

> **机器学习模型部署只是开始，持续的监控、反馈、更新与回滚机制是支撑业务长期成功的关键。** 这就是 MLOps —— 一个专业、系统化管理机器学习生命周期的工程实践。









## 汇总总结

主要记录import与其解决的问题

```python
# 1. 对数据的简单随机划分，分化训练集和测试集
from sklearn.model_selection import train_test_split	
train_set, test_set = train_test_split(数据变量名,stra, test_size=0.2, random_state=42)
# 注：stratify 参数当传入某列时，会进行分层采样

# 2. 对数据进行分层划分，避免某个属性对于标签有过大的影响
from sklearn.model_selection import StratifiedShuffleSplit
splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
for train_index, test_index in splitter.split(housing, housing["income_cat"])

# 3. 缺失值处理
from sklearn.impute import SimpleImputer  # 归责（把缺失值，填充成某个具体值）
imputer =  SimpleImputer(strategy = 'median/mean/most_frequent')
imputer.fit(数据变量名)

# 4. 异常值处理
from sklearn.ensemble import IsolationForest    # 异常检测模型
isolation_forest = IsolationForest(random_state=42)
outlier_pred = isolation_forest.fit_predict(X)   # 异常特征预测为-1，非异常特征预测为1

# 5. 对于str列的处理--分类编码   不常用
from sklearn.preprocessing import OrdinalEncoder        # 分类编码器
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(数据变量名)

# 6. 对于str列的处理--独热编码   
from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(数据变量名) 			# 得到的时稀疏矩阵

# 7. 独热编码拓展--多标签平摊
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
tags_encoded = mlb.fit_transform(data['tags'])
# 另注：MultiLabelBinarizer处理的时列表，如果列上的数据为'A|B|C'时，可以用data['tags'].str.split('|')来将其转化

# 8. 将数据进行规范，将所有数据换成区间数
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler(feature_range = (下界,上界))
mms.fit_transform(数据变量名)

# 9. 将数据进行规范，将所有数据进行标准化  标准化  (data - 均值mean) / 标准差std
from sklearn.preprocessing import StandardScaler
ss =  StandardScaler()
ss.fit_transform(数据变量名)

# 10. 距离计算-处理多峰
from sklearn.metrics.pairwise import rbf_kernel
rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)

# 11. 线性回归模型
from sklearn.linear_model import LinearRegression
target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())

# 12. 
from sklearn.compose import TransformedTargetRegressor
model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())

# 13. 自定义转换器
from sklearn.preprocessing import FunctionTransformer
log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)  # inverse_func 指定反函数
log_pop = log_transformer.transform(housing[["population"]])

rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))  # 不指定反函数，通过kw_args传字典，指定转换函数的超参数
age_simil_35 = rbf_transformer.transform(housing[["housing_median_age"]])

ratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])
ratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))

# 14. 构建自定义转换器，当用到fit，transform时，可以继承父类在重构覆盖一下原本的类方法，另外，还有get_params()和set_params()。这些对于自动超参数调整很有用。
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

# 15. kmeans识别输入数据的集群， rbf_kernel测量每个样本与每个集群中心的相似程度
from sklearn.cluster import KMeans
self.kmeans_ = KMeans(self.n_clusters, n_init=10, random_state=self.random_state) 
self.kmeans_.fit(X, sample_weight=sample_weight )

# 16. 流水线
from sklearn.pipeline import Pipeline			# Pipeline('名称'，估计器)
num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])

# 17. 流水线制作          				不命名时，系统自动生成流水线的名字，
from sklearn.pipeline import make_pipeline
num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

# 18. 数据清洗，将不同的列与其对应的方法进行整合   传入的是列表，列表元素是三元组！！！！！
from sklearn.compose import ColumnTransformer
processing = ColumnTransformer([(流水线名称1,流水线1,具体的列名1) , (流水线名称2,流水线2,具体的列名2) ])

# 19. 根据列的数据进行整合   			传入的是元组，不能传入列表
from sklearn.compose import make_column_selector, make_column_transformer
preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)

# 20. 计算均方误差
from sklearn.metrics import mean_squared_error
lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False) 

# 21. 决策树模型调用
from sklearn.tree import DecisionTreeRegressor
tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree.fit_transform(训练集,训练集标签)

# 22. k折验证
from sklearn.model_selection import cross_val_score
tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
   
# 23. 随机森林模型调用    			注意：此处为回归任务，RandomForestClassifier是分类任务
from sklearn.ensemble import RandomForestRegressor
forest_reg = make_pipeline(preprocessing,
                           RandomForestRegressor(random_state=42))

# 24. 网格搜素
from sklearn.model_selection import GridSearchCV

# 25. 随机搜素
from sklearn.model_selection import RandomizedSearchCV

# 26. 选择特征
from sklearn.feature_selection import SelectFromModel
```

$\delta$

