# 神经网络
**神经网络（Neural Network）** 指的是一种特定类型的假设函数，它由 **多个参数化的、可微分的函数（也称为“层”）组合而成，用于形成输出**。

这个术语源自生物学的启发，但实际上只要符合上述类型的任何假设函数，都可以被称为神经网络，和生物学关联不大

神经网络是深度学习的核心。它们用途广泛、功能强大且可扩展，使其非常适合处理大型和高度复杂的机器学习任务，例如对数十亿个图像进行分类（例如Google Images），为语音识别服务提供支持，每天向成千上万的用户推荐（例如抖音）观看的最佳视频，或学习在围棋游戏（DeepMind的AlphaGo）中击败世界冠军。

学习顺序：浏览下最简单的神经网络，然后到多层感知机，之后通过Tensorflow的Keras API实现神经网络


## 神经网络的发展过程

1. 人工神经网络已经存在很长一段时间了：它们于1943首次提出，该模型计算了生物神经元如何在动物大脑中协同工作，利用命题逻辑进行复杂的计算。这是第一个人工神经网络架构。从那时起，许多其他架构被发明出来。

2. 人工神经网络的早期成功导致人们普遍相信，我们很快将与真正的智能机器进行对话。当在20世纪60年代人们清楚地知道不能兑现这一承诺（至少相当长一段时间）时，资金流向了其他地方，人工神经网络进入了漫长的冬天。

3. 在20世纪80年代初期，发明了新的架构，并开发了更好的训练技术，从而激发了人们对连接主义（神经网络的研究）的兴趣。但是进展缓慢。

4. 到了20世纪90年代，发明了其他强大的机器学习技术，例如支持向量机。这些技术提供了似乎比人工神经网络更好的结果和更坚实的理论基础，神经网络的研究再次被搁置。

5. 现在目睹了对人工神经网络的另一波兴趣。这波浪潮会像以前一样消灭吗？这里有一些充分的理由使我们相信这次是不同的，人们对人工神经网络重新充满兴趣将对我们的生活产生更深远的影响：
    - 现在有大量数据可用于训练神经网络，并且在非常大和复杂的问题上，人工神经网络通常优于其他机器学习技术。
    - 自20世纪90年代以来，计算能力的飞速增长使得现在有可能在合理的时间内训练大型神经网络。这部分是由于摩尔定律（集成电路中的器件数量在过去的50年中，每两年大约增加一倍）这还要归功于游戏产业——刺激了数百万计强大的GPU卡的生产。此外，云平台已使所有人都可以使用这个功能。
    - 训练算法已得到改进。它们与20世纪90年代使用的略有不同，但是这些相对较小的调整产生了巨大的积极影响。
    - 在实践中，人工神经网络的一些理论局限性被证明是良性的。例如，许多人认为ANN训练算法注定要失败，因为它们可能会陷入局部最优解，但事实证明这在实践中并不是一个大问题，尤其是对于较大的神经网络：局部最优解通常表现得几乎与全局最优解一样好。
    - 人工神经网络似乎已经进入了资金和发展的良性循环。基于人工神经网络的好产品会成为头条新闻，这吸引了越来越多的关注和资金，从而带来了越来越多的进步甚至产生了惊人的产品。


## 感知机

感知机是最简单的神经网络架构，于 1957 年发明。它称为阈值逻辑单元（Threshold Logic Unit，TLU），有时也称为线性阈值单元（Linear Threshold Unit，LTU）。输入和输出是数字（而不是二进制开/关值），并且每个输入连接都与权重相关联。TLU 首先计算其输入的线性函数：

$$z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b = \mathbf{w}^T \mathbf{x} + b.$$

然后它对结果应用阶跃函数：

$$h_w(x) = \text{step}(z).$$

所以它几乎与逻辑回归一样，只是它使用阶跃函数而不是sigmoid函数。就像在逻辑回归中一样，模型参数是输入权重 $\mathbf{w}$ 和偏置项 $b$。

![TLU单元](./images/neural_network/p1.png)



感知机中最常用的阶跃函数是 Heaviside 阶跃函数。有时使用符号函数代替。

**公式：感知机中使用的常见阶跃函数（假设阈值 = 0）**

$$
\text{heaviside}(z) =
\begin{cases}
0, & z < 0 \\
1, & z \ge 0
\end{cases}
\qquad
\text{sgn}(z) =
\begin{cases}
-1, & z < 0 \\
0, & z = 0 \\
+1, & z > 0
\end{cases}
$$

单个 TLU 可用于简单的线性二元分类。它计算其输入的线性函数，如果该结果大于阈值，则输出正类；否则，它输出负类。

有点像逻辑回归 或线性 SVM 分类。例如，可以使用单个 TLU 根据花瓣长度和宽度对鸢尾花进行分类。训练这样的 TLU 需要求出 $w_1、w_2$ 和 $b$ 的正确值（训练算法将在稍后讨论）。

 感知机由一个或多个组织在单层中的TLU组成，其中每个TLU都连接到每个输入。这样的层称为全连接层，或密集层。这些输入构成输入层。由于TLU层产生最终输出，因此称为输出层。例如，具有两个输入和三个输出的感知机如下图所示：

![两个输入神经元和三个输出神经元的感知机](./images/neural_network/p2.png)

这个感知机可以同时将实例分类为三个不同的二元类，这使它成为一个多标签分类器。它也可以用于多类分类。

借助线性代数，下面公式 可用于同时针对多个实例有效地计算一层人工神经元的输出。

**公式：计算全连接层的输出**

$$
h_{\mathbf{w}, b}(\mathbf{X}) = \phi(\mathbf{XW} + \mathbf{b})
$$

其中：
- $\mathbf{X}$ 代表输入特征的矩阵。每个实例一行，每个特征一列。
- 权重矩阵 $\mathbf{W}$ 包含所有连接权重。每个输入一行，每个神经元一列。

- 偏置向量 $\mathbf{b}$ 包含所有偏置项：每个神经元一个。

- 函数 $\phi$ 称为激活函数：当神经元是 TLU 时，它是阶跃函数（后面会讨论其他激活函数）。



## 感知机的训练

感知机一次被送入一个训练实例，并且针对每个实例进行预测。对于产生错误预测的每个输出神经元，它会增强来自输入的连接权重，这些权重有助于正确的预测。这个规则公式如下：

**公式：感知机学习规则（权重更新）**

$$
w_{i,j}^{\text{下一步}} = w_{i,j} + \eta (y_j - \hat{y}_j) x_i
$$

其中：
- $w_{i,j}$ 是第 $i$ 个输入和第 $j$ 个神经元之间的连接权重。
- $x_i$ 是当前训练实例的第 $i$ 个输入值。
- $\hat{y}_j$ 是当前训练实例的第 $j$ 个输出神经元的输出。
- $y_j$ 是当前训练实例的第 $j$ 个输出神经元的目标输出。
- $\eta$ 是学习率

每个输出神经元的决策边界都是线性的，因此感知机无法学习复杂的模式（和逻辑回归很像）；但如果训练实例是线性可分离的，这个算法会收敛到一个解

```python
# Scikit-Learn的 Perceptron类
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron
from sklearn.preprocessing import StandardScaler

iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = (iris.target == 0)

per_clf = Perceptron(random_state=42)
per_clf.fit(X, y)

print(per_clf.coef_, per_clf.intercept_)
X_new = [[2,0.5],[3,1]]
y_pred = per_clf.predict(X_new)
y_pred
```

```python
# 练习： 自己实现一下（numpy) 感知机的训练，就用上面代码的数据集
iris = load_iris(as_frame=True)
X = iris.data[["petal length (cm)", "petal width (cm)"]].values
y = (iris.target == 0)


# 训练过程
np.random.seed(42)
w = np.random.rand(2)
b = np.random.rand()
eta = 0.1
epoches = 20


for _ in range(epoches):
    for i in range(X.shape[0]):
        y_real = y[i]
        y_pred = 1 if np.dot(w, X[i]) + b >= 0 else 0

        w = w + eta * (y_real - y_pred) * X[i]
        b   = b + eta*(y_real - y_pred)
```

```python
print(w, b)
X_new = [[2,0.5],
         [3,1]]

print(X_new @ w + b  >= 0)

from sklearn.metrics import accuracy_score
accuracy_score(X@w + b >=0, y)
```

感知机有严重缺陷，特别是它们无法解决一些微不足道的问题［例如，异或(XOR)分类问题］。任何其他线性分类模型（例如逻辑回归分类器）都是如此

![XOR分类问题](./images/neural_network/p3.png)

可以通过堆叠多个感知机来消除感知机的局限性，得出来的神经网络叫多层感知机。 MLP(Multi Layer Perceptron)可以解决异或问题：可以把值(0,0),(0,1),(1,0),(1,1)代入下图验证：

![XOR的MLP](./images/neural_network/p4.png)



```python
# 随堂练习：使用Numpy模拟这个感知机，并测试4个输入：(0,0),(0,1),(1,0),(1,1)
import numpy as np

def step(z):  # 阶跃函数
    # 实现阶跃函数
    return np.where(z >=0, 1, 0)


def mlp_xor(X):
    # 按图片给定的权重，实现两层感知机

    # 第一层
    W1 = np.ones((2,2))
    b1 = np.array([[-1.5, -0.5]])
    z1 = step(X@W1 + b1)  # shape: (m,2)

    # 第二层
    W2 = np.array([-1,1]).reshape(2,1)
    b2= -0.5
    z2 = step(z1 @W2 + b2)

    return z2



# 测试 4 个输入
X = np.array([[0,0],[1,0],[0,1],[1,1]])
print("X:\n", X)
print("XOR:\n", mlp_xor(X))  # 期望 [0,1,1,0]
```

## 多层感知机和反向传播



MLP由一层输入层、一层或多层TLU（称为隐藏层）和一个TLU的最后一层（称为输出层）组成。靠近输入层的层通常称为较低层，靠近输出层的层通常称为较高层。

![2个输入，1个4个神经元的隐藏层和3个输出神经元的MLP](./images/neural_network/p5.png)

输入到输出仅沿一个方向流动，因此该架构是前馈神经网络（Feedforward Neural Network, FNN）的一个示例

当一个神经网络包含一个深层的隐藏层时，它称为深度神经网络(Deep Neural Network，DNN)。深度学习领域研究DNN，

更广泛地说，它对包含深度计算堆栈的模型感兴趣。即便如此，只要涉及神经网络（甚至是浅层的神经网络），许多人就会谈论深度学习。

- 通用函数逼近 (Universal Function Approximation)

**定理（1维情况）**：
给定任意光滑函数
$$f: \mathbb{R} \to \mathbb{R},$$
闭区间
$$\mathcal{D} \subset \mathbb{R},$$
以及一个 $\epsilon > 0$，

我们可以构造一个单隐层神经网络 $\hat{f}$，使得：
$$
\max_{x \in \mathcal{D}} \, | f(x) - \hat{f}(x) | \le \epsilon
$$

通用逼近定理说明：只要网络有足够的隐藏单元，一个单隐层神经网络就能在任意精度上逼近任何连续函数。

训练MLP的方法是梯度下降：计算模型误差（损失函数）相对于模型参数的梯度。

具有多层的MLP如何计算梯度？

### 多层感知机（MLP）的梯度计算方法

在训练多层感知机（MLP）时，目标是最小化损失函数

$$
L(\hat{y}, y)
$$

并求解各层参数（权重和偏置）的梯度，以便进行梯度下降更新。

1. 数值微分（数值模拟方式）

最直观的方法是使用**数值微分**：
对每个参数 $w$，我们用有限差分近似：
$$
\frac{\partial f}{\partial w} \approx \frac{f(w + \epsilon) - f(w)}{\epsilon}
$$


优点：思路直观，易于理解。

缺点：

* **计算开销极大**：若网络有成千上万个参数，就需要成千上万次函数计算。
* **不准确**：有限差分会引入截断误差和舍入误差。
* **不可扩展**：在深度神经网络中完全不现实。

可以作为教学演示/梯度检查，但不能作为实际训练方法。

---

2. 符号法（手动推公式）

另一种方式是**符号推导**：

* 对网络的计算公式逐层写出，
* 然后直接按链式法则（chain rule）手工推导每个参数的偏导数。

优点：

* 能得到**精确的解析梯度**，没有数值误差。
* 数学上严谨。

缺点：

* **枯燥繁琐**：网络层数一多，公式就会爆炸式增长。
* **容易出错**：人工推导常常漏项或符号错误。
* **难以通用**：每换一个网络结构，都要重新手工推导一遍，无法转化成通用程序。



考虑一个最小的两层网络：
输入 $\mathbf{x} \in \mathbb{R}^2$，隐藏层 2 个神经元，输出层 1 个神经元。

**模型公式：**

**隐藏层：**
$$
z^{(1)} = W^{(1)} \mathbf{x} + b^{(1)}, \quad a^{(1)} = \sigma(z^{(1)})
$$

**输出层：**
$$
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}, \quad \hat{y} = \sigma(z^{(2)})
$$

其中 $\sigma$ 为 Sigmoid 激活函数：
$$
\sigma(u) = \frac{1}{1+e^{-u}}
$$



采用平方误差：
$$
L = \frac{1}{2} (y - \hat{y})^2
$$



我们想要求 $\frac{\partial L}{\partial W^{(2)}}$ 和 $\frac{\partial L}{\partial W^{(1)}}$。



对输出层权重 $W^{(2)}$：
$$
\frac{\partial L}{\partial W^{(2)}}
= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(2)}}
$$

分别计算：

- $\frac{\partial L}{\partial \hat{y}} = -(y - \hat{y})$
- $\frac{\partial \hat{y}}{\partial z^{(2)}} = \hat{y}(1-\hat{y})$
- $\frac{\partial z^{(2)}}{\partial W^{(2)}} = a^{(1)}$

所以：
$$
\frac{\partial L}{\partial W^{(2)}} = -(y - \hat{y}) \cdot \hat{y}(1-\hat{y}) \cdot a^{(1)}
$$



对输入层权重 $W^{(1)}$：
$$
\frac{\partial L}{\partial W^{(1)}}
= \frac{\partial L}{\partial \hat{y}} \cdot
\frac{\partial \hat{y}}{\partial z^{(2)}} \cdot
\frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot
\frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot
\frac{\partial z^{(1)}}{\partial W^{(1)}}
$$

分别计算：

- $\frac{\partial L}{\partial \hat{y}} = -(y - \hat{y})$
- $\frac{\partial \hat{y}}{\partial z^{(2)}} = \hat{y}(1-\hat{y})$
- $\frac{\partial z^{(2)}}{\partial a^{(1)}} = W^{(2)}$
- $\frac{\partial a^{(1)}}{\partial z^{(1)}} = a^{(1)}(1-a^{(1)})$ （逐元素）
- $\frac{\partial z^{(1)}}{\partial W^{(1)}} = \mathbf{x}$

合并：
$$
\frac{\partial L}{\partial W^{(1)}}
= \Big( -(y - \hat{y}) \cdot \hat{y}(1-\hat{y}) \cdot W^{(2)} \Big)
\odot \big(a^{(1)}(1-a^{(1)})\big) \cdot \mathbf{x}^T
$$



**结论与评价：**

- 一层还能手写。
- 层数一多，链式法则展开就非常冗长。
- 推导过程容易漏项、符号出错。

这就是符号法的 **枯燥与不易编程** 之处。
因此我们需要 **反向传播算法**，用统一的矩阵运算高效实现梯度计算。



3. 反向传播

用统一的矩阵运算，把梯度传播过程高效、程序化地实现出来。

- 设定与记号

* 批量输入：$X\in\mathbb{R}^{m\times d\_0}$，$m$ 为 batch 大小
* 第 $i$ 层参数：$W\_i\in\mathbb{R}^{d\_i\times d\_{i+1}}$，$b\_i\in\mathbb{R}^{1\times d\_{i+1}}$（按行广播）
* 预激活/激活：$A\_i=Z\_i W\_i + b\_i,\quad Z\_{i+1}=\sigma\_i(A\_i)$
* 输出：$\hat Y=Z\_{L+1}$，标签 $Y\in\mathbb{R}^{m\times d\_{L+1}}$
* 损失（均方误差，取平均）：

  $$
  \ell(\hat Y,Y)=\frac{1}{2m}\,\lVert \hat Y-Y\rVert_F^2
  $$



- 前向传播 （Forward pass）

1. ==初始化：$Z_1=X$==
2. ==迭代（$i=1,\dots,L$）：==
   $$
   A_i = Z_i W_i + b_i,\qquad Z_{i+1}=\sigma_i(A_i)
   $$



- 反向传播 （Backward pass）

1. ==初始化输出层梯度：==
   $$
   G_{L+1} \;=\; \nabla_{Z_{L+1}}\ell \;=\; \frac{1}{m}\,(\,Z_{L+1}-Y\,)
   $$
2. 反向迭代（$i=L,\dots,1$）：

   * ==层内误差信号（Hadamard 乘积）==
$$
     \delta_i \;=\; G_{i+1}\ \circ\ \sigma_i'(A_i)
$$
   * ==本层参数梯度==
     $$
     \nabla_{W_i}\ell \;=\; Z_i^\top\delta_i,
      \qquad
      \nabla_{b_i}\ell \;=\; \mathbf{1}^\top\delta_i \quad(\mathbf{1}\in\mathbb{R}^{m\times 1})
     $$
     
   * ==传回上一层的梯度==
     $$
     G_i \;=\; \delta_i\, W_i^\top
     $$
     

-  总结

**反向传播（Backpropagation） = 链式求导法则 + 中间结果缓存（$Z\_i, A\_i$）**

> 计算量与一次前向传播同阶（仅常数倍开销），且解析精确、可向量化、高效实现。



随堂练习：用numpy实现简单 反向传播

考虑一个两层 MLP：

* 输入 $\mathbf{x} \in \mathbb{R}^2$
* 隐藏层：2 个神经元，激活函数为 **Sigmoid**
* 输出层：1 个神经元，激活函数为 **Sigmoid**



1. 神经网络结构

**前向传播 Forward pass：**

* 隐藏层：
$$
a^{(1)} = W^{(1)} \mathbf{x} + b^{(1)}, \quad
z^{(1)} = \sigma(a^{(1)})
$$
* 输出层：
$$
\hat{y} = W^{(2)} z^{(1)} + b^{(2)}
$$
* 损失函数（平方误差）：
$$
L = \tfrac{1}{2} (\hat{y} - y)^2
$$
2. 参数设置

给定一组具体参数：

* 输入 $\mathbf{x} = \begin{bmatrix}1 \ 0\end{bmatrix}$
* 目标输出 $y = 1$
* $W^{(1)} = \begin{bmatrix}0.5 & -0.5 \ 0.3 & 0.8 \end{bmatrix}, \quad b^{(1)} = \begin{bmatrix}0 \ 0\end{bmatrix}$
* $W^{(2)} = \begin{bmatrix}1 \ -1\end{bmatrix}^T, \quad b^{(2)} = 0$



3. 任务

a. **前向传播**

b. **反向传播**：
   计算以下梯度：

   * $\frac{\partial L}{\partial W^{(2)}}, \frac{\partial L}{\partial W^{(1)}}$

提示：Sigmoid 导数为
$$
\sigma'(u) = \sigma(u)(1-\sigma(u))
$$
==总结==



> 前向传播计算各层之间的$Z\_i, A\_i$
>
> 反向传播计算梯度   隐藏层最后有一层为输出层，输出层初始化的梯度根据回归或是分类问题进行不同的初始化
>
> 均方误差损失（适用于回归问题）：输出层的初始梯度为$\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
>
> 交叉熵损失（适用于分类问题）输出层的初始梯度为$ \frac{\partial L}{\partial \hat{y}} = \frac{\hat{y} - y}{\hat{y}(1-\hat{y})}$（未经过激活函数时），分类任务（交叉熵损失 + Softmax 激活），输出层梯度会简化（因 Softmax 与交叉熵的导数存在抵消），公式为 $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$（二分类 / 多分类通用，无需额外计算激活函数导数）
>
> 通过前向传播记录的$Z_i , A_i$计算梯度


```python
# 随堂练习： 自己实现 有1个隐藏层的MLP的 反向传播计算梯度的过程 
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def forward_pass(w1, b1, w2, b2, x):
    z1 = x
    a2 = np.dot(z1, w1) + b1
    z2 = sigmoid(a2)

    a3 = np.dot(z2, w2) + b2
    z3 = sigmoid(a3)

    return {"a2":a2, "z2":z2, "a3":a3, "z3":z3}


def backprop(w1, b1, w2, b2, x, y):
    forward_result = forward_pass(w1, b1, w2, b2, x)
    G3 = forward_result["z3"] - y      # 1*1
    dw2 = np.dot(forward_result["z2"].T   , G3 * forward_result["z3"] * (1 -  forward_result["z3"]))  # d*1
    db2 = G3 * forward_result["z3"] * (1 -  forward_result["z3"])                                     # 1*1

    G2 = np.dot(G3 * forward_result["z3"] * (1 -  forward_result["z3"]), w2.T)                        # 1*d
    dw1 = np.dot(x.T, G2 * forward_result["z2"] * (1 -  forward_result["z2"]))                        # n*d

    return {"dw1": dw1, "dw2": dw2}


def loss(w1, b1, w2, b2, x, y):
    return 1/2 * ((y - forward_pass(w1, b1, w2, b2, x)["z3"]) ** 2)

def compute_dw_check(w1, b1, w2, b2, x, y, eps=1e-7):
    dw1 = []
    for i in range(w1.shape[0]):
        for j in range(w1.shape[1]):
            eps_mat = np.zeros_like(w1)
            eps_mat[i, j] = eps
            dw1.append((loss(w1 + eps_mat, b1, w2, b2, x, y) - loss(w1 - eps_mat, b1, w2, b2, X_input, y_target)) / (2*eps))

    w2_1_approx = (loss(w1, b1, w2 + np.array([[eps], [0]]), b2, x, y) - loss(w1, b1, w2 - np.array([[eps], [0]]), b2, X_input, y_target)) / (2*eps)
    w2_2_approx = (loss(w1, b1, w2 + np.array([[0], [eps]]), b2, x, y) - loss(w1, b1, w2 - np.array([[0], [eps]]), b2, X_input, y_target)) / (2*eps)

    return  {"dw1": np.array(dw1).reshape(w1.shape), "dw2": np.array([w2_1_approx, w2_2_approx]).reshape(w2.shape)}

# 变量初始化
X_input = np.array([[1,0]])
y_target = 1

W1 = np.array([[0.5,-0.5],
               [0.3, 0.8]])
b1 = np.array([[0,0]])

W2 = np.array([[1],
               [-1]])

b2 = 0


gradient_check_result = compute_dw_check(W1, b1, W2, b2, X_input, y_target)
backprop_result = backprop(W1, b1, W2, b2, X_input, y_target)

for k in ["dw1", "dw2"]:
    print(f"梯度检查 {k}：\n{gradient_check_result[k]}")
    print(f"反向传播 {k} \n{backprop_result[k]}")
```

了解反向传播计算梯度的优势后，更详细地了解一下反向传播的工作流程：

- 它一次处理一个小批量（例如，每次包含32个实例），并且多次遍历整个训练集。每次遍历都称为一个轮次。
- 每个小批量通过输入层进入网络。然后，该算法为小批量中的每个实例计算第一个隐藏层中所有神经元的输出。结果传递到下一层，计算其输出并传递到下一层，以此类推，直到获得最后一层的输出，即输出层。这就是前向传递(forward pass)：就像进行预测一样，只是保留了所有中间结果，因为反向传递需要它们。
- 接下来，该算法测量网络的输出误差（即该算法使用一种损失函数，该函数将网络的期望输出与实际输出进行比较，并返回一些误差测量值）。
- 然后它计算每个输出偏置和到输出层的每个连接对误差的贡献程度。通过应用链式求导法则来进行分析，从而使此步骤变得快速而精确。
- 之后，该算法再次使用链式法则测量这些误差贡献中有多少来自下层中的每个连接，算法一直进行，到达输入层为止。如前所述，这种反向传递通过在网络中向后传播误差梯度（因此得名该算法），有效地测量了网络中所有连接权重和偏置的误差梯度。
- 最后，该算法执行梯度下降步骤，使用刚刚计算出的误差梯度来调整网络中的所有连接权重

注意：随机初始化所有隐藏层的连接权重很重要，否则训练将失败。例如，如果将所有权重和偏置初始化为零，则给定层中的所有神经元将完全相同，从而反向传播将以完全相同的方式影响它们，因此它们将保持相同。换句话说，尽管每层有数百个神经元，但是模型会像每层只有一个神经元一样工作：不会太聪明。

相反，如果随机初始化权重，则会破坏对称性，并允许反向传播来训练各种各样的神经元。简而言之，反向传播对小批量进行预测（前向传递），测量误差，然后反向通过每一层以测量每个参数的误差贡献（反向传递），最后调整连接权重和偏置以减少误差（梯度下降步骤）。



为了让反向传播算法正常工作，作者对MLP的架构进行了重要更改：将阶跃函数替换为逻辑函数σ(z)=1/(1+exp(-z))，也称为sigmoid函数。这一点很重要，因为阶跃函数仅包含平坦段，所以没有梯度可使用（梯度下降不能在平面上移动），而sigmoid函数在各处均具有定义明确的非零导数，从而使梯度下降在每一步都可以有所进展。实际上，反向传播算法可以与许多其他激活函数（不仅是sigmoid函数）一起很好地工作。这是另外两个受欢迎的选择：

**双曲正切函数**：
$$
\tanh(z) = 2\sigma(2z) - 1
$$
与 sigmoid 函数一样，该激活函数为 S 形，连续且可微，但其输出值范围为 $-1 \sim 1$ （而不是 sigmoid 函数的 $0 \sim 1$）。
在训练开始时，该范围倾向于使每一层的输出或多或少地以 0 为中心，这通常有助于加快收敛速度。



**线性整流单元函数**：
$$
\text{ReLU}(z) = \max(0, z)
$$
ReLU 函数是连续的，但不幸的是，在 $z=0$ 时，该函数不可微分（斜率会突然变化，这可能使梯度下降反弹），如果 $z<0$ 则其导数为 $0$。
但是，实际应用中，它运行良好并且具有计算快速的优点，因此它已成为默认值 。

重要的是，它没有最大输出值这一事实有助于减少“梯度下降”期间的某些问题（之后对此进行讨论）。

```python
import matplotlib.pyplot as plt

def relu(z):
    return np.maximum(0, z)

def derivative(f, z, eps=0.000001):
    return (f(z + eps) - f(z - eps))/(2 * eps)

max_z = 4.5
z = np.linspace(-max_z, max_z, 200)

plt.figure(figsize=(11, 3.1))

plt.subplot(121)
plt.plot([-max_z, 0], [0, 0], "r-", linewidth=2, label="Heaviside")
plt.plot(z, relu(z), "m-.", linewidth=2, label="ReLU")
plt.plot([0, 0], [0, 1], "r-", linewidth=0.5)
plt.plot([0, max_z], [1, 1], "r-", linewidth=2)
plt.plot(z, sigmoid(z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, np.tanh(z), "b-", linewidth=1, label="Tanh")
plt.grid(True)
plt.title("Activation functions")
plt.axis([-max_z, max_z, -1.65, 2.4])
plt.gca().set_yticks([-1, 0, 1, 2])
plt.legend(loc="lower right", fontsize=13)

plt.subplot(122)
plt.plot(z, derivative(np.sign, z), "r-", linewidth=2, label="Heaviside")
plt.plot(0, 0, "ro", markersize=5)
plt.plot(0, 0, "rx", markersize=10)
plt.plot(z, derivative(sigmoid, z), "g--", linewidth=2, label="Sigmoid")
plt.plot(z, derivative(np.tanh, z), "b-", linewidth=1, label="Tanh")
plt.plot([-max_z, 0], [0, 0], "m-.", linewidth=2)
plt.plot([0, max_z], [1, 1], "m-.", linewidth=2)
plt.plot([0, 0], [0, 1], "m-.", linewidth=1.2)
plt.plot(0, 1, "mo", markersize=5)
plt.plot(0, 1, "mx", markersize=10)
plt.grid(True)
plt.title("Derivatives")
plt.axis([-max_z, max_z, -0.2, 1.2])

plt.show()
```

为什么需要激活函数？

如果连接多个线性变换，那么得到的只是一个线性变换。例如，如果 $f(x)=2x+3$ 且 $g(x)=5x-1$，则连接这两个线性函数可以得到另一个线性函数：
$$
f(g(x)) = 2(5x-1)+3 = 10x+1
$$
因此，如果层之间没有非线性，那么即使是很深的层堆叠也等同于单个层，因此你无法解决非常复杂的问题。

相反，具有非线性激活的足够大的 DNN 理论上可以近似任何连续函数。

```python
def sigmoid(zi_bian_liang):
    return 1 / (1+np.exp(-zi_bian_liang))

x = 0.5
y = sigmoid(0.5)

dy_dx = (sigmoid(0.5 + 1e-7) - sigmoid(0.5)) / (1e-7)
print(dy_dx)

print(y * (1-y))
```

## MLP回归

MLP可用于回归任务。如果要预测单个值（例如，房屋的价格，给定其许多特征），则只需要单个输出神经元：其输出就是预测值。

对于多元回归（即一次预测多个值），每个输出维度需要一个输出神经元。例如，要在图像中定位物体的中心，你需要预测2D坐标，因此需要两个输出神经元。

如果还想在物体周围放置边框，则还需要两个数值：物体的宽度和高度。因此，得到了四个输出神经元。

Scikit-Learn包含一个MLPRegressor类，用它来构建一个具有三个隐藏层的MLP，每个隐藏层由50个神经元组成，并在房屋数据集上对其进行训练。

为简单起见，我们将使用Scikit-Learn的fetch_california_housing()函数来加载数据。这个数据集只包含数值特征（没有ocean_proximity特征），并且没有缺失值。

下面的代码首先获取和拆分数据集，然后创建一个流水线来标准化输入特征，之后再将它们送到MLPRegressor。

标准化神经网络非常重要，因为它们是使用梯度下降训练的，当特征具有非常不同的尺度时，梯度下降不会很好地收敛。

最后，代码训练模型并评估其验证误差。该模型在隐藏层中使用ReLU激活函数，并使用称为Adam的梯度下降变体（后面会讨论）来最小化均方误差，并使用一点点l2正则化（你可以通过超参数alpha进行控制）：

```python
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

housing = fetch_california_housing()  # 可能要换成作业5的数据集
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)

mlp_reg = MLPRegressor(hidden_layer_sizes=[50,50,50], random_state=42)
pipeline = make_pipeline(StandardScaler(), mlp_reg)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_valid)
rmse = mean_squared_error(y_valid, y_pred, squared=False)

rmse
```

注意，此 MLP 不对输出层使用任何激活函数，因此它可以自由输出它想要的任何值。这通常没问题，但是如果你想保证输出永远是正数，那么你应该在输出层使用 ReLU 激活函数，或者 softplus 激活函数，它是 ReLU 的平滑变体：
$$
\text{softplus}(z) = \log(1 + \exp(z))
$$
当 $z$ 为负时，softplus 接近 0；当 $z$ 为正时，softplus 接近 $z$。

最后，如果你想保证预测总是落在给定的值范围内，那么你应该使用 sigmoid 函数或双曲正切函数，并将目标缩放到适当的范围：

- sigmoid 为 $0 \sim 1$
- tanh 为 $-1 \sim 1$

遗憾的是，`MLPRegressor` 类不支持输出层中的激活函数。

只需几行代码即可使用Scikit-Learn构建和训练标准MLP，但构建出的神经网络功能有限。这就是马上使用Keras实现MLP的原因。

`MLPRegressor` 类使用均方误差，这通常是回归所需的，但如果训练集中有很多异常值，可能更愿意改用平均绝对误差。

或者，可能想要使用 Huber 损失，它是两者的组合。当误差小于阈值 $\delta$（通常为 1）时，它是二次的，但当误差大于 $\delta$ 时，它是线性的。线性部分使其对异常值的敏感度低于均方误差，而二次部分使其收敛速度更快，比平均绝对误差更精确。但是，`MLPRegressor` 只支持 MSE。

下表总结了回归 MLP 的典型架构。

**典型的回归 MLP 架构**

| 超参数                 | 典型值                                                       |
| ---------------------- | ------------------------------------------------------------ |
| 隐藏层数量             | 取决于问题，但通常为 1～5                                    |
| 每个隐藏层的神经元数量 | 取决于问题，但通常为 10～100                                 |
| 输出神经元数量         | 每个预测维度输出 1 个神经元                                  |
| 隐藏层激活             | ReLU                                                         |
| 输出激活               | 无，或 ReLU/softplus（如果输出为正）或 sigmoid/tanh（如果输出有界） |
| 损失函数               | MSE，或 Huber（如果存在异常值）                              |



## MLP分类

MLP也可以用于分类任务。对于二进制分类问题，只需要使用sigmoid激活函数的单个输出神经元：输出将是0和1之间的数字，可以将其解释为正类的估计概率。负类的估计概率等于1减去该数字。

MLP还可以轻松处理多标签二元分类任务。例如，你可能有一个电子邮件分类系统，该系统可以预测每个收到的电子邮件是正常邮件还是垃圾邮件，并同时预测它是否是紧急电子邮件。

在这种情况下，需要两个输出神经元，两个都使用sigmoid激活函数：第一个输出电子邮件件为垃圾邮件的可能性，第二个输出电子邮件为紧急邮件的可能性。更一般地，为每个正类用一个输出神经元。请注意，每个输出概率不一定要加起来等于1。这使得模型可以输出标签的任意组合：可以包含非紧急正常邮件、紧急正常邮件、非紧急垃圾邮件，甚至可能是紧急垃圾邮件（尽管这可能是一个错误）。

如果每个实例只属于三个或更多分类中的一个类（例如，用于数字图像分类的类0~9），则每个类需要一个输出神经元，并且应该将输出层使用softmax激活函数（见下图）。softmax函数将确保所有估计的概率都在0和1之间，并且它们加起来等于1，因为分类是互斥的。这称为多类分类。

![softmax输出的神经网络](./images/neural_network/p6.png)



关于损失函数，由于正在预测概率分布，因此交叉熵损失（或对数损失）通常是一个不错的选择。

Scikit-Learn在`sklearn.neural_network`包中有一个`MLPClassifier`类。它几乎与`MLPRegressor`类相同，只是它最小化交叉熵而不是MSE。

**典型的分类MLP架构**

| 超参数         | 二元分类                      | 多标签二元分类 | 多类分类 |
| :--------------: | :-----------------------------: | :--------------: | :--------: |
| 隐藏层         | 通常为 1~5 层，具体取决于任务 |同二元分类|同二元分类|
| 输出神经元数量 | 1                             | 每个二元标签 1 | 每个类 1 |
| 输出层激活     | sigmoid                       | sigmoid        | softmax  |
| 损失函数       | 交叉熵                        | 交叉熵         | 交叉熵   |



随堂练习：使用Tensorflow playground（神经网络模拟器）：https://playground.tensorflow.org/， 感受神经网络的相关超参数

a.神经网络学习到的模式。单击“Run”按钮（左上方），尝试训练默认的神经网络。请注意，它如何快速找到一个适合分类任务的最优解。第一隐藏层中的神经元已经学会了简单模式，而第二隐藏层中的神经元已经学会了将第一隐藏层的简单模式组合为更复杂的模式。通常，层数越多，模式越复杂。

b.激活函数。尝试用ReLU激活函数替换tanh激活函数，然后再次训练网络。注意，它很快找到了最优解，但这次边界是线性的。这是由于ReLU函数的形状引起的。

c.局部极小值的风险。修改网络架构，使其只有一个具有3个神经元的隐藏层。对其进行多次训练（要重置网络权重，请单击“Play”按钮旁边的“Reset”按钮）。请注意，训练时间变化很大，有时甚至会停留在局部最小值中。

d.当神经网络太小时会发生的情况。删除一个神经元，只保留两个。请注意，即使你尝试多次，神经网络现在也无法找到一个好的解。该模型的参数太少，系统欠拟合训练集。

e.当神经网络足够大时会发生的情况。将神经元的数量设置为8个，然后对网络进行几次训练。请注意，它现在始终能快速运行，并且不会卡在某一点。这显示了神经网络理论中的一个重要发现：大型神经网络很少会陷入局部极小值，即使陷入了，这些局部最优也几乎与全局最优一样好。但是，它们仍然可能在很长一段时间内停滞不前。

f. 深层网络中梯度消失的风险。选择spiral数据集（“DATA”下的右下数据集），并将网络架构更改为4个隐藏层，每个隐藏层具有8个神经元。请注意，训练花费的时间更长，并且经常长时间停滞不前。还要注意，（右侧）最高层的神经元倾向于比（左侧）最低层的神经元更快地进化。这个问题称为“梯度消失”，可以通过后面要讨论的其他技术来缓解。

g. 尝试其他参数，建立对神经网络的直观了解。

## 使用Keras实现MLP

Keras是TensorFlow的高级深度学习API，可以构建、训练、评估和执行各种神经网络

Keras仅支持TensorFlow。同样，TensorFlow曾经包含多个高级API，但在TensorFlow 2问世时，Keras被正式选为其首选的高级API。安装TensorFlow也会自动安装Keras，如果没有安装TensorFlow，则Keras将无法运行。简而言之，Keras和TensorFlow绑定在一起

其他流行的深度学习库有Facebook的PyTorch (https://pytorch.org) 和谷歌的JAX (https://github.com/google/jax)

归功于它的简单性和出色的文档，Pytorch的受欢迎程度在2018年后呈指数级增长，它的API和Keras非常相似，了解Keras后容易迁移到Pytorch

接下来将通过Keras构建用于图像分类的MLP

### 使用顺序API构建图像分类器

加载数据集。将使用Fashion MNIST，它是MNIST的直接替代品。

它具有与MNIST完全相同的格式（70000个灰度图像，每幅28×28像素，有10个类），但是这些图像代表的是时尚物品，而不是手写数字，

因此每个类都更加多样，问题比MNIST更具挑战性。例如，简单的线性模型在MNIST上可以达到约92%的精度，但在Fashion MNIST上仅能达到约83%的精度。

```python
# Keras提供了一些实用函数来获取和加载常见数据集，包括MNIST、Fashion MNIST等。
import tensorflow as tf

# 加载Fashion MNIST。它已经被打乱并分成了训练集（60000个图像）和测试集（10000个图像）
fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test , y_test) = fashion_mnist

# 保留训练集中的最后5000个图像进行验证
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]

# Tensorflow通常导入为tf，Keras API可通过tf.keras使用
```

```python
X_train.shape
X_train.dtype

X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255. # 将像素强度降低到0-1的范围，255.是为了转成浮点数
#
# 之前的mnist，标签等于5时，说明图像代表手写数字5；但是对于Fashion MNIST,需要一个类名列表来了解需要处理的内容
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
y_train[0]
class_names[y_train[0]]
```

```python
import matplotlib.pyplot as plt
plt.imshow(X_train[0], cmap="binary")
plt.axis('off')
plt.show()
```

```python
n_rows = 4
n_cols = 10
plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))
for row in range(n_rows):
    for col in range(n_cols):
        index = n_cols * row + col
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(X_train[index], cmap="binary", interpolation="nearest")
        plt.axis('off')
        plt.title(class_names[y_train[index]])
plt.subplots_adjust(wspace=0.2, hspace=0.5)

plt.show()
```

```python
# 使用顺序（Squential API创建模型），创建具有两个隐藏层的分类MLP
tf.random.set_seed(42)
model = tf.keras.Sequential()

model.add(tf.keras.layers.Input(shape=[28, 28]))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(300, activation="relu"))  # tf.keras.activations.relu
model.add(tf.keras.layers.Dense(100, activation="relu"))
model.add(tf.keras.layers.Dense(10, activation="softmax"))
```

```python
tf.keras.backend.clear_session()
```

- 首先，设置TensorFlow的随机种子以使结果可重现：每次运行notebook时，隐藏层和输出层的随机权重都是相同的。
- 还可以选择使用tf.keras.utils.set_random_seed()函数，它可以方便地为TensorFlow、Python (random.seed())和NumPy(np.random.seed())设置随机种子
- 下一行创建一个Sequential模型。这是用于神经网络的最简单的Keras模型，仅由顺序连接的单层堆栈组成。这称为顺序API。
- 然后，构建第一层（Input层）并将其添加到模型中。我们指定输入shape，其中不包括批量大小，仅包括实例的形状。Keras需要知道输入的形状，以便确定第一个隐藏层连接权重矩阵的形状。
- 接下来添加一个Flatten层。它的作用是将每个输入图像转换为一维数组：例如，如果它接收到形状为[32，28，28]的批次，就把它重塑为[32，784]。
- 换句话说，如果它接收到输入数据X，它会计算X.reshape(-1，784)。该层没有任何参数，只是用来做一些简单的预处理。
- 接下来添加具有300个神经元的Dense隐藏层。它使用ReLU激活函数。每个Dense层管理自己的权重矩阵，其中包含神经元及其输入之间的所有连接权重。它还管理偏置项的一个向量（每个神经元一个）。
- 之后，添加第二个有100个神经元的Dense隐藏层，还是使用ReLU激活函数。
- 最后，添加一个包含10个神经元的Dense输出层（每个类一个），使用softmax激活函数，因为这些类是互斥的

激活函数的完整列表：https://keras.io/api/layers/activations

```python
# 与其逐层添加，不如传递一个层列表
# 还可以不用Input层，直接用Flatten，并指定input_shape

model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28,28]),
    tf.keras.layers.Dense(300, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])
```

```python
# 模型的summary()方法显示模型的所有层，
# 包括每个层的名称（除非在创建层时进行设置，否则会自动生成）、其输出形状（None表示批大小任意），以及它的参数数量。摘要(summary)以参数总数结尾，包括可训练和不可训练的参数。在这里，只有可训练的参数
model.summary()
```

```python
tf.keras.utils.plot_model(model, "./images/neural_network/my_fashion_mnist_model.png", show_shapes=True)
```

请注意，Dense层通常有很多参数。例如，第一个隐藏层的连接权重为784×300，外加300个偏置项，总共有235500个参数！这为模型提供了足够的灵活性来拟合训练数据，但这也意味着模型存在过拟合的风险，尤其是在你没有大量训练数据的情况下；

模型中的每一层都必须有一个唯一的名称（例如，"dense_2"）。可以使用构造函数的name参数显式设置图层名称，但通常让Keras自动命名图层更简单

==Keras还通过在需要时附加索引来确保名称全局唯一，即使跨模型也是如此，如"dense_2"。 命名唯一保证合并模型时不会出现名称冲突==

==Keras管理的所有全局状态都存储在Keras会话中，可以使用tf.keras.backend.clear_session()清除该会话，可以重置名称计数器==

```python
# tf.keras.backend.clear_session()    	# keras 会对层级自动命名，且为了命名唯一并不会重复利用，此时通过上述代码重置
```

```python
# 使用layers属性轻松获取模型的层列表，或使用get_layers()方法按名称访问层
model.layers
hidden1 = model.layers[1]
hidden1.name
model.get_layer("dense") is hidden1
```

```python
# 使用get_weights()和 set_weights()方法访问层的所有参数。对于Dense层，包括连接权重和偏置项
weights, biases = hidden1.get_weights()
weights
weights.shape
biases
biases.shape
```

注意，Dense层随机初始化了连接权重（这是打破对称性所必需的，正如前面所讨论的）

并且偏置被初始化为零，这是可以的。如果要使用其他初始化方法，则可以在创建层时设置==kernel_initializer==（核是连接权重矩阵的另一个名称）或==bias_initializer==，完整初始化器列表：https://keras.io/api/layers/initializers

权重矩阵的形状取决于输入的数量，这就是在创建模型时指定input_shape的原因。

如果不指定输入形状，也没关系：Keras会简单地等待，直到它知道输入形状，然后才会实际构建模型参数。

当向它提供一些数据（例如，在训练期间）或调用它的build()方法时，就会发生这种情况。

在构建模型参数之前，将无法执行某些操作，例如显示模型摘要或保存模型。因此，如果在创建模型时知道输入形状，最好指定它。

```python
# 创建模型后，必须调用compile()方法来指定损失函数 和 要使用的优化器。
# 也可以选择指定在训练和评估期间要计算的其他指标


# model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
# model.compile(loss="sparse_categorical_crossentropy", optimizer=tf.keras.optimizers.SGD(), metrics=["accuracy"])

model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=[tf.keras.metrics.sparse_categorical_accuracy])
```

使用loss="sparse_categorical_crossentropy"等同于使用loss=tf.keras.losses.sparse_categorical_crossentropy。

同样，使用optimizer="sgd"等同于使用optimizer=tf.keras.optimizers.SGD()

使用metrics=["accuracy"]等同于使用metrics=[tf.keras.metrics.sparse_categorical_accuracy]（使用此损失时）。

将使用许多其他损失、优化器和指标，有关完整列表，请参阅 https://keras.io/api/losses https://keras.io/api/optimizers 和 https://keras.io/api/metrics。

使用"sparse_categorical_crossentropy"损失，因为有稀疏标签（即对于每个实例，只有一个目标类索引，在这种情况下为0～9），并且这些类是互斥的。

相反，如果每个实例的每个类都有一个目标概率（例如独热向量，[0.，0.，0.，1.，0.，0.，0.，0.，0.，0]代表类3），则需要使用"categorical_crossentropy"损失。

如果进行二元分类或多标签二元分类，则在输出层中使用"sigmoid"激活函数，而不是"softmax"激活函数，并且使用"binary_crossentropy"损失。

```python
# 如果要将稀疏标签（即类索引）转换为独热向量标签，则使用tf.keras.utils.to_categorical()函数。反之，请使用np.argmax()函数和axis=1。
import numpy as np
tf.keras.utils.to_categorical([0, 5, 1, 0], num_classes=10)

np.argmax(
    [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
     [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
     [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
     [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],
    axis=1
)
```

关于优化器，"sgd" 表示我们使用随机梯度下降来训练模型。换句话说，Keras 将执行先前所述的反向传播算法（即反向模式自动微分加梯度下降）。之后讨论更有效的优化器，它们改进了梯度下降，而不是自动微分。

使用 SGD 优化器时，调整学习率很重要。因此，通常会希望使用

>  optimizer = tf.keras.optimizers.SGD(learning_rate=?)

```python
# 训练和评估模型
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))
```

将输入特征(X_train)和目标类(y_train)以及要训练的轮次数传递给它（否则它将默认为1，这绝对不足以收敛为一个好的模型）。

还传递了一个验证集（这是可选的）。Keras将在每个轮次结束时测量此集合上的损失和其他指标，这对于查看模型的实际效果非常有用。

如果训练集的性能好于验证集，则模型可能过拟合训练集，或者存在错误，例如训练集和验证集之间的数据不匹配。

随堂练习：形状错误很常见，尤其是在刚开始时，因此应该熟悉错误消息：尝试使用错误形状的输入或标签拟合模型，然后查看你得到的错误。

同样，尝试使用loss="categorical_crossentropy"而不是loss="sparse_categorical_crossentropy"编译模型。

或者可以删除Flatten层

或者使用metrics=[tf.keras.metrics.categorical_crossentroy]  看看运行出来会出什么报错 / 有什么不对的地方

```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28,28]),
    tf.keras.layers.Dense(300, activation="relu"),
    tf.keras.layers.Dense(100, activation="relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])
# model.summary()
model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
              optimizer=tf.keras.optimizers.SGD(),
              metrics=[tf.keras.metrics.sparse_categorical_accuracy])
model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))
```

已经训练了神经网络！

在训练期间的每个轮次，Keras在进度条的左侧显示到目前为止已处理的批次数。批量大小默认为32，由于训练集有55000个图像，因此模型每轮经过1719个批次：1718个批次大小为32，1个批次大小为24。

在进度条之后，可以看到每个样本的平均训练时间，以及训练集和验证集的损失和精度（或你要求的任何其他额外指标）。请注意，训练损失下降了，这是一个好兆头，30轮后验证精度达到88.54%。这略低于训练精度，因此会出现一点点过拟合，但不会很大。

可以将validation_split设置为希望Keras用于验证的训练集的比率，而不是使用validation_data参数传递验证集。例如，validation_split=0.1告诉Keras使用数据的最后10%（在打乱之前）进行验证。

如果训练集非常不平衡，其中某些类的样本过多，而其他类的样本不足，那么在调用fit()方法时设置class_weight参数会很有用，给样本不足的类更大的权重，给样本过多的类更小的权重。

Keras在计算损失时将使用这些权重。如果需要每个实例的权重，请设置sample_weight参数。如果同时提供了class_weight和sample_weight，则Keras会将它们相乘。

例如，如果某些实例由专家标记，而另一些实例使用众包平台标记，则每个实例的权重可能很有用：可能希望为前者赋予更大权重。

还可以通过将其作为validation_data元组的第三项添加到验证集中来提供样本权重（但不提供类权重）

```python
# fit()方法返回一个History对象，其中包含训练参数(history.params)、经历的轮次列表(history.epoch)，
# 最重要的是包含在训练集和验证集（如果有）上每轮结束时测得的损失和额外指标的字典(history.history)。
history.params
history.epoch
history.history

# 使用此字典创建pandas DataFrame并调用其plot()方法
import pandas as pd
pd.DataFrame(history.history)

pd.DataFrame(history.history).plot(
    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel="Epoch",
    style=["r--", "r--.", "b-", "b-*"])
plt.legend(loc="lower left")

plt.show()
```

可以看到训练期间训练精度和验证精度都在稳步提高，而训练损失和验证损失则在下降。这很好。

验证曲线起初彼此比较接近，但随着时间的推移它们变得越来越远，这表明存在一点点过拟合。

在这种情况下，刚开始训练时，该模型看起来在验证集上的表现要好于在训练集上的表现。

但是事实并非如此：验证误差是在每个轮次结束时计算的，而训练误差是使用每个轮次运行平均值计算的。

因此，训练曲线应向左移动半个轮次。

```python
# 训练曲线向左移动0.5个轮次
plt.figure(figsize=(8, 5))
for key, style in zip(history.history, ["r--", "r--.", "b-", "b-*"]):
    epochs = np.array(history.epoch) + (0 if key.startswith("val_") else -0.5)
    plt.plot(epochs, history.history[key], style, label=key)
plt.xlabel("Epoch")
plt.axis([-0.5, 29, 0., 1])
plt.legend(loc="lower left")
plt.grid()
plt.show()
```

训练集的性能最终会超过验证性能，就像通常情况下训练足够长的时间一样。可以说模型尚未完全收敛，

因为验证损失仍在下降，因此可能应该继续训练。可以直接再次调用fit()方法，因为Keras只是从它停止的地方继续训练，应该能够达到大约89.8%的验证精度，

而训练精度将继续上升到100%（这并非总是如此）。

如果对模型的性能不满意，则应回头调整超参数。首先要检查的是学习率。

如果这样做没有帮助，请尝试使用另一个优化器（并在更改任何超参数后始终重新调整学习率）。

如果性能仍然不佳，则尝试调整模型超参数（例如层数）、每层神经元数，以及用于每个隐藏层的激活函数的类型。

还可以尝试调整其他超参数，例如批量大小（可以在fit()方法中使用batch_size参数进行设置，默认为32）。

我们将学习神经网络的超参数调整。对模型的验证精度感到满意后，应在测试集上评估其泛化误差，然后再将模型部署到生产环境中。可以使用evaluate()方法轻松完成此操作（它还支持其他几个参数，例如batch_size和sample_weight）

在测试集上获得比在验证集上略低的性能是很常见的，因为超参数是在验证集而不是测试集上进行调优的

```python
model.evaluate(X_test, y_test)
```

接下来，我们可以使用模型的predict()方法对新实例进行预测。由于没有实际的新实例，因此将仅使用测试集的前三个实例：

```python
# 使用模型进行预测
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)
```

```python
import numpy as np
y_pred = np.argmax(y_proba, axis=-1)
y_pred
np.array(class_names)[y_pred]
```

```python
# 对3个图像都进行了正确分类
plt.figure(figsize=(9, 2.4))

X_new = X_test[10:20]
y_proba = model.predict(X_new)
y_pred = np.argmax(y_proba, axis=-1)
print(y_pred)

for index, image in enumerate(X_new):
    plt.subplot(1, 10, index + 1)
    plt.imshow(image, cmap="binary", interpolation="nearest")
    plt.axis('off')
    plt.title(class_names[y_pred[index]])
plt.subplots_adjust(wspace=0.5, hspace=0.5)
plt.show()
```

### 使用顺序API构建回归MLP

使用顺序API来构建、训练、评估和使用回归MLP与分类所做的非常相似。

以下代码示例的主要区别在于输出层只有一个神经元（因为只预测一个值），并且它不使用激活函数，损失函数是均方误差，度量是RMSE，

使用像Scikit-Learn的MLPRegressor那样的Adam优化器。

此外，在这个示例中，不需要Flatten层，而是使用Normalization层作为第一层：它与Scikit-Learn的StandardScaler做同样的事情，但它必须在调用模型的fit()方法之前调用adapt()方法来拟合训练数据。（Keras还有其他预处理层）。

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target, random_state=42)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, random_state=42)
```

```python
from sklearn.datasets import get_data_home

get_data_home()  # 看sklearn数据存在本地的位置
```

```python
tf.random.set_seed(42)

# (m,n) -> 单个样数据的形状： (n,)
norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])  # keras的input_shape要的是单个样本的形状， 把第一个维度去掉就是标准化需要的样本形状
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1)
])

model.summary()
```

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
norm_layer.adapt(X_train)
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))

mse_test, rmse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3]
y_pred = model.predict(X_new)
```

```python
print(mse_test)
rmse_test
y_pred
```

当调用adapt()方法时，Normalization层会学习训练数据中的特征均值和标准差。然而，当显示模型的摘要时，这些统计数据被列为不可训练的。这是因为这些参数不受梯度下降的影响。

### 使用函数式API构建复杂模型

顺序API非常简洁明了。然而，虽然Sequential模型非常常见，但有时构建具有更复杂拓扑结构或具有多个输入或输出的神经网络很有用。为此，Keras提供了函数式API。

非顺序神经网络的一个示例是“宽深”神经网络。这种神经网络架构是在2016年发表的论文引入的。它将所有或部分输入直接连接到输出层，如图1所示。这种架构使神经网络能够学习深度模式（使用深度路径）和简单规则（通过短路径)。

相比之下，常规的MLP迫使所有数据流经整个层的堆栈。因此，数据的简单模式最终可能会因为顺序被转换而失真。

![宽深神经网络](./images/neural_network/p7.png)



```python
class MyClass:
    def __init__(self):
        self.x = 10

    def __call__(self, a):
        print(f"这个__call__正在被调用 {a}")

mc = MyClass()
mc("abc")  # MyClass.__call__(mc)   # 展示自定义类的实例 为什么可以被当成函数一样 调用
callable(mc)  # True, mc可以被调用
callable(MyClass)  # True, 类本身可以被调用

a = 1
callable(a)  # False，整数不能被调用
```

上面是python类的魔术方法，`___call__`可以使得类的实例方法可以像函数一样调用

```python
tf.keras.backend.clear_session()

normalization_layer = tf.keras.layers.Normalization()
hidden_layer1 = tf.keras.layers.Dense(30, activation="relu")
hidden_layer2 = tf.keras.layers.Dense(30, activation="relu")
concat_layer = tf.keras.layers.Concatenate()   # 创建合并层
output_layer = tf.keras.layers.Dense(1)

input_ = tf.keras.layers.Input(shape=X_train.shape[1:])
normalized = normalization_layer(input_)
hidden1 = hidden_layer1(normalized)
hidden2 = hidden_layer2(hidden1)

concat = concat_layer([normalized, hidden2])
output = output_layer(concat)

model = tf.keras.Model(inputs=[input_], outputs=[output])
```

```python
# input_ = tf.keras.layers.Input(shape=X_train.shape[1:])
# normalized_layer = tf.keras.layers.Normalization()
# normalized  =   normalized_layer(input_)
# hidden1 = tf.keras.layers.Dense(30, activation="relu")(normalized)
# hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
# concat =  tf.keras.layers.Concatenate()([hidden1, hidden2])
# output = tf.keras.layers.Dense(1)(concat)
#
# model = tf.keras.Model(inputs=[input_], outputs=[output])
```

```python
model.summary()
```

```python
tf.keras.utils.plot_model(model, "./images/neural_network/wide_deep_nn.png", show_shapes=True)
```

在高层次上，前5行创建了我们构建模型所需的所有层，接下来的6行就像函数一样使用这些层，从输入到输出，最后一行通过指向输入和输出创建一个Keras Model对象。

详细地看一下这段代码：
- 首先，创建5层：一个用于标准化输入的Normalization层，两个Dense层（每层有30个神经元，使用ReLU激活函数），一个Concatenate层，以及一个Dense层（输出层只有一个单神经元，没有任何激活函数）。
- 接下来，创建一个Input对象（变量名称input_用于避免掩盖Python的内置input()函数)。这是模型将要获得的输入类型的一个规范，包括其shape和可选的dtype，默认为32位浮点数。一个模型实际上可能有多个输入
- 然后，像函数一样使用Normalization层，将Input对象传递给它。这就是将其称为函数式API的原因。请注意，只是告诉Keras应该如何将各层连接在一起，尚未处理任何实际数据，因为Input对象只是一个数据规范。换句话说，它是一个符号输入。此调用的输出也是符号化的：normalized不存储任何实际数据，仅用于构建模型。· 之后以同样的方式，我们将normalized传递给hidden_layer1，它输出hidden1，再将hidden1传递给hidden_layer2，它输出hidden2。
- 到目前为止，虽然已经按顺序连接了层，但还要使用concat_layer连接输入和第二个隐藏层的输出。同样，还没有连接实际数据：都是象征性的，用于构建模型。
- 之后，我们将concat传递给output_layer，它给我们最终的output。
- 最后，我们创建一个Keras Model，指定要使用的输入和输出。一旦构建了Keras模型，一切就都与之前的一样，因此无须在此重复：编译模型，Normalization层拟合数据，拟合模型，评估模型并使用它进行预测。

```python
# 用这个神经网络来解决房价预测问题
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
normalization_layer.adapt(X_train)
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))
mse_test, rmse_test = model.evaluate(X_test, y_test)
```

```python
X_new = X_test[:3]
y_pred = model.predict(X_new)
```

```python
# 如果想通过宽路径送入特征的子集，而通过深路径送入特征的另一个子集 ： 一次使用多个输入
input_wide = tf.keras.layers.Input(shape=[5], name="NoHidden")  # 5个特征
input_deep = tf.keras.layers.Input(shape=[6], name="YesHidden")  # 6个特征

norm_layer_wide = tf.keras.layers.Normalization()
norm_layer_deep = tf.keras.layers.Normalization()

norm_wide = norm_layer_wide(input_wide)
norm_deep = norm_layer_deep(input_deep)
hidden1 = tf.keras.layers.Dense(30, activation="relu")(norm_deep)
hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
concat = tf.keras.layers.concatenate([norm_wide, hidden2])
output = tf.keras.layers.Dense(1)(concat)

model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])
```

```python
model.summary()
```

```python
tf.keras.utils.plot_model(model, "./images/neural_network/wide_deep_inputs_nn.png", show_shapes=True)
```

![多输入宽深神经网络](./images/neural_network/p8.png)

- 每个Dense层都在同一行上创建和调用。这是一种常见的做法，因为它使代码更简洁而不失清晰。
- 但是，我们不能对Normalization层执行此操作，因为我们需要一个对该层的引用才能在拟合模型之前调用其adapt()方法。
- 使用了tf.keras.layers.concatenate()，创建一个Concatenate层并使用给定的输入调用它。·
- 在创建模型时指定了inputs=[input_wide，input_deep]，因为有两个输入。



```python
# 可以像往常一样编译模型，但当调用fit()方法时，必须传递一对矩阵（X_train_wide, X_train_deep), 代表两个输入，而不是传递单个输入矩阵X_train
# 当调用evaluate() 或 predict()时，对于X_valid以及X_test和X_new也是如此

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])

X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]   # 宽的5个特征（0-4）， 深的6个特征（2-7）
X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]
X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]

X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]

norm_layer_wide.adapt(X_train_wide)
norm_layer_deep.adapt(X_train_deep)

X_train_dict = {"YesHidden": X_train_deep, "NoHidden": X_train_wide}
history = model.fit(X_train_dict, y_train, epochs=20, validation_data=((X_valid_wide, X_valid_deep), y_valid))
```

```python
mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)
y_pred = model.predict((X_new_wide, X_new_deep))
```

创建输入时设置name="input_wide"和name="input_deep"，则可以传递字典{"input_wide"：X_train_wide，"input_deep"：X_train_deep，而不是传递元组(X_train_wide，X_train_deep)。当有很多输入时，强烈建议这样做，以澄清代码和避免弄错顺序。

 在许多用例中，可能需要多个输出：
- 例如，可能想在图片中定位和分类主要对象。这既是回归任务又是分类任务。

同样，可能有基于同一数据的多个独立任务。当然，可以为每个任务训练一个神经网络，但是在许多情况下，通过训练每个任务一个输出的单个神经网络，会在所有任务上获得更好的结果。这是因为神经网络可以学习数据中对任务有用的特征。

例如，可以对面部图片执行多任务分类，使用一个输出对人的面部表情进行分类（微笑、惊讶等），并使用另一个输出来识别他们是否戴着眼镜。

另一个示例是作为正则化技术（即训练约束，其目的是减少过拟合，从而提高模型的泛化能力）。
- 例如，可能希望在神经网络架构中添加一个辅助输出， 避免网络层的参数拟合数据的噪声

辅助输出给了前半段层一个独立的优化目标：前半段参数不仅要服务于主输出，还要让辅助输出的预测达标。这就分散了 “过拟合风险”—— 后半段层不用再承担 “修正所有问题” 的压力，前半段层也被迫学更通用的特征，整个网络的参数不会过度偏向拟合训练数据的局部特性。

![处理多输出，添加辅助输出以正则化](./images/neural_network/p9.png)



```python
# 添加额外的输出非常容易，只需将它们连接到适当的层，然后将它们添加到模型的输出列表中

tf.keras.backend.clear_session()
tf.random.set_seed(42)

input_wide = tf.keras.layers.Input(shape=[5])
input_deep = tf.keras.layers.Input(shape=[6])
norm_layer_wide = tf.keras.layers.Normalization()
norm_layer_deep = tf.keras.layers.Normalization()
norm_wide = norm_layer_wide(input_wide)
norm_deep = norm_layer_deep(input_deep)
hidden1 = tf.keras.layers.Dense(30, activation="relu")(norm_deep)
hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)
concat = tf.keras.layers.concatenate([norm_wide, hidden2])
output = tf.keras.layers.Dense(1)(concat)
aux_output = tf.keras.layers.Dense(1)(hidden2)

model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output, aux_output])
```

```python
tf.keras.utils.plot_model(model, "./images/neural_network/wide_deep_inputs_outputs_nn.png", show_shapes=True)
```

```python
model.summary()
```

每个输出都需要自己的损失函数。因此当编译模型时，应该传递一系列损失。

如果传递单个损失，Keras将假定所有输出必须使用相同的损失。默认情况下，Keras将计算所有这些损失，并将它们简单累加即可得到用于训练的最终损失。

我们更关心主要输出而不是辅助输出（因为它仅用于正则化），因此要给主要输出的损失更大的权重。可以在编译模型时设置所有的损失权重：

```python
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss=("mse", "mse"), loss_weights=(0.9, 0.1), optimizer=optimizer, metrics=["RootMeanSquaredError"])
```

可以传递字典loss={"output"："mse"，"aux_output"："mse"，而不是传递元组loss=("mse"，"mse")

假设用name="output"和name="aux_output"创建了输出层。就像输入一样，这可以澄清代码并避免在有多个输出时出错。也可以为loss_weights传递一个字典。

现在当训练模型时，需要为每个输出提供标签。在此示例中，主输出和辅助输出应尝试预测同一事物，因此它们应使用相同的标签。

因此，如果输出名为"output"和"aux_output"，需要传递(y_train，y_train)或字典{"output"：y_train，"aux_output"：y_train，而不是传递y_train。y_valid和y_test也是如此：

```python
norm_layer_wide.adapt(X_train_wide)
norm_layer_deep.adapt(X_train_deep)

history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20, validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))
)
```

```python
# 评估模型时，Keras返回损失的加权和，以及所有单独的损失和指标
eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test), return_dict=False) # 设置return_dict=True，那么evaluate()将返回一个字典而不是一个列表
eval_results
```

```python
# predict()方法将为每个输出返回预测值
y_train_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))

# predict()方法返回一个元组，它没有return_dict参数来获取字典，但可以利用model.output_names创建一个
y_pred_tuple = model.predict((X_new_wide, X_new_deep))
y_pred = dict(zip(model.output_names, y_pred_tuple))
y_pred
```

```python
# 随堂练习：使用字典改写这个多输入 和 多输出的神经网络模型，并进行训练和评估
tf.keras.backend.clear_session()
tf.random.set_seed(42)
```

### 使用子类化API构建动态模型

顺序API和函数式API都是声明性的：首先声明要使用的层以及应该如何连接它们，然后才能开始向模型提供一些数据进行训练或推断。这有许多优点：可以轻松地保存、克隆和共享模型；可以显示和分析它的结构；该框架可以推断形状和检查类型，因此可以及早发现错误（即在任何数据通过模型之前）。由于整个模型是一个静态图，因此调试起来也相当容易。但另一方面是它是静态的

一些模型涉及循环、变化的形状、条件分支和其他动态行为。对于这种情况，或者只是你喜欢命令式的编程风格，则子类化API非常适合。

子类化Model类，在构造函数中创建所需的层，然后在call()方法中执行所需的计算。例如，创建以下WideAndDeepModel类的实例将给我们一个等效于刚刚使用函数式API构建的模型：

```python
# 构造函数中创建层与在call()方法中使用层分开。
# 不需要创建Input对象：可以使用call()方法的Input参数
import tensorflow as tf
class WideAndDeepModel(tf.keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)  # 需要调用这句话，模型才有名字
        self.norm_layer_wide = tf.keras.layers.Normalization()
        self.norm_layer_deep = tf.keras.layers.Normalization()
        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)
        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        self.main_output = tf.keras.layers.Dense(1)
        self.aux_output = tf.keras.layers.Dense(1)

    def call(self, inputs):
        input_wide, input_deep = inputs

        norm_wide = self.norm_layer_wide(input_wide)
        norm_deep = self.norm_layer_deep(input_deep)
        hidden1 = self.hidden1(norm_deep)
        hidden2 = self.hidden2(hidden1)
        concat = tf.keras.layers.concatenate([norm_wide, hidden2])
        output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        return output, aux_output

tf.random.set_seed(42)
model = WideAndDeepModel(30, activation="relu", name="my_cool_model")
```

```python
# 现在有了一个模型实例，可以编译它，
# 调用它的归一化层（例如，使用model.norm_layer_wide.adapt(...)和model.norm_layer_deep.adapt(...)），拟合它，
# 评估它，并使用它进行预测，就像对函数式API所做的那样。
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer=optimizer,
              metrics=["RootMeanSquaredError"])
model.norm_layer_wide.adapt(X_train_wide)
model.norm_layer_deep.adapt(X_train_deep)

history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))
eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))
y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))
```

```python
class MyModel(tf.keras.Model):
    def __init__(self, units=30, activation="relu", **kwargs):
        super().__init__(**kwargs)
        self.norm_layer = tf.keras.layers.Normalization()
        self.hidden1 = tf.keras.layers.Dense(units=units, activation=activation)
        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)
        self.main_output = tf.keras.layers.Dense(1)

    def call(self, inputs):
        print(f"My Model is called： {inputs}")
        norm = self.norm_layer(inputs)
        hidden1 = self.hidden1(norm)
        hidden2 = self.hidden2(hidden1)
        concat = tf.keras.layers.concatenate([norm, hidden2])
        output = self.main_output(concat)
        return output

my_model = MyModel()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
my_model.compile(loss=["mse"], optimizer=optimizer,
              metrics=["RootMeanSquaredError"])
my_model.norm_layer.adapt(X_train)

```

```python
my_model.fit(X_train, y_train, epochs=10)
```

这个API的最大区别在于，可以在call()方法中包含几乎任何想要的内容：for循环、if语句、低级TensorFlow操作

这使得它在尝试新想法时成为一个很好的API，尤其是对于研究人员而言。

然而，这种额外的灵活性确实是有代价的：模型的架构隐藏在call()方法中，因此Keras无法轻松检查它

无法使用tf.keras.models.clone_model()克隆模型，并且当调用summary()方法时，只会得到一个层列表，而没有任何关于它们是如何相互连接的信息。

而且Keras无法提前检查类型和形状，更容易出错

因此，除非真的需要额外的灵活性，否则你应该坚持使用顺序API或函数式API。

### 保存和还原模型

设置save_format="tf"时，Keras使用TensorFlow的保存模型格式保存模型：这是一个包含多个文件和子目录的目录（具有给定的名称）。特别地，saved_model.pb文件以序列化计算图的形式包含模型的架构和逻辑，因此无须部署模型的源代码即可在生产环境中使用它, SavedModel足够。

keras_metadata.pb文件包含Keras所需的额外信息。variables子目录包含所有参数值（包括连接权重、偏置、归一化统计信息和优化器参数），如果模型非常大，则可能会拆分为多个文件。最后，assets目录可能包含额外的文件，例如数据样本、特征名称、类名称等。默认情况下，assets目录是空的。因为优化器也已保存，包括它的超参数和它可能具有的任何状态，加载模型后可以根据需要继续训练。

```python
model.save("./models/my_keras_model", save_format="tf")

# todo: Warning: Keras's model.save() method no longer supports TensorFlow's SavedModel format. However, you can still export models to the SavedModel format using model.export() like this:
# model.export("./models/my_keras_model")
```

通常会有一个脚本来训练模型并保存它，以及一个或多个脚本（或Web服务）来加载模型并使用它来评估模型或进行预测。加载模型和保存模型一样简单：

还可以使用save_weights()和load_weights()来仅保存和加载参数值。这包括连接权重、偏置、预处理统计数据、优化器状态等。参数值保存在一个或多个文件，以及一个索引文件中

```python
# todo：Warning: Keras now requires the saved weights to have the .weights.h5 extension. There are no longer saved using the SavedModel format.
model.save_weights("./models/my_weights.weights.h5")
model.load_weights("./models/my_weights.weights.h5")
```

```python
# To load a .keras model, use the tf.keras.models.load_model() function. If the model uses any custom object, you must pass them to the function via the custom_objects argument:
model = tf.keras.models.load_model("./models/my_keras_model", custom_objects={"WideAndDeepModel": WideAndDeepModel})
y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))

# todo：Warning: In Keras 3, it is no longer possible to load a TensorFlow SavedModel as a Keras model. However, you can load a SavedModel as a tf.keras.layers.TFSMLayer layer, but be aware that this layer can only be used for inference: no training.
# tfsm_layer = tf.keras.layers.TFSMLayer("./models/my_keras_model")
# y_pred_main, y_pred_aux = tfsm_layer((X_new_wide, X_new_deep))
```

### 使用回调函数

与保存整个模型相比，仅保存权重速度更快且使用的磁盘空间更少，适用于训练期间保存快速检查点

如果你正在训练一个大模型，并且需要数小时或数天，那么必须定期保存检查点以防计算机崩溃。但是如何告诉fit()方法保存检查点呢？使用回调函数。

fit()方法接收一个callbacks参数，该参数允许你指定Keras将在训练前后、每个轮次前后，甚至处理每个批次前后调用的对象列表。例如，在训练期间ModelCheckpoint回调会定期保存模型的检查点，默认情况下，在每个轮次结束时：

```python
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint("./models/my_checkpoints.weights.h5",
                                                   save_weights_only=True)
history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
    callbacks=[checkpoint_cb])
```

此外，如果在训练期间使用验证集，则可以在创建ModelCheckpoint时设置save_best_only=True。在这种情况下，只有当模型在验证集上的性能达到当前最佳时，它才会保存模型。这样，就不必担心训练时间太长而过拟合训练集：只需还原训练后保存的最后一个模型，这就是验证集中的最佳模型。这是实现早停的一种方法，但它不会真正停止训练。

另一种方法是使用EarlyStopping回调。当它在多个轮次（由patience参数定义）测量验证集没有进展时，它将中断训练，如果设置restore_best_weights=True，则它将在训练结束时回滚到最佳模型。可以结合这两个回调来保存模型的检查点，以防计算机崩溃，并在没有更多进展时提前中断训练，以避免浪费时间和资源并减少过拟合：



```python
# 轮次的数量可以设置为一个很大的值，因为当没有更多进展时训练会自动停止（只要确保学习率不要太小，否则它可能会一直进展缓慢，直到结束
early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,
                                                     restore_best_weights=True)
history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=100,
    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
    callbacks=[checkpoint_cb, early_stopping_cb])

# EarlyStopping回调会将最佳模型的权重存储在内存中，并在训练结束时为你恢复它们。
```

tf.keras.callbacks包 (https://keras.io/api/callbacks) 中提供了许多其他回调。

如果需要额外的控制，可以轻松编写自己的自定义回调。例如，以下自定义回调将显示训练期间验证损失与训练损失之间的比率（例如，检测过拟合）：

可以实现on_train_begin()、on_train_end()、on_epoch_begin()、on_epoch_end()、on_batch_begin()和on_batch_end()。

如果需要它们（例如，为了调试），也可以在评估和预测期间使用回调。对于评估，应该实现on_test_begin()、on_test_end、on_test_batch_begin()或on_test_batch_end()，它们由evaluate()调用。对于预测，你应该实现on_predict_begin()、on_predict_end()、on_predict_batch_begin()或on_predict_batch_end()，它们由predict()调用。

```python
class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        ratio = logs["val_loss"] / logs["loss"]
        print(f"Epoch={epoch}, val/train={ratio:.2f}")
```

```python
val_train_ratio_cb = PrintValTrainRatioCallback()
history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
    callbacks=[val_train_ratio_cb], verbose=0)
```

```python
import matplotlib.pyplot as plt
class MyDrawCallback(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs):
        self.losses = []

    def on_epoch_end(self, epoch, logs):
        self.losses.append(logs["loss"])

    def on_train_end(self, logs):

        plt.plot(self.losses)

history = model.fit(
    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,
    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),
    callbacks=[MyDrawCallback()], verbose=0)
```

### 使用TensorBoard进行可视化

TensorBoard是一款出色的交互式可视化工具，可用于在训练期间查看学习曲线；

比较多次运行的学习曲线和指标；可视化计算图；

分析训练统计数据；查看由模型生成的图像；

把复杂的多维数据投影到3D，并自动进行聚类；分析你的网络（即测量其速度以识别瓶颈）等等！

安装TensorFlow时会自动安装TensorBoard。但是，需要一个TensorBoard插件来可视化分析数据。如果按照 **环境安装说明.md** 上的安装说明在本地运行所有内容, 说明已经安装插件

要使用TensorBoard，必须修改程序，以便它将想要可视化的数据输出到称为事件文件的特殊二进制日志文件。

pip/conda install -q -U tensorboard-plugin-profile

每个二进制数据记录称为摘要。TensorBoard服务器将监视日志目录，并将自动获取更改并更新可视化效果：这可以可视化实时数据（有短暂延迟），例如训练期间的学习曲线。

通常想把TensorBoard服务器指向根日志目录并配置自己的程序，以使其在每次运行时都写入不同的子目录。这样相同的TensorBoard服务器实例可以可视化并比较程序多次运行中的数据，而不会混淆所有内容。

```python
from pathlib import Path
from time import strftime

# 根据当前日期和时间生成日志子目录的路径，以便每次运行时都不同：
def get_run_logdir(root_logdir="my_logs"):
    return Path(root_logdir) / strftime("run_%Y_%m_%d_%H_%M_%S")

run_logdir = get_run_logdir()
run_logdir
```

Keras提供了一个方便的TensorBoard()回调，它负责创建日志目录（如果需要，还包括其父目录），

并且它将创建事件文件并在训练期间向它们写入摘要。它将测量模型的训练损失、验证损失和指标（在本例中为MSE和RMSE），并且还将分析神经网络。使用起来很简单

```python
tf.keras.backend.clear_session()
tf.random.set_seed(42)
norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(30, activation="relu"),
    tf.keras.layers.Dense(30, activation="relu"),
    tf.keras.layers.Dense(1)
])
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)
model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])
norm_layer.adapt(X_train)
```

它将在第一个轮次期间对批次100和200之间的网络进行分析。

神经网络通常需要几个批次才能“预热”，因此，不想过早地分析，分析会占用资源，所以最好不要对每个批次都进行分析。

```python
tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,
                                                profile_batch=(100, 200)) # 对批次100和200之间的网络进行分析
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid),
                    callbacks=[tensorboard_cb])
```

每次运行有一个目录，每个目录包含一个用于训练日志的子目录和一个用于验证日志的子目录。两者都包含事件文件，训练日志还包括分析跟踪。

准备好数据后，启动TensorBoard服务器。

这可以使用TensorBoard的Jupyter扩展直接在Jupyter中完成，它与TensorBoard库一起安装。

以下代码为TensorBoard加载Jupyter扩展，第二行为my_logs目录启动TensorBoard服务器，连接到该服务器并直接在Jupyter内部显示用户界面。服务器监听大于或等于6006的第一个可用TCP端口（或者可以使用--port选项设置想要的端口）

```python
# todo： 在notebook里查看tensorboard，第二次如何正常运行
# %load_ext tensorboard
# %tensorboard --logdir=./my_logs
# 随堂练习  学习率改成0.002， 生成整个日志，-> 启动tensroboard。 对比两次产生的曲线
```

```python
如果在自己的机器上运行所有内容，则可以通过在终端中执行tensorboard --logdir=./my_logs来启动TensorBoard。

必须先激活安装TensorBoard的Conda环境，然后进入项目目录。服务器启动后，访问http://localhost：6006。
```

## 微调神经网络超参数

神经网络的灵活性也是它们的主要缺点之一：有许多超参数需要调整。不仅可以使用任何可以想象的网络架构，而且即使在基本的MLP中，也可以更改层数、神经元数量和每层中使用的激活函数类型、权重初始化逻辑、要使用的优化器类型、它的学习率、批量大小，等等。如何知道哪种超参数组合最适合当前任务？

可以使用Keras Tuner库，这是一个用于Keras模型的超参数微调库。它提供多种微调策略，高度可定制，并且与TensorBoard完美集成。

如果按照 **环境安装说明.md**安装的环境，Keras Tuner已经安装。导入keras_tuner，通常作为kt，然后编写一个创建、编译并返回Keras模型的函数。

该函数必须将kt.HyperParameters对象作为参数，它可以用来定义超参数（整数、浮点数、字符串等）及其可能值的范围，这些超参数可用于构建和编译模型。例如，以下函数构建和编译MLP以对Fashion MNIST图像进行分类，使用的超参数有隐藏层数(n_hidden)、每层神经元数(n_neurons)、学习率(learning_rate)，以及要使用的优化器类型(optimizer)：



```python
import keras_tuner as kt


def build_model(hp):
    # 通过 HyperParameter实例（超参数容器）， 去随机出各种超参数，每个一个
    n_hidden = hp.Int("n_hidden", min_value=0, max_value=8, default=2)
    n_neurons = hp.Int("n_neurons", min_value=16, max_value=256)
    learning_rate = hp.Float("learning_rate", min_value=1e-4, max_value=1e-2, sampling="log")
    optimizer = hp.Choice("optimizer", values=["sgd", "adam"])  # 对字符串随机选择
    activation = hp.Choice("activation", values=["relu", "sigmoid", "tanh", "softplus"])

    if optimizer == "sgd":
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)
    else:
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten())

    for _ in range(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation=activation))
    model.add(tf.keras.layers.Dense(10, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
    return model
```

该函数的第一部分定义超参数。例如，hp.Int("n_hidden"，min_value=0，max_value=8，default=2)检查名为"n_hidden"的超参数是否已存在于HyperParameters对象hp中，如果存在，则返回其值。如果不存在，则注册一个名为"n_hidden"的新的整数超参数，其可能取值范围为0到8（含），并返回默认值，本例中为2（未设置default时，返回min_value）。

"n_neurons"超参数以类似的方式注册。"learning_rate"超参数注册为一个范围是10^-4～10^-2的浮点数，并且由于sampling="log"，所有尺度的学习率都将被平均采样。

最后，optimizer超参数注册了两个可能的值："sgd"或"adam"（默认值是第一个，在本例中为"sgd"）。根据optimizer的值，我们创建具有给定学习率的SGD优化器或Adam优化器。

该函数的第二部分只是使用超参数值构建模型。它创建一个Sequential模型，从Flatten层开始，然后是使用ReLU激活函数的隐藏层（由n_hidden超参数确定），以及使用softmax激活函数的具有10个神经元（每个类一个）的输出层。最后，该函数编译模型并将其返回。

```python
# 加载Fashion MNIST。它已经被打乱并分成了训练集（60000个图像）和测试集（10000个图像）
import tensorflow as tf

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test , y_test) = fashion_mnist

# 保留训练集中的最后5000个图像进行验证
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
```

```python
# 想做一个基本的随机搜索，可以创建一个kt.RandomSearch微调器，将build_model函数传递给构造函数，然后调用微调器的search()方法

random_search_tuner = kt.RandomSearch(build_model, objective="val_accuracy", max_trials=5, overwrite=True, directory="./models/my_fashion_mnist", project_name="my_rnd_search", seed=42)

random_search_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
```

RandomSearch微调器首先使用一个空的Hyperparameters对象调用build_model()一次，只是为了收集所有超参数规范。

然后，在这个示例中，它运行了5次实验。对于每次实验，它使用在各自范围内随机采样的超参数构建模型，然后对该模型进行10轮训练并将其保存到my_fashion_mnist/my_rnd_search目录的子目录中。

由于overwrite=True，my_rnd_search目录在训练开始前被删除。如果第二次运行此代码但设置了overwrite=False和max_trials=10，则微调器将继续在它停止的地方进行微调，再运行5次实验：这意味着不必一次性运行所有实验。

最后，由于objective设置为"val_accuracy"，微调器更喜欢验证精度更高的模型，因此一旦微调器完成搜索，就可以获得最佳模型，如下所示：

```python
top3_models = random_search_tuner.get_best_models(num_models=3)
best_model = top3_models[0]
```

```python
# 获取最佳模型的超参数
top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)
top3_params
top3_params[0].values  # best hyperparameter values
```

每个微调器都由一个所谓的“神谕”（Oracle）引导：在每次实验之前，微调器要求神谕告诉它下一次实验应该是什么。RandomSearch微调器使用RandomSearch Oracle，它只是随机选择下一个实验，

由于“神谕”跟踪所有实验，可以要求它给你最好的一个，并且可以显示该实验的摘要

```python
# random_search_tuner.oracle.get_best_trials(num_trials=1)
best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]
best_trial.summary()
```

```python
# 直接访问具体指标
best_trial.metrics.get_last_value("val_accuracy")
```

```python
# 对最佳性能满意，则可以在完整训练集（X_train_full和y_train_full)上继续几轮训练，然后在测试集上评估，最后部署到生产环境中

best_model.fit(X_train_full, y_train_full, epochs=10)
```

```python
test_loss, test_accuracy = best_model.evaluate(X_test, y_test)
```

在某些情况下，想要微调数据预处理超参数或model.fit()参数，例如批量大小。为此，必须使用稍微不同的技术：必须子类化kt.HyperModel类并定义两个方法，即build()和fit()，而不是编写build_model()函数。build()方法做的事情与build_model()函数完全相同。fit()方法接受一个HyperParameters对象和一个编译模型作为参数，以及所有model.fit()参数，拟合模型并返回History对象。至关重要的是，fit()方法可以使用超参数来决定如何预处理数据、调整批量大小等。例如，以下类创建与之前相同的模型，具有相同的超参数，但它还使用布尔"normalize"超参数来控制是否在拟合模型之前对训练数据进行标准化：

```python
class MyClassificationHyperModel(kt.HyperModel):
    def build(self, hp):
        return build_model(hp) # 返回编译后的模型

    def fit(self, hp, model, X, y, **kwargs):
        if hp.Boolean("normalize"):
            norm_layer = tf.keras.layers.Normalization()
            X = norm_layer(X)  # 对训练数据标准化

        batch_size = hp.Int(
            "batch_size",
            min_value=2,
            max_value=128,
            step=2,
            sampling="log",default=32)
        return model.fit(X, y, batch_size=batch_size, **kwargs)
```

```python
kt_random_search = kt.RandomSearch(MyClassificationHyperModel(), objective="val_accuracy", max_trials=5, overwrite=True, directory="./models/my_fashion_mnist_123", project_name="my_rnd_search", seed=42)

kt_random_search.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))
```

可以将此类的一个实例传递给你选择的微调器, 基于MyClassificationHyperModel实例构建一个kt.Hyperband微调器

```python
hyperband_tuner = kt.Hyperband(MyClassificationHyperModel(), objective="val_accuracy", seed=42, max_epochs=10, factor=3, hyperband_iterations=2, overwrite=True, directory="./models/my_fashion_mnist", project_name="hyperband")
```

Scikit-Learn有HalvingRandomSearchCV和HalvingGridSearchCV超参数搜索类，它们的目标是用更少的计算资源在超参数空间内搜索，工作原理如下：

- 在资源有限的情况下（小部分训练集/减少训练迭代次数），训练+交叉验证评估 网格/随机生成的许多超参数组合（候选）
- 一旦每个候选都进行了评估，只有好的候选才能被留下来，继续新的一轮 训练+交叉验证评估，新的一轮增加训练资源
- 经过几轮后，将使用全部资源对最终候选进行评估，这可能会节省一些调整超参数的时间

这个微调器类似于HalvingRandomSearchCV，首先在几个轮次中训练许多不同的模型，然后消除最差的模型并只保留前1/factor个模型（即本例中的前三分之一），重复这个选择过程，直到剩下一个模型。

max_epochs参数控制最佳模型将被训练的最大轮次数。在这种情况下，整个过程重复两次(hyperband_iterations=2)。

每次超参数迭代的所有模型的训练轮次总数约为max_epochs * (log(max_epochs)/log(factor))** 2，因此在此示例中约为44个轮次。其他参数与kt.RandomSearch相同。

```python
# 使用TensorBoard回调函数，运行HyperBand微调器，指向根日志目录
# 同时指定EarlyStopping回调函数

from pathlib import Path

root_logdir = Path(hyperband_tuner.project_dir) / "tensorboard"
tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)
early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)

hyperband_tuner.search(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb, tensorboard_cb])
```

现在，如果打开TensorBoard，将--logdir指向./models/my_fashion_mnist/hyperband/tensorboard目录，将看到所有展开的实验结果。

确保访问HPARAMS选项卡：它包含已尝试的所有超参数组合的摘要以及相应的指标。请注意，HPARAMS选项卡中包含三个选项卡：表格视图、平行坐标视图和散点图矩阵视图。

在左侧面板的下部，取消选中除validation.epoch_accuracy之外的所有指标：这将使图表更清晰。

在平行坐标视图中，尝试在validation.epoch_accuracy列中选择一系列高值：这将仅过滤达到良好性能的超参数组合。

单击其中一个超参数组合，相应的学习曲线将出现在页面底部。

花一些时间浏览每个选项卡，这有助于了解每个超参数对性能的影响，以及超参数之间的相互作用。

Hyperband在分配资源的方式上比纯随机搜索更聪明，但其核心仍然是随机探索超参数空间。它很快，但很粗糙。然而，Keras Tuner还包括一个kt.BayesianOptimization微调器：该算法通过拟合称为高斯过程的概率模型，逐渐了解超参数空间的哪些区域最有希望。这允许它逐渐放大最佳超参数空间。缺点是该算法有自己的超参数：alpha代表在跨实验的性能测量中期望的噪声水平（默认为10^-4），beta指定希望算法进行多少次探索，而不是简单地利用超参数空间的已知良好区域（默认为2.6）。除此之外，这个微调器可以像以前的微调器一样使用：

```python
bayesian_opt_tuner =kt.BayesianOptimization(MyClassificationHyperModel(),objective="val accuracy",seed=42,max_trials=10,alpha=1e-4,beta=2.6,overwrite=True, directory="my_fashion_mnist", project_name="bayesian_opt")

bayesian_opt_tuner.search(...)
```

### 隐藏层数量

对于许多问题，你可以从单个隐藏层开始并获得合理的结果。只要有足够多的神经元，只有一个隐藏层的MLP理论上就可以对最复杂的功能进行建模。但是对于复杂的问题，深层网络的参数效率要比浅层网络高得多：与浅层网络相比，深层网络可以使用更少的神经元对复杂的功能进行建模，从而使它们在相同数量的训练数据下可以获得更好的性能。

为了理解其中的原因，假设要求使用某些绘图软件来绘制森林，但禁止复制和粘贴任何内容。这要花费大量时间：必须分别绘制每棵树（逐个分支，逐个叶子）。如果可以改为先绘制一片叶子，然后复制并粘贴该叶子来绘制一个分支，再复制并粘贴该分支来创建一棵树，最后复制并粘贴该树来创建森林，那么你将很快完成。现实世界中的数据通常以这种层次结构进行构造，而深度神经网络会自动利用这一事实：较低的隐藏层对低层结构（例如形状和方向不同的线段）建模，中间的隐藏层组合这些低层结构，对中间层结构（例如正方形、圆形）进行建模，而最高的隐藏层和输出层将这些中间结构组合起来，对高层结构（例如人脸）进行建模。

这种分层架构不仅可以帮助DNN更快地收敛到一个好的解，而且还可以提高DNN泛化到新数据集的能力。例如，如果已经训练了一个模型来识别图片中的人脸，并且现在想训练一个新的神经网络来识别发型，则可以通过重用第一个网络的较低层来开始训练。你可以将它们初始化为第一个网络较低层的权重和偏置，而不是随机初始化新神经网络前几层的权重和偏置。这样，网络就不必从头开始学习大多数图片中出现的所有低层结构。只需学习更高层次的结构（例如发型）。这称为迁移学习。

总而言之，对于许多问题，你可以仅从一两个隐藏层开始，然后神经网络就可以正常工作。例如，仅使用一个具有几百个神经元的隐藏层，就可以轻松地在MNIST数据集上达到97%以上的精度，而使用具有相同总数神经元的两个隐藏层，则可以在大致相同训练时间上达到98%以上的精度。对于更复杂的问题，你可以增加隐藏层的数量，直到开始过拟合训练集为止。非常复杂的任务（例如图像分类或语音识别）通常需要具有数十层（甚至数百层，但不是全连接的网络，如我们将在CNN（卷积神经网络）中看到的）的网络，并且它们需要大量的训练数据。几乎不必从头开始训练这样的网络，更常见的方法是重用一部分执行类似任务的预训练过的最新网络。这样，训练就会快得多，所需的数据也要少得多

### 每个隐藏层的神经元数量

输入和输出层中神经元的数量取决于任务所需的输入和输出类型。例如，MNIST任务需要28×28=784个输入神经元和10个输出神经元。对于隐藏层，过去通常将它们的大小划分成金字塔状，每一层的神经元越来越少。理由是许多低层特征可以合并成更少的高层特征。MNIST的典型神经网络可能有3个隐藏层，第一层包含300个神经元，第二层包含200个神经元，第三层包含100个神经元。

但是，这种做法在很大程度上已被放弃，因为在所有隐藏层中使用相同数量的神经元似乎在大多数情况下都表现一样好，甚至更好；另外，只需要调整一个超参数，而不是每层一个。

就像层数一样，可以尝试逐渐增加神经元的数量，直到网络开始过拟合为止。或者，可以尝试构建一个模型，其层数和神经元比你实际需要的多一些，然后使用早停和其他正则化技术来防止过拟合。Google的科学家将这种方法称为“弹力裤”方法：与其浪费时间寻找与自己的尺码完全匹配的裤子，不如使用大尺寸的弹力裤来缩小到合适的尺寸。使用这种方法，可以避免可能会破坏模型的瓶颈层。

事实上，如果一层的神经元太少，那么它将没有足够的表征能力来保留来自输入的所有有用信息（例如，一个有两个神经元的层只能输出2D数据，因此如果它获得3D数据作为输入，那么一些信息将丢失）。无论网络的其余部分有多强大，这些信息都将永远无法恢复。

通常通过增加层数而不是每层神经元数，将获得更多收益。

### 学习率，批量大小和其他超参数

#### 学习率

学习率可以说是最重要的超参数。一般而言，最佳学习率约为最大学习率的一半（即学习率大于算法发散的学习率，线性回归梯度下降）。

找到一个好的学习率的一种方法是对模型进行数百次迭代训练，从非常小的学习率（例如10^(-5))开始，然后逐渐将其增加到非常大的值（例如10）。这是通过在每次迭代中将学习率乘以恒定因子来完成的（例如，乘以(10/10^(-5))^(1/500)以在500次迭代中从10^(-5)变为10）。

如果将损失作为学习率的函数进行绘制（对学习率使用对数坐标），应该首先看到它在下降。但是过一会儿学习率将变得过大，因此损失将重新上升：最佳学习率将比损失开始攀升的点低一些（通常是转折点的十分之一）。然后你可以重新初始化模型，并以这种良好的学习率正常训练模型

最佳学习率取决于其他超参数，尤其是批量大小，因此如果你修改了任何超参数，请确保也更新学习率。

#### 优化器

选择比普通的小批量梯度下降更好的优化器（并调整其超参数）也很重要

#### 批量大小

批量大小可能会对模型的性能和训练时间产生重大影响。使用大批量的主要好处是像GPU这样的硬件加速器可以有效地对其进行处理 ，因此训练算法每秒会看到更多的实例。因此，许多研究人员和从业人员建议使用GPU RAM可容纳的最大批量。但是这里有一个陷阱：在实践中，大批量通常会导致训练不稳定，尤其是在训练开始时，结果模型的泛化能力可能不如小批量训练的模型

Yann LeCun甚至在推特上写道：“朋友不会让朋友使用大于32的小批量处理。”并引用了在2018年发表的一篇论文，得出的结论是首选使用小批量(2--32），因为小批量可以在更少的训练时间内获得更好的模型

但是，其他研究者提出相反意见，论文表明，可以通过各种技术手段使用非常大的批量（最多8192）处理，例如提高学习率（即以较小的学习率开始训练，然后提高学习率），并可以获得非常短的训练时间，没有泛化能力的差距。因此一种策略是尝试使用大批量处理，慢慢增加学习率，如果训练不稳定或最终表现令人失望，则尝试使用小批量处理。

#### 激活函数

通常，ReLU激活函数是所有隐藏层的好的默认设置。但是对于输出层，这实际上取决于具体任务。

#### 迭代次数

在大多数情况下，实际上不需要调整训练迭代次数，只需使用早停



# 神经网络加深内容

需要解决一个复杂的问题，例如检测高分辨率图像中的数百种物体，可能需要训练更深的ANN，也许有10层或更多层，每层包含数百个神经元，有成千上万个连接。训练深度神经网络不简单。

以下是可能会遇到的一些问题：
- 在训练过程中反向流过DNN时，可能会遇到梯度变得越来越小或越来越大的问题。这两个问题都使较低的层很难训练。
- 对于如此大的网络，可能没有足够的训练数据，或者做标签的成本太高。
- 训练速度可能会非常缓慢。
- 具有数百万个参数的模型会有很高的风险过拟合训练集，尤其是在没有足够的训练实例或噪声太大的情况下。

将研究这些问题，并介绍解决这些问题的技术。首先探索梯度消失和梯度爆炸问题及它们的一些受欢迎的解决方法。接下来，将研究迁移学习和无监督预训练，即使在标签数据很少的情况下，它们也可以帮助你完成复杂的任务。然后，将讨论可以极大加速训练大型模型的各种优化器。最后，我们将介绍一些流行的针对大型神经网络的正则化技术。使用这些工具，就能够训练非常深的网络。

## 梯度消失和梯度爆炸问题

反向传播算法的第二阶段从输出层到输入层，沿途传播误差梯度。一旦算法计算出代价函数相对于网络中每个参数的梯度，就可以通过梯度下降步骤使用这些梯度来更新每个参数。

随着算法向下传播到较低层，梯度通常会越来越小。因此，梯度下降更新使较低层的连接权重保持不变，训练不能收敛到一个最优解。这称为梯度消失问题。在某些情况下，可能会出现相反的情况：梯度可能会越来越大，各层需要更新很大的权重直到算法发散为止。这是梯度爆炸问题，更笼统地说，深度神经网络受梯度不稳定的影响，不同的层可能以不同的速度学习。

查看sigmoid激活函数，可以看到，当输入变大（负数或正数）时，该函数会以0或1饱和，并且导数非常接近0（即曲线在两个极端处都很平坦）。因此，当反向传播开始时，它几乎没有梯度可以通过网络传播回去，并且随着反向传播通过顶层向下传播，存在的少量梯度会不断被稀释，因此对于较低层来说，实际上什么也没有留下

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-5, 5, 200)

plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [1, 1], 'k--')
plt.plot([0, 0], [-0.2, 1.2], 'k-')
plt.plot([-5, 5], [-3/4, 7/4], 'g--')
plt.plot(z, sigmoid(z), "b-", linewidth=2,
         label=r"$\sigma(z) = \dfrac{1}{1+e^{-z}}$")
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props,
             fontsize=14, ha="center")
plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props,
             fontsize=14, ha="center")
plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props,
             fontsize=14, ha="center")
plt.grid(True)
plt.axis([-5, 5, -0.2, 1.2])
plt.xlabel("$z$")
plt.legend(loc="upper left", fontsize=16)

plt.show()
```

### Glorot初始化和He初始化

Glorot和Bengio在他们的论文中提出了一种能显著缓解不稳定梯度问题的方法。他们指出，需要让信号在两个方向上正确流动：在进行预测时，信号正向流动；在反向传播梯度时，信号反向流动。我们既不希望信号消失，也不希望它“爆炸”并“饱和”。

为了使信号正确流动，作者认为，需要让每层输出的方差等于其输入的方差，并且在反方向时需要让梯度流过某层之前和之后具有相同的方差。

除非该层具有相等数量的输入和输出（这些数字称为该层的扇入和扇出），否则实际上不可能同时保证两者，但是Glorot和Bengio提出了一个很好的折中方案，该方案已在实践中证明能够很好地发挥作用：必须按照公式随机初始化每层的连接权重，这种初始化策略称为Xavier初始化或者Glorot初始化，以论文第一作者的名字命名。

随机初始化权重公式：使用sigmoid激活函数时，均值为0，方差为1/fan_avg的正态分布 或 -r和+r之间的均匀分布，其中r=sqrt(3/fan_avg), 其中fan_avg = (fan_in + fan_out) / 2

如果在公式中用`fan_in`替换`fan_avg`，则会得到Yann LeCun在20世纪90年代提出的初始化策略。他称之为**LeCun初始化**。

当`fan_in = fan_out`时，LeCun初始化等效于Glorot初始化。

研究人员花了十多年的时间才意识到这一技巧的重要性: 使用Glorot初始化可以大大加快训练速度，这是导致深度学习成功的实践之一。



一些论文为不同的激活函数提供了类似的策略。这些策略的差异仅在于方差的大小以及它们使用的是`fan_avg`还是`fan_in`，如表中所示（对于均匀分布，只需使用$-\sqrt{3\sigma^2}$到$\sqrt{3\sigma^2}$的范围）。为ReLU激活函数及其变体提出的初始化策略称为**He初始化**或**Kaiming初始化**，以论文第一作者[何恺明] 名字命名。对于SELU，使用Yann LeCun的初始化方法，最好使用正态分布。

**表：每种激活函数的初始化参数**

| 初始化方法    | 激活函数                                 | $\sigma^2$ (方差)           |
| :-------------: | :----------------------------------------: | :---------------------------: |
| Glorot 初始化 | None、tanh、sigmoid、softmax             | $1/\text{fan}_{\text{avg}}$ |
| He 初始化     | ReLU、Leaky ReLU、ELU、GELU、Swish、Mish | $2/\text{fan}_{\text{in}}$  |
| LeCun 初始化  | SELU                                     | $1/\text{fan}_{\text{in}}$  |

默认情况下，Keras使用具有均匀分布的Glorot初始化。创建层时，可以通过设置`kernel_initializer="he_uniform"`或`kernel_initializer="he_normal"`来将其更改为He初始化：

```python
import tensorflow as tf
dense = tf.keras.layers.Dense(50, activation="relu", kernel_initializer="he_normal")
```

或者，可以使用VarianceScaling初始化器获得表中列出的任何初始化以及更多。例如，如果想用均匀分布并且基于fan_avg（而不是fan_in）进行He初始化，则可以使用下面的代码：

```python
he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode="fan_avg", distribution="uniform")
dense = tf.keras.layers.Dense(50, activation="relu", kernel_initializer=he_avg_init)
```

### 更好的激活函数

Glorot和Bengio在2010年的论文中提出的一项见解是，梯度不稳定的问题部分原因是激活函数选择不当。在此之前，大多数人都认为使用类似sigmoid的激活函数必定是一个好选择。

但是事实证明，其他激活函数在深度神经网络中的表现要更好，尤其是ReLU激活函数，主要是因为它不会在正值输入下饱和，而且它的计算速度非常快。

不幸的是，ReLU激活函数并不完美。它有一个被称为“濒死的ReLUs”的问题：在训练过程中，某些神经元实际上“死亡”了，这意味着它们停止输出除0以外的任何值。在某些情况下，可能会发现网络中一半的神经元都“死亡”了，特别是使用较大的学习率时。当神经元的权重被调整为ReLU函数的输入（即神经元输入的加权和加上它的偏置项）对于训练集中的所有实例都为负时，神经元就会“死亡”。当发生这种情况时，它只会继续输出0，梯度下降不会再影响它，因为ReLU函数的输入为负时其梯度为零。

要解决此问题，你可能需要使用ReLU函数的变体，例如leaky ReLU。

(注意：除非神经元在第一个隐藏层，否则“死亡”的神经元可能会复活：梯度下降会调整各层中的神经元，使得“死亡”神经元输入的加权和再次为正）

#### leaky Relu

```python
def leaky_relu(z, alpha):
    return np.maximum(alpha * z, z)

z = np.linspace(-5, 5, 200)
plt.plot(z, leaky_relu(z, 0.1), "b-", linewidth=2, label=r"$LeakyReLU(z) = max(\alpha z, z)$")
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([0, 0], [-1, 3.7], 'k-')
plt.grid(True)
props = dict(facecolor='black', shrink=0.1)
plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.3), arrowprops=props,
             fontsize=14, ha="center")
plt.xlabel("$z$")
plt.axis([-5, 5, -1, 3.7])
plt.gca().set_aspect("equal")
plt.legend()

plt.show()
```

该函数定义为 $\text{leakyReLU}_\alpha(z) = \max(\alpha z, z)$。超参数 $\alpha$ 定义函数“泄漏”的程度：它是 $z < 0$ 时函数的斜率。$z < 0$ 时的斜率确保 leaky ReLU 永不“死亡”，它们可能会陷入长时间的“昏迷”，但是有机会“醒来”。

2015年的一篇论文比较了ReLU激活函数的几种变体，其结论之一是leaky变体要好于严格的ReLU激活函数。实际上，设置 $\alpha = 0.2$（大泄漏）似乎比 $\alpha = 0.01$（小泄漏）能产生更好的性能。

该论文还对**随机性leaky ReLU (RReLU)** 进行了评估，在训练过程中在给定范围内随机选择 $\alpha$，在测试过程中将其固定为均值。RReLU的表现也相当不错，似乎可以充当正则化函数，降低了过度拟合训练集的风险。

该论文评估了**参数化leaky ReLU (PReLU)**，其中 $\alpha$ 可以在训练期间学习（不是超参数），它像其他任何参数一样，可以通过反向传播进行修改。据报道，PReLU在大型图像数据集上的性能明显优于ReLU，但是在较小的数据集上，它存在过度拟合训练集的风险。

```python
# Keras在tf.keras.layers包中包含LeakyReLU和PReLU类
leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3


dense = tf.keras.layers.Dense(50, activation=leaky_relu,
                              kernel_initializer="he_normal")   # Relu变体使用He初始化
```

```python
# 将LeakyReLU单独用作模型中的层，和之前写activation = leak_relu,对比，对训练和预测没有影响
model = tf.keras.models.Sequential([
    # [...]  # more layers
    tf.keras.layers.Dense(50, kernel_initializer="he_normal"),  # no activation
    tf.keras.layers.LeakyReLU(alpha=0.2),  # 激活函数作为一个独立的层
    # [...]  # more layers
])
```

#### ELU和SELU

ReLU，leaky ReLU都有不是光滑函数的缺点：它们的导数会突然变化，这种不连续性会使梯度下降在最优值附近反弹，并减慢收敛速度。所以关注ReLU激活函数的一些平滑变体：从ELU和SELU开始



2015年提出了一种新的激活函数，称为**指数线性单元 (ELU)**，在作者的实验中其性能优于所有ReLU变体：训练时间减少，神经网络在测试集上表现更好。公式显示了该激活函数的定义。
$$
\text{ELU}_{\alpha}(z) =
\begin{cases}
\alpha(\exp(z)-1), & z < 0 \\
z, & z \geq 0
\end{cases}
$$
ELU激活函数看起来很像ReLU函数，但有几个主要区别：

- **负值输出**：当 \( z < 0 \) 时，它取负值，这使该单元的平均输出接近于0，有助于缓解梯度消失问题。超参数 \( alpha ) 定义为绝对值较大的负数时 ELU 函数逼近的值的相反值。通常将其设置为1，但是你可以像对其他任何超参数一样对其进行调整。

- **非零梯度**：对于 \( z < 0 \)，它具有非零梯度，从而避免了神经元"死亡"的问题。

- **平滑性**：如果 \( alpha ) 等于1，则该函数在所有位置（包括 \( z = 0 \) 左右）都是平滑的，这有助于加速梯度下降，因为它在 \( z = 0 \) 的左右两侧弹跳不大。


在Keras中使用ELU就像设置 `activation="elu"` 一样简单，并且与其他ReLU变体一样，应该使用**He初始化**。


**优点**：
- 训练过程中收敛更快
- 缓解梯度消失问题
- 避免神经元死亡问题

**缺点**：
- 计算速度比ReLU函数及其变体要慢（因为使用了指数函数）
- 在测试时，ELU网络速度将比ReLU网络的慢一点

> 注意：ELU在训练过程中更快的收敛速度通常可以弥补其计算缓慢的缺点。

 不久之后，2017年提出了**可扩展的ELU (Scaled ELU，SELU)** 激活函数：顾名思义，它是ELU激活函数的可扩展实体（约1.05倍ELU，使用α≈1.67）。

作者表明，如果构建一个仅由密集层堆叠组成的神经网络（即MLP），并且所有隐藏层都使用SELU激活函数，则该网络是**自归一化的**：每层的输出倾向于在训练过程中保留均值0和标准差1，从而解决了梯度消失和梯度爆炸的问题。因此，SELU激活函数可能优于MLP（尤其是深度MLP）的其他激活函数。

要在Keras中使用，只需设置 `activation="selu"`。然而，自归一化的发生有几个条件：

- **输入特征标准化**：输入特征必须是标准化的，平均值为0，标准差为1。
- **权重初始化**：每个隐藏层的权重必须使用LeCun初始化（采用正态分布）。在Keras中，这意味着设置 `kernel_initializer="lecun_normal"`。
- **架构限制**：自归一化属性只能通过普通MLP来保证。如果尝试在其他架构中使用SELU，例如在循环神经网络（后期学习的神经网络架构）或具有跳过连接的网络[即跳过层的连接，例如宽深(Wide & Deep)网络]，它可能不会优于ELU。
- **正则化限制**：不能使用正则化技术，例如L1/L2正则化、最大范数、批处理归一化或常规dropout（这些将在后面讨论）。

这些都是重要约束，因此尽管SELU在MLP上有前途，并没有获得很大的吸引力。此外，另外三个激活函数似乎在大多数任务上的表现都非常稳定：GELU、Swish和Mish。

```python
from scipy.special import erfc

# 互补误差函数 erfc(z)：它计算的是从 z 到无穷大的高斯分布曲线下的面积。
# SELU 函数的参数 alpha 和 scale 的推导过程复杂，涉及到确保输出均值为 0、方差为 1 的积分计算。这些积分的形式与高斯分布密切相关，因此自然就会用到 erfc 函数。
# 代码中的 alpha_0_1 和 scale_0_1 的复杂表达式是这些积分计算的结果。
alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1 / np.sqrt(2)) * np.exp(1 / 2) - 1)
scale_0_1 = (
    (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e))
    * np.sqrt(2 * np.pi)
    * (
        2 * erfc(np.sqrt(2)) * np.e ** 2
        + np.pi * erfc(1 / np.sqrt(2)) ** 2 * np.e
        - 2 * (2 + np.pi) * erfc(1 / np.sqrt(2)) * np.sqrt(np.e)
        + np.pi
        + 2
    ) ** (-1 / 2)
)

def elu(z, alpha=1):
    return np.where(z < 0, alpha * (np.exp(z) - 1), z)

def selu(z, scale=scale_0_1, alpha=alpha_0_1):
    return scale * elu(z, alpha)

z = np.linspace(-5, 5, 200)
plt.plot(z, elu(z), "b-", linewidth=2, label=r"ELU$_\alpha(z) = \alpha (e^z - 1)$ if $z < 0$, else $z$")
plt.plot(z, selu(z), "r--", linewidth=2, label=r"SELU$(z) = 1.05 \, $ELU$_{1.67}(z)$")
plt.plot([-5, 5], [0, 0], 'k-')
plt.plot([-5, 5], [-1, -1], 'k:', linewidth=2)
plt.plot([-5, 5], [-1.758, -1.758], 'k:', linewidth=2)
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.axis([-5, 5, -2.2, 3.2])
plt.xlabel("$z$")
plt.gca().set_aspect("equal")
plt.legend()

plt.show()
```

```python
import tensorflow as tf

tf.random.set_seed(42)
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
for layer in range(100):
    model.add(tf.keras.layers.Dense(100, activation="selu",
                                    kernel_initializer="lecun_normal"))
model.add(tf.keras.layers.Dense(10, activation="softmax"))
```

```python
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])
```

```python
fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.
```

```python
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
```

```python
pixel_means = X_train.mean(axis=0, keepdims=True)
pixel_stds = X_train.std(axis=0, keepdims=True)
X_train_scaled = (X_train - pixel_means) / pixel_stds
X_valid_scaled = (X_valid - pixel_means) / pixel_stds
X_test_scaled = (X_test - pixel_means) / pixel_stds
```

```python
history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
```

```python
tf.random.set_seed(42)
model = tf.keras.Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))
for layer in range(100):
    model.add(tf.keras.layers.Dense(100, activation="relu",
                                    kernel_initializer="he_normal"))

model.add(tf.keras.layers.Dense(10, activation="softmax"))
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])
```

```python
# 使用relu学习效果不好，遇到了梯度消失/爆炸的问题， Selu避免了这个问题

history = model.fit(X_train_scaled, y_train, epochs=5,
                    validation_data=(X_valid_scaled, y_valid))
```

#### GELU, Swish和Mish

```python
def swish(z, beta=1):
    return z * sigmoid(beta * z)

def approx_gelu(z):
    return swish(z, beta=1.702)

def softplus(z):
    return np.log(1 + np.exp(z))

def mish(z):
    return z * np.tanh(softplus(z))

z = np.linspace(-4, 2, 200)

beta = 0.6
plt.plot(z, approx_gelu(z), "b-", linewidth=2,
         label=r"GELU$(z) = z\,\Phi(z)$")
plt.plot(z, swish(z), "r--", linewidth=2,
         label=r"Swish$(z) = z\,\sigma(z)$")
plt.plot(z, swish(z, beta), "r:", linewidth=2,
         label=fr"Swish$_{{\beta={beta}}}(z)=z\,\sigma({beta}\,z)$")
plt.plot(z, mish(z), "g:", linewidth=3,
         label=fr"Mish$(z) = z\,\tanh($softplus$(z))$")
plt.plot([-4, 2], [0, 0], 'k-')
plt.plot([0, 0], [-2.2, 3.2], 'k-')
plt.grid(True)
plt.axis([-4, 2, -1, 2])
plt.gca().set_aspect("equal")
plt.xlabel("$z$")
plt.legend(loc="upper left")


plt.show()
```

GELU在2016年引入。同样，可以将其视为ReLU激活函数的平滑实体。其定义在公式中给出，其中$\Phi$是标准高斯累积分布函数(CDF)：$\Phi(z)$对应于从均值为0、方差为1的正态分布中随机采样的值低于z的概率。

**公式：GELU激活函数**
$$
\text{GELU}(z) = z\Phi(z)
$$
如上图所示，GELU类似于ReLU：当其输入$z$为负时，它接近于0；当z为正时，它接近z。然而，尽管我们目前讨论的所有激活函数都是凸函数和单调函数，但GELU激活函数两者都不是：从左到右，它从直线开始，然后向下摆动，到达附近的低点-0.17（接近z < -0.75），最后弹起并最终直奔右上角。

这种相当复杂的形状以及它在每个点都有曲率的事实可以解释为什么它工作得如此好，特别是对于复杂的任务：梯度下降可能会发现它更容易适应复杂的模式。在实践中，它通常优于目前讨论的所有其他激活函数。

然而，它的计算量更大一些，而且它的性能提升并不总是足以证明出额外代价的合理性。也就是说，可以证明它近似等于$z\sigma(1.702z)$，其中$\sigma$是sigmoid函数：使用这种近似值也很有效，而且它的优点是计算速度快得多。

GELU论文还引入了sigmoid线性单元(SiLU)激活函数，它等于 $z\sigma(z)$，但在作者的测试中已被GELU超越了。有趣的是，其他人在2017年发表的论文中中通过自动搜索好的激活函数重新发现了SiLU函数。作者将其命名为**Swish**，并且这个名字流行了起来。

在他们的论文中，Swish优于其他所有函数，包括GELU。Ramachandran等人后来通过添加额外的超参数$\beta$来缩放sigmoid函数的输入，从而泛化了Swish。广义Swish函数为 $\text{Swish}_\beta(z) = z\sigma(\beta z)$，因此GELU近似等于使用$\beta = 1.702$的广义Swish函数。

可以像调整任何其他超参数一样调整$\beta$。你也可以使$\beta$可以通过梯度下降对其进行优化：与PReLU非常相似，这可以使模型更强大，但它也存在过度拟合数据的风险。

另一个非常相似的激活函数是**Mish**，它是在2019年发表的论文中介绍的。它被定义为 $\text{Mish}(z) = z\tanh(\text{softplus}(z))$，其中 $\text{softplus}(z) = \log(1 + \exp(z))$。

就像GELU和Swish一样，它是ReLU的平滑、非凸和非单调变体。同样，作者进行了许多实验，发现Mish通常优于其他激活函数——即使是Swish和GELU，有微小的差距。

上图示了GELU、Swish（默认$\beta = 1$和$\beta = 0.6$），以及Mish激活函数。当$z$为负时，Mish与Swish几乎完美重叠，而当$z$为正时，与GELU几乎完美重叠。

**==应该为深度神经网络的隐藏层使用哪种激活函数呢？==**

- ReLU仍然是简单任务的良好默认选择：它通常与更复杂的激活函数一样好，而且计算速度非常快，许多库和硬件加速器提供特定于ReLU的优化方式。
- 但是，对于复杂的任务，Swish可能是更好的默认设置，
- 对于非常复杂的任务，甚至可以尝试使用具有可学习的β参数的参数化Swish。Mish可能会带来更好的结果，但它需要更多的计算量。
- 如果你非常关心运行时延迟，那可能更喜欢leaky ReLU，或者适合复杂任务的参数化leaky ReLU。
- 对于深度MLP，请尝试使用SELU，但请确保遵守前面列出的约束。
- 如果你有空闲时间和计算能力，也可以使用交叉验证来评估其他激活函数。

Keras开箱即用地支持GELU和Swish；只需使用activation="gelu"或activation="swish"。但是，它还不支持Mish或广义的Swish激活函数 （后续会学自定义激活函数和层）

```python
# 随堂练习：尝试使用gelu/swish / 参数化的leak relu(tf.keras.layers.PReLU)

# leaky_relu(z) = max(az, z)
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(50, kernel_initializer="he_normal"),  # no activation
    tf.keras.layers.PReLU(),  # 激活函数作为一个独立的层
    tf.keras.layers.Dense(50, kernel_initializer="he_normal"),
    tf.keras.layers.PReLU(),  # 激活函数作为一个独立的层
    tf.keras.layers.Dense(10, activation="softmax"),
])
model.summary()
model.compile(loss="sparse_categorical_crossentropy", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), metrics=["accuracy"])
model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))
```

### 批量归一化

尽管将 He 初始化与 ReLU（或其任何变体）一起使用可以显著降低在训练开始时出现梯度消失/爆炸问题的风险，但这并不能保证它们在训练期间不会再出现。

在2015年提出了一种称为**批量归一化（Batch Normalization，BN）** 的技术来解决这些问题。该技术包括在模型中每个隐藏层的激活函数之前或之后添加一个操作。该操作对每个输入进行零中心化并归一化，然后在每层使用两个新的参数向量缩放并偏移其结果：一个用于缩放，另一个用于偏移。换句话说，该操作可以使模型学习各层输入的最佳缩放和均值。在许多情况下，如果将 BN 层添加为神经网络的第一层，则无须归一化训练集（也就是说，不需要使用 `StandardScaler` 或 `Normalization`），BN 层会为你完成此操作（因为它一次只能查看一个批次，它还可以重新缩放和偏移每个输入特征）。

为了使输入零中心化并归一化，该算法需要估计每个输入的均值和标准差。我们通过评估当前小批次上的输入的均值和标准差（因此称为“批量归一化”）来实现。下面公式逐步总结了整个操作。

**公式：批量归一化算法**

1. $$\mu_B = \frac{1}{m_B} \sum_{i=1}^{m_B} \mathbf{x}^{(i)}$$
2. $$\sigma_B^2 = \frac{1}{m_B} \sum_{i=1}^{m_B} (\mathbf{x}^{(i)} - \mu_B)^2$$
3. $$\hat{\mathbf{x}}^{(i)} = \frac{\mathbf{x}^{(i)} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
4. $$\mathbf{z}^{(i)} = \boldsymbol{\gamma} \otimes \hat{\mathbf{x}}^{(i)} + \boldsymbol{\beta}$$

在此算法中：

- $\mu_B$ 是输入均值的向量，在整个小批次 $B$ 上评估（每个输入包含一个均值）。
- $m_B$ 是小批次中的实例数量。
- $\sigma_B$ 是输入标准差的向量，也在整个小批次上评估（每个输入包含一个标准差）。
- $\epsilon$ 是一个平滑项，用于避免除以零（通常取一个很小的值，例如 $10^{-5}$）。
- $\boldsymbol{\gamma}$ 是层的缩放参数向量（每个输入包含一个缩放参数）。
- $\boldsymbol{\beta}$ 是层的偏移参数向量（每个输入包含一个缩放参数    ）。
- $\otimes$ 表示逐元素乘法（每个输入乘以其相应的输出缩放参数）。
- $\mathbf{z}^{(i)}$ 是 BN 操作的输出，将传递给下一层或激活函数。



因此在训练期间，BN操作会归一化其输入，然后重新缩放并偏移它们。

那在测试期间，可能需要对单个实例而不是批量的实例进行预测：在这种情况下，无法计算每个输入的均值和标准差。而且，即使确实有一批实例，它也可能太小，或者这些实例可能不是独立同分布的，因此在这批实例上计算统计信息是不可靠的

一种解决方法是等到训练结束，然后通过神经网络运行整个训练集，计算BN层每个输入的均值和标准差。然后，在进行预测时，可以使用这些“最终”的输入均值和标准差，而不是批量输入的均值和标准差

然而，大多数批量归一化的实现都通过该层输入的均值和标准差的移动平均值来估计训练期间的最终统计信息。这是Keras在使用BatchNormalization层时自动执行的操作

综上所述，在每个批量归一化层中学习四个参数向量：通过常规反向传播学习γ（输出缩放向量）和β（输出偏移向量），使用指数移动平均值估计μ（最终输入均值向量）和σ（最终输入标准差向量）。请注意，μ和σ是在训练期间估算的，但仅在训练后使用（以替换公式中的批量输入均值和标准差）。

研究者已证明批量归一化极大地改善了他们实验过的所有深度神经网络，从而极大地提高了ImageNet分类任务的性能（ImageNet是将图像分类为许多类的大型图像数据库，通常用于评估计算机视觉系统）。梯度消失的问题已大大减少，以至于可以使用饱和的激活函数，例如tanh甚至sigmoid激活函数。网络对权重初始化也不太敏感。作者可以使用更大的学习率，大大加快了学习过程。他们特别指出：

"将批量归一化应用于最先进的图像分类模型，用原来14分之一的训练步骤即可达到相同的精度，在很大程度上击败了原始模型。......使用批量归一化网络的集成网络，我们在ImageNet分类中改进了已发布的最好结果：前5位的验证错误达到了4.9%（测试错误达到4.8%），超过了人工评分者的精度。"

```python
# 随堂练习：用python设计一个移动平均的操作，每次可以接收一个数组，然后根据数组，更新整体平均值

class MovingAverage:
    def __init__(self):
        self.count = 0
        self.average = None

    def __call__(self, new_batch):
        # new_average = (sum(new_batch)  + self.count*self.average) / (self.count + 批次的长度）
        #  self.count += 批次的长度
        # self.average = new_average
        # return self.average
        if self.average is None:
            self.average = sum(new_batch) / len(new_batch)
            self.count = len(new_batch)
            return self.average

        avg_batch = sum(new_batch) / len(new_batch)
        momentum = self.count / (self.count + len(new_batch))
        new_average = momentum * self.average + (1 - momentum) * avg_batch
        #   new_average = avg(new_batch) * (1-u) + u*当前平均值  （0<u<1)
        self.count += len(new_batch)
        self.average = new_average
        return new_average

move_avg = MovingAverage()
move_avg([1,2]) # -> 1.5
move_avg([3,4,5])# -> 3.0
move_avg([3,4,5])
```

最后，批量归一化的作用之一就是正则化，大大减少了对其他正则化技术（如 dropout，本章稍后将介绍）的需求。

但是，批量归一化确实增加了模型的复杂度（尽管它可以消除对输入数据进行归一化的需求，正如前面所讨论的）。此外，还有运行时间的损失：由于每一层都需要进行额外的计算，因此神经网络的预测速度较慢。幸运的是，我们经常可以在训练后将 BN 层与上一层融合，从而避免运行时间的损失。方法是更新前一层的权重和偏置项，以便它直接产生适当的缩放和偏移的输出。例如，如果前一层计算 $XW + b$，那么 BN 层将计算 $\boldsymbol{\gamma} \otimes (XW + b - \boldsymbol{\mu}) / \boldsymbol{\sigma} + \boldsymbol{\beta}$（忽略分母中的平滑项 $\epsilon$）。如果我们定义 $W' = \boldsymbol{\gamma} \otimes W / \boldsymbol{\sigma}$ 和 $b' = \boldsymbol{\gamma} \otimes (b - \boldsymbol{\mu}) / \boldsymbol{\sigma} + \boldsymbol{\beta}$，那么前一层可以直接计算 $XW' + b'$ 来得到与原始计算（$XW + b$ 后接 BN 层）相同的输出。 因此用更新后的W'和b'替换W和b就可以去掉BN层

可能会发现训练速度相当慢，因为使用批量归一化时每轮(epoch)要花费更多时间。通常情况下，这会被BN的快速收敛速度而抵消，因此达到相同性能所需的轮次更少，总而言之，所消耗的总时间通常会更短。

#### 用Keras实现批量归一化

与使用Keras的大多数任务操作一样，实施批量归一化非常简单，只需在每个隐藏层的激活函数之前或之后添加一个BatchNormalization层。也可以添加一个BN层作为模型的第一层，但是普通的Normalization层在这个位置通常也表现得非常好（它唯一的缺点是必须首先调用它的adapt()方法）。例如，此模型在每个隐藏层之后应用BN,并将其作为模型的第一层（展平层之后)

```python
import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(300, activation="relu", kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal"),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation="softmax")
])

# 只有两个隐藏层的示例中，批量归一化不会产生什么影响，但是对于更深层的网络，它可以产生巨大的影响
```

```python
model.summary()
```

每个 BN 层为每个输入添加了四个参数向量：γ、β、μ 和 σ
（例如，第一个 BN 层添加了 3136 个参数，即 4×784）。

最后两个参数向量 μ 和 σ 是移动平均值。它们不受反向传播的影响，因此 Keras 称其为“不可训练”参数。

如果你计算 BN 参数的总数：3136 + 1200 + 400，然后除以 2，则得到 2368，即此模型中不可训练参数的总数

注意：平均值和标准差是在训练期间根据训练数据估计的，因为可以说它们是可训练的。在Keras中，"不可训练"的真正意思是不受反向传播的影响

```python
# 第一个BN层的参数，两个是可训练的（受反向传播影响），两个不是
[(var.name, var.trainable) for var in model.layers[1].variables]
```

BN论文的作者主张在激活函数之前而不是之后（就像刚刚的代码）添加BN层。对此存在一些争论，因为哪个更可取似乎取决于任务——也可以对此进行实验，看看哪个选择最适合自己的数据集。要在激活函数之前添加BN层，必须从隐藏层中移除激活函数，并将它们作为单独的层添加到BN层之后。此外，由于批量归一化层的每个输入都包含一个偏移参数，因此你可以在创建它时传递use_bias=False，从上一层中删除偏置项。最后，通常可以删除第一个BN层以避免将第一个隐藏层夹在两个BN层之间，更新后的代码如下所示：

```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(300, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(100, kernel_initializer="he_normal", use_bias=False),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Activation("relu"),
    tf.keras.layers.Dense(10, activation="softmax")
])
model.summary()
```

```python
fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255
```

```python
# 展示批量归一化的效果
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])
model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))
```

BatchNormalization类有可以调整的超参数，比如可以调整动量（momentum）。

BatchNormalization 层在更新指数移动平均值时使用此超参数。
给定一个新值 $u$（即在当前批次中计算的输入均值或标准差的新向量），该层使用以下公式来更新运行平均值 $\hat{u}$：
$$
\hat{u} \leftarrow \hat{u} \times momentum + u \times (1 - momentum)
$$

良好的动量值通常接近 1，例如 0.9、0.99 或 0.999。
对于较大的数据集和较小的批处理，需要的 9 更多。

另一个重要的超参数是轴 (axis)：它确定哪个轴应该被归一化。默认值为 -1，这意味着默认情况下它将对最后一个轴进行归一化（使用在其他轴计算得到的均值和标准差）。

当输入批次为 2D（即批次形状为 [批次大小, 特征]）时，这意味着将基于在此批次中所有实例上计算得到的均值和标准差对每个输入特征进行归一化。
例如，先前代码示例中的第一个 BN 层将独立地归一化（重新缩放和偏移）784 个输入特征中的每一个。

如果将第一个 BN 层移动到 Flatten 层之前，则输入批次将为 3D，形状为 [批次大小, 高度, 宽度]，因此，BN 层将计算 28 个均值和 28 个标准差（每列像素 1 个，在批次的所有实例以及列中所有行之间计算），它将使用相同的均值和标准差对给定列的所有像素进行归一化。也将有 28 个缩放参数和 28 个偏移参数。

如果仍然要独立地处理 784 个像素中的每一个，则应设置 axis=[1, 2]。

批量归一化已经成为深度神经网络中常用的一层，尤其是后面学习的深度卷积神经网络（CNN），以至于在架构图中它经常被省略：假设已经在每一层之后添加了 BN。

### 梯度裁剪

最后一种在训练期间稳定梯度的技术：梯度裁剪，在反向传播过程中裁剪梯度，使它们永远不会超过某个阈值。这种技术通常用于循环神经网络（RNN），它很难使用批量归一化

在Keras中，实现梯度裁剪只是在创建优化器时设置clipvalue或clipnorm参数

```python
# optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)
optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer)
```

该优化器会将梯度向量的每个分量都裁剪为-1.0～1.0之间的值。这意味着损失函数的所有偏导数（相对于每个可训练的参数）将限制在-1.0～1.0之间。阈值是可以调整的超参数。

注意，它可能会改变梯度向量的方向。例如，如果原始梯度向量为[0.9，100.0]，则其主要指向第二个轴的方向，但是按值裁剪后，将得到[0.9，1.0]，这将大致指向两个轴之间的对角线。实际上，这种方法非常有效。

如果要确保“梯度裁剪”不更改梯度向量的方向，那么应该通过设置clipnorm而不是clipvalue来按照范数裁剪。如果ℓ2范数大于选择的阈值，则会裁剪整个梯度。例如，如果设置clipnorm=1.0，则向量[0.9，100.0]将被裁剪为[0.00899964，0.9999595]，它保留了方向，但几乎消除了第一个分量。

如果在训练期间观察到梯度爆炸（可以使用TensorBoard跟踪梯度的大小），可能想使用不同的阈值尝试按值裁剪或按范数裁剪，看看哪种在验证集上表现最好。

## 重用预训练层

如果不先尝试找到一个现有的神经网络——该神经网络可以完成与你要处理的任务类似的任务，就从头开始训练非常大的DNN，这通常不是一个好主意。如果能够找到这样的神经网络，那么通常可以重用它的大部分层，除了最上面的层。这种技术称为迁移学习。

它不仅会大大加快训练速度，而且需要的训练数据也会大大减少。假设你可以访问一个训练过的DNN，它能分类100种不同类别的图像，其中包括动物、植物、车辆和日常物品，现在想训练该DNN来对特定类型的车辆进行分类。这些任务非常相似，甚至有部分重叠，因此应该尝试重用第一个网络的一部分

![重用预训练层](./images/neural_network/p10.png)

如果新任务的输入图像的大小与原始任务中使用的图像不同，通常必须添加预处理步骤将其调整为原始模型所需的大小。一般而言，当输入具有类似的低级特征时，迁移学习最有效。

通常应该替换掉原始模型的输出层，因为它对于新任务而言很有可能根本没有用，甚至对于新任务而言，可能没有准确数量的输出。

类似地，原始模型上面的隐藏层不太可能像下面的那样有用，因为对新任务最有用的高级特征可能与对原始任务最有用的特征有很大的不同。需要确定要重用的具体层数。

任务越相似，可重用的层越多（从较低的层开始）。对于非常相似的任务，请尝试保留所有的隐藏层，只替换掉输出层。

首先尝试冻结所有可重用的层（即使它们的权重不可训练，以便梯度下降不会修改它们并且它们将保持固定），训练模型并查看其表现。然后尝试解冻上面隐藏层中的一两层，使反向传播可以对其进行调整，再查看性能是否有所提高。拥有的训练数据越多，可以解冻的层就越多。当解冻重用层时，降低学习率也很有用：这可以避免破坏其已经调整好的权重。

如果仍然无法获得良好的性能，并且训练数据很少，那么试着去掉顶部的隐藏层，然后再次冻结其余所有的隐藏层。不断迭代，直到找到合适的可以重用的层数。如果有大量的训练数据，则可以尝试替换顶部的隐藏层而不是去掉它们，甚至可以添加更多的隐藏层。

### 用Keras进行迁移学习

将Fashion MNIST训练集拆分为两部分：

- **X_train_A**：包含除T恤/上衣和套头衫（类别0和2）之外的所有物品图像。
- **X_train_B**：仅包含前200张T恤/上衣和套头衫图像的小型训练集。

验证集和测试集也按此方式划分，但不限制图像数量。

将在数据集A（8分类任务）上训练模型，并尝试将其复用至数据集B（二分类任务）。由于数据集A中的类别（裤子、连衣裙、外套、凉鞋、衬衫、运动鞋、包和短靴）与数据集B中的类别（T恤/上衣和套头衫）具有一定相似性，期望能够实现一定程度的知识迁移。但需要注意的是，由于使用的是全连接层，只有相同位置出现的模式才能被复用（相比之下，卷积层的迁移效果会好得多，因为学习到的模式可以在图像任意位置被检测到，将在卷积神经网络详细讨论）。

```python
import tensorflow as tf
fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255
```

```python
# 将Fashion MNIST分成 任务A和任务B，然后训练和 保存my_model_A
import numpy as np
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]
pos_class_id = class_names.index("Pullover")
neg_class_id = class_names.index("T-shirt/top")

def split_dataset(X, y):
    y_for_B = (y == pos_class_id) | (y == neg_class_id)
    y_A = y[~y_for_B]
    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)
    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))
    for old_class_id, new_class_id in zip(old_class_ids, range(8)):
        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A
    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))

(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)
(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)
(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)
X_train_B = X_train_B[:200]
y_train_B = y_train_B[:200]

tf.random.set_seed(42)

model_A = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(8, activation="softmax")
])

model_A.compile(loss="sparse_categorical_crossentropy",
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                metrics=["accuracy"])
history = model_A.fit(X_train_A, y_train_A, epochs=20,
                      validation_data=(X_valid_A, y_valid_A))
model_A.save("./models/my_model_A.keras")
```

```python
# 不用model_A, 从头训练和评估modelB
tf.random.set_seed(42)
model_B = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model_B.compile(loss="binary_crossentropy",
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                metrics=["accuracy"])
history = model_B.fit(X_train_B, y_train_B, epochs=20,
                      validation_data=(X_valid_B, y_valid_B))
model_B.evaluate(X_test_B, y_test_B)
```

现在试下复用预训练过的模型A

```python
model_A = tf.keras.models.load_model("./models/my_model_A.keras")
model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])
model_B_on_A.add(tf.keras.layers.Dense(1, activation="sigmoid"))

# 需要注意的是，由于 model_B_on_A 和 model_A 现在共享网络层，
# 因此当训练其中一个模型时，另一个模型的参数也会同步更新。若需避免此情况，我们需要基于 model_A 的克隆结构来构建 model_B_on_A。
```

```python
tf.random.set_seed(42)

model_A_clone = tf.keras.models.clone_model(model_A)
model_A_clone.set_weights(model_A.get_weights())  # tf.keras.models.clone_model()只复制架构，不复制权重。如果不使用set_weights手动复制它们# ，它们将在首次使用复制的模型时随机初始化
```

现在你可以为任务B训练model_B_on_A，但是由于新的输出层是随机初始化的，它会产生较大的错误（至少在前几个轮次内），因此将存在较大的错误梯度，这可能会破坏重用的权重。为了避免这种情况，一种方法是在前几个轮次时冻结重用的层，给新层一些时间来学习合理的权重。为此，将每一层的trainable属性设置为False并编译模型：

```python
for layer in model_B_on_A.layers[:-1]:
    layer.trainable = False

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)

# 冻结或者解冻层后，必须要编译模型
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])
```

将模型训练几个轮次，然后解冻重用的层（这需要再次编译模型），并继续进行训练以基于任务B来微调重用层。解冻重用层之后，降低学习率通常可以再次避免损坏重用权重：

```python
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,
                           validation_data=(X_valid_B, y_valid_B))

for layer in model_B_on_A.layers[:-1]:
    layer.trainable = True

optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)

# 冻结或者解冻层后，必须要编译模型
model_B_on_A.compile(loss="binary_crossentropy", optimizer=optimizer,
                     metrics=["accuracy"])
history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,
                           validation_data=(X_valid_B, y_valid_B))
```

```python
model_B_on_A.evaluate(X_test_B, y_test_B)
```

实践证明，迁移学习在小型密集网络中不能很好地工作，大概是因为小型网络能学习的模式很少，密集网络学习的是非常具体的模式，这在其他任务中不是很有用。迁移学习最适合用于深度卷积神经网络，后者倾向于学习更为通用的特征检测器（尤其是在较低层）。将在卷积神经网络中使用刚刚讨论的技术重新审视迁移学习

### 无监督预训练

假设要处理一个没有太多标签训练数据的复杂任务，但不幸的是，找不到已在类似任务上训练过的模型，如何解决？

首先，应该尝试收集更多带有标签的训练数据，但如果做不到，仍然可以执行无监督预训练（参见下图）。确实，收集未标记的训练实例通常成本很低，但标记它们却成本很高。

如果可以收集大量未标记的训练数据，则可以尝试使用它们来训练无监督模型，例如自动编码器或生成对抗网络（GAN）。

然后，可以重用自动编码器的较低层或GAN判别器的较低层，在顶部添加针对自己的任务的输出层，并使用监督学习（即使用带有标签的训练实例）来微调最终的网络。

在监督训练中，使用无监督学习技术基于所有数据（包括未标记数据）训练模型，然后使用监督学习技术基于带标签的数据针对最终任务进行微调；无监督部分可以一次训练一层，也可以直接训练整个模型：

<img alt="无监督预训练" height="500" src="./images/neural_network/p11.png" width="500"/>

总之，解决的任务复杂，没有可重用的相似模型，带标签的训练数据很少，但是无标签的训练数据很多时，无监督预训练（GAN/自动编码器）是个不错选择

### 基于辅助任务的预训练

对于缺乏带标签训练数据的情况，另一种有效方法是在辅助任务上训练初始神经网络（该任务的标记数据易于获取或生成），然后针对实际任务重用该网络的较低层。较低层将学习到通用的特征检测器，第二个网络可复用这些特征检测器以提升性能。

实例：人脸识别系统
- **问题**：每人仅有的几张照片不足以训练高质量分类器，而收集每人数百张图片又不切实际。
- **解决方案**：
  1. 从网络收集大量随机人物图像。
  2. 训练初始神经网络判断两张图片是否属于同一人。
  3. 该网络将学习有效的人脸特征检测器。
  4. 复用其较低层，即可用极少数据训练出高性能人脸分类器。

实例：自然语言处理（NLP）中的自监督学习

在NLP领域，可从包含数百万文档的语料库中自动生成带标签数据。例如：

- **文本屏蔽任务**：随机遮蔽部分单词，训练模型预测缺失内容。
  示例：句子“What \_\_ you saying？”中，模型应预测缺失词可能为“are”或“were”。
- **优势**：模型在此任务上达到良好性能后，已对语言有深入理解，可迁移至实际任务并通过带标签数据微调（迁移学习/预训练）。

自监督学习的定义

自监督学习指从数据本身自动生成标签（如文本屏蔽示例），随后使用监督学习技术在生成“带标签”数据集上训练模型的方法。



## 更快的优化器

训练一个非常大的深度神经网络可能会非常缓慢。目前接触过4种加快训练速度的方法：对连接权重应用良好的初始化策略，使用良好的激活函数，使用批量归一化，以及重用建立在辅助任务或无监督学习获得的预训练网络。

现在提出另一种加速训练的方式，对梯度下降做改进：优化器。将介绍流行的优化算法：动量优化，`Nesterov`加速梯度，`AdaGrad`，`RMSProp`，以及`Adam`及其变体

### 动量优化

想象一个球从斜坡上滚下，刚开始速度很慢，但速度会越来越快，直到达到最终速度（假设有摩擦力+空气阻力，速度不能无限增长），这就是动量优化背后的核心思想

常规梯度下降法会在坡度平缓时采取小步，而在坡度大时采取大步，但永远不会关心之前的梯度是什么，永远不会加快速度，如果局部梯度很小，则它会下降得非常缓慢

动量优化把梯度看成力的方向/加速度的方向，会累积之前的梯度：在每次迭代时，都会从动量向量（速度）m 减去 学习率乘以局部梯度（加速度），并通过加上该动量来更新权重。

为了模拟摩擦力，防止动量（速度）变得过大，该算法引入了一个新的超参数β，0代表高摩擦，退化为之前的梯度下降，1表示没有摩擦，β会设置为0-1之间的某个值，典型的β值是0.9

动量优化公式：
$$
\begin{aligned}
& m \leftarrow \beta m - \eta \nabla_\theta J(\theta) \\
& \theta \leftarrow \theta + m
\end{aligned}
$$
可以轻松验证，如果梯度保持恒定，则最终速度（即权重更新的最大大小）等于该梯度乘以学习率 $\eta$ 再乘以 $1/(1-\beta)$（忽略符号）。例如，如果 $\beta=0.9$，则最终速度等于梯度乘以学习率的10倍，因此动量优化最终比梯度下降快10倍！这使动量优化比梯度下降要更快地从"平台"逃脱。

在讨论线性回归的梯度下降时，当输入的尺寸差别非常巨大，代价函数将看起来像一个拉长的碗。梯度下降相当快地沿着陡峭的"斜坡"下降，但是沿着"山谷"下降需要很长时间。相反，动量优化将沿着"山谷"滚动得越来越快，直到达到"谷底"（最优解）。在不使用批量归一化的深度神经网络中，上面的层通常会得到尺寸差别较大的输入，因此使用动量优化会有所帮助。它还可以帮助绕过局部优化问题。

由于这种动量势头，优化器可能会稍微过调，然后又回来，再次过调，在稳定于最小点之前会多次振荡。这是系统中应该有一些摩擦力的原因之一：它消除了这些振荡，从而加快了收敛速度。



一、动量优化核心公式

动量优化通过**累积历史梯度的 “动量”** 来更新参数，同时用超参数 β 模拟摩擦力限制动量大小。假设我们优化的参数为权重矩阵**W**（或偏置**b**，公式完全一致），核心公式分为两步：**更新动量向量** 和 **更新参数**。

1. 符号定义

在给出公式前，先明确所有符号的含义，避免混淆：

| 符号   | 含义说明                                                     |
| ------ | ------------------------------------------------------------ |
| **W**  | 待优化的参数（如神经网络的权重矩阵，为向量 / 矩阵形式，此处统一用粗体表示） |
| **gₜ** | 第 t 次迭代时，参数**W**的**局部梯度**（即损失函数对**W**的偏导数在当前位置的值）即$ \nabla_\theta J(\theta)$ |
| **mₜ** | 第 t 次迭代时的**动量向量**（类比物理中的 “速度”，累积了历史梯度的方向和大小） |
| β      | 动量超参数（模拟摩擦力，取值范围 [0,1)，典型值 0.9）         |
| α      | 学习率（控制每次参数更新的 “步长”，与常规梯度下降一致）      |

2. 迭代公式

动量优化的迭代过程分为两步，从初始状态开始逐步更新：

（1）初始化

在第 1 次迭代前，需要初始化动量向量（通常设为 0，即初始 “速度” 为 0）： **m₀** = 0 （动量向量初始值为零向量）

（2）第 t 次迭代（t ≥ 1）

① **更新动量向量** 动量向量 = 历史动量的 “惯性” + 当前梯度产生的 “加速度” （β×**mₜ₋₁** 代表历史动量的保留，-α×**gₜ** 代表当前梯度对动量的贡献，负号因为梯度方向是 “上升” 方向，我们需要 “下降”） \($\boldsymbol{m}_t = \beta \cdot \boldsymbol{m}_{t-1} - \alpha \cdot \boldsymbol{g}_t$\)

② **更新参数** 参数 = 当前参数 + 动量向量（动量直接决定参数移动的方向和距离） \($\boldsymbol{W}_t = \boldsymbol{W}_{t-1} + \boldsymbol{m}_t$)

二、公式的物理意义解读

你提到的 “球从斜坡滚下” 的类比非常贴切，我们结合公式再深入对应：

| 物理概念     | 动量优化中的对应     | 公式中的体现                                                 |
| ------------ | -------------------- | ------------------------------------------------------------ |
| **斜坡坡度** | 局部梯度**gₜ**的大小 | 梯度越大，对动量的贡献（-α×**gₜ**）越强                      |
| **球的速度** | 动量向量**mₜ**       | 速度由历史惯性（β×**mₜ₋₁**）和当前加速度决定                 |
| **摩擦力**   | 超参数 β             | β 越小（摩擦越大），历史动量的保留越少；β=0 时，动量为 0，退化为**常规梯度下降**（$\boldsymbol{W}_t = \boldsymbol{W}_{t-1} - \alpha \cdot \boldsymbol{g}_t$） |
| **球的位置** | 参数**Wₜ**           | 位置随速度（动量）的方向和大小更新                           |

三、关键超参数 β 的作用详解

β 是动量优化的核心超参数，其取值直接影响算法的性能：

- **β = 0**：完全没有动量，公式退化为 ** vanilla gradient descent（常规梯度下降）**，只依赖当前梯度更新，无法加速。
- **β = 0.9**（典型值）：表示每次迭代保留 90% 的历史动量，仅加入 10% 的当前梯度贡献。这意味着动量会 “累积” 前 10 次左右的梯度方向（因为\($(0.9)^{10} ≈ 0.35$\)，超过 10 次的历史影响会减弱），非常适合平滑梯度、加速收敛。
- **β → 1**：摩擦极小，动量会不断累积甚至 “失控”（类似球在无摩擦斜坡上无限加速），可能导致参数更新过大，跳过最优解。

四、与常规梯度下降的对比（优势）

通过公式和思想的对比，可以清晰看到动量优化的核心优势：

| 对比维度     | 常规梯度下降                   | 动量优化                                     |
| ------------ | ------------------------------ | -------------------------------------------- |
| **梯度利用** | 仅使用当前梯度**gₜ**           | 累积历史梯度的动量**mₜ**，利用全局趋势       |
| **收敛速度** | 坡度平缓时（梯度小）下降极慢   | 动量累积后，即使局部梯度小，仍能保持较快速度 |
| **震荡抑制** | 在鞍点或局部起伏处易来回震荡   | 动量的 “惯性” 会平滑震荡，帮助跳出局部波动   |
| **核心类比** | 盲人摸路，每一步只看脚下的坡度 | 球滚下坡，结合历史惯性和当前坡度加速前进     |



```python
# 在Keras中实现动量优化：只需要使用SGD优化器并设置其momentum超参数即可
optimizer_momentum = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
# 动量优化的一个缺点是它增加了另一个需要调整的超参数。但是，动量值取0.9通常在实践中效果很好，几乎总是比常规的梯度下降法更快。
```

```python
def build_model(seed=42):
    tf.random.set_seed(seed)
    return tf.keras.Sequential([
        tf.keras.layers.Flatten(input_shape=[28, 28]),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(100, activation="relu",
                              kernel_initializer="he_normal"),
        tf.keras.layers.Dense(10, activation="softmax")
    ])

def build_and_train_model(optimizer):
    model = build_model()
    model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
                  metrics=["accuracy"])
    return model.fit(X_train, y_train, epochs=5,
                     validation_data=(X_valid, y_valid))
```

```python
optimizer_zero_momentum = tf.keras.optimizers.SGD(learning_rate=0.001)
```

```python
history_sgd = build_and_train_model(optimizer_zero_momentum)
```

```python
history_momentum = build_and_train_model(optimizer_momentum)
```

### Nesterov加速梯度

公式： Nesterov加速梯度算法
$$
\begin{aligned}
& m \leftarrow \beta m - \eta \nabla_\theta J(\theta + \beta m) \\
& \theta \leftarrow \theta + m
\end{aligned}
$$

它是动量优化的一个小变体，会比常规动量优化快。Nesterov加速梯度(NAG)方法也称为Nesterov动量优化，它不是在局部位置 $\theta$，而是在动量方向稍前方 $\theta+\beta m$ 处测量代价函数的梯度。

这种小的调整之所以有效是因为通常动量向量会指向正确的方向（即朝向最优值），因此使用在该方向上更远处而不是原始位置测得的梯度会稍微准确一些，如下图所示（其中 $\nabla_1$ 代表在起点 $\theta$ 处测量的代价函数的梯度， $\nabla_2$ 代表 $\theta+\beta m$ 点的梯度）。

![常规动量优化与Nesterov动量优化](./images/neural_network/p12.png)

Nesterov动量优化更新最终稍微接近最优解。一段时间后，这些小的改进累积起来，NAG就比常规的动量优化要快得多。此外，请注意，当动量势头推动权重跨越谷底时，∇1继续推动越过谷底，而∇2则推回谷底。这有助于减少振荡，因此NAG收敛速度更快。

```python
# 要使用NAG，只需在创建SGD优化器时设置nesterov=True即可
optimizer_nesterov = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)
```

```python
history_nesterov = build_and_train_model(optimizer_nesterov)
```

### AdaGrad

公式：AdaGrad算法
$$
\begin{aligned}
& 1.\quad s \leftarrow s + \nabla_{\theta} J(\theta) \otimes \nabla_{\theta} J(\theta) \\
& 2.\quad \theta \leftarrow \theta - \eta \nabla_{\theta} J(\theta) \oslash \sqrt{s + \epsilon}
\end{aligned}
$$

再次考虑拉长的碗状问题：梯度下降沿最陡的坡度快速开始下降，该坡度没有直接指向全局最优值，然后非常缓慢地下降到谷底。如果算法可以更早地纠正其方向，使它更多地指向全局最优值，那将更好。AdaGrad算法通过沿最陡峭的维度按比例缩小梯度向量来实现此校正。

第一步先将梯度的平方累加到向量 $s$ 中（$\otimes$ 符号表示逐元素相乘）。此向量化形式等效于针对向量 $s$ 中的每个元素 $s_i$ 计算 $s_i \leftarrow s_i + (\partial J(\theta)/\partial \theta_i)^2$。换句话说，每个 $s_i$ 累加代价函数关于参数 $\theta_i$ 的偏导数的平方。如果代价函数沿第 $i$ 个维度陡峭，则 $s_i$ 将在每次迭代中变得越来越大。

第二步几乎与梯度下降相同，但有一个很大的区别：梯度向量按比例因子 $\sqrt{s + \epsilon}$ 缩小了（$\oslash$ 符号代表逐元素相除，$\epsilon$ 是避免除以零的平滑项，通常设置为 $10^{-10}$）。此向量化形式等效于对所有参数 $\theta_i$ 同时计算 $\theta_i \leftarrow \theta_i - \eta \partial J(\theta)/\partial \theta_i / \sqrt{s_i + \epsilon}$。

简而言之，该算法会降低学习率，但是对于陡峭的维度，学习率降低速度比缓慢下降的维度的降低速度更快。这称为自适应学习率。它有助于将结果更新更直接地指向全局最优值。另一个好处是，它几乎不需要调整学习率超参数 $\eta$。

![AdaGrad vs 梯度下降](./images/neural_network/p13.png)

对于简单的二次问题，AdaGrad经常表现良好，但是在训练神经网络时，它往往停止得太早：学习率被按比例缩小，以至于算法在最终达到全局最优值之前就完全停止了。因此，即使Keras有Adagrad优化器，也不应使用它来训练深度神经网络（不过，它对于诸如线性回归之类的简单任务可能是有效的）。尽管如此，了解AdaGrad仍有助于理解其他自适应学习率优化器。

一、AdaGrad 核心公式

AdaGrad 通过**累积每个参数的历史梯度平方**来动态调整学习率，解决了常规梯度下降中 “单一学习率难以适配所有参数” 的问题。以下是完整公式、符号定义及迭代过程。

1. 符号定义

| 符号   | 含义说明                                                     |
| ------ | ------------------------------------------------------------ |
| **W**  | 待优化的参数（如权重、偏置，为向量 / 矩阵形式，粗体表示）    |
| **gₜ** | 第 t 次迭代时，参数**W**的**局部梯度**（损失函数对**W**的偏导数） |
| **Gₜ** | 第 t 次迭代时，**参数梯度平方的累积矩阵**（与**W**维度相同，记录每个参数的历史梯度平方和） |
| α      | 全局初始学习率（需要手动设置，与常规梯度下降的学习率作用不同，更偏向 “基准系数”） |
| ε      | 平滑项（通常取 1e-8，防止分母为 0，不影响梯度累积的趋势）    |
| ⊙      | 元素级乘法（Hadamard 乘积，对应位置元素相乘）                |
| √(·)   | 元素级开平方（对应位置元素单独开平方）                       |

2. 迭代公式

AdaGrad 的迭代过程分为**累积梯度平方**和**更新参数**两步，核心是为每个参数计算 “自适应学习率”。

（1）初始化

第 1 次迭代前，梯度平方累积矩阵初始化为 0（表示无历史梯度）： **G₀** = 0 （与**W**同维度的零矩阵 / 零向量）

（2）第 t 次迭代（t ≥ 1）

① **累积参数的梯度平方** 每次迭代都将当前梯度的平方 “累加到” 历史累积矩阵中（元素级累加），记录每个参数的梯度波动幅度： $\boldsymbol{G}_t = \boldsymbol{G}_{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t$

② **更新参数** 用**全局初始学习率 α**除以**累积梯度平方的平方根（加平滑项 ε）**，得到每个参数的**自适应学习率**；再用自适应学习率乘以当前梯度，更新参数：$ \boldsymbol{W}_t = \boldsymbol{W}_{t-1} - \frac{\alpha}{\sqrt{\boldsymbol{G}_t} + \varepsilon} \odot \boldsymbol{g}_t$

二、核心思想：“谁频繁更新，谁的学习率就变小”

AdaGrad 的本质是**为每个参数维护一个 “学习率衰减计数器”（即 Gₜ）**，其逻辑可以通过一个直观例子理解：

假设我们在训练词向量，模型中有两个参数：

- **参数 A**：对应高频词（如 “的”“是”），每次迭代都会被更新，梯度累积平方**Gₜ**会快速变大 → 自适应学习率$\frac{\alpha}{\sqrt{G_t}+\varepsilon} $变小 → 避免参数更新幅度过大，稳定收敛。
- **参数 B**：对应低频词（如 “量子”“微积分”），很少被更新，梯度累积平方**Gₜ**很小 → 自适应学习率较大 → 即使梯度小，也能通过大学习率快速调整，避免收敛过慢。

这种 “按需分配” 学习率的机制，正是 AdaGrad 适配稀疏数据的关键。

三、与常规梯度下降、动量优化的核心区别

为了更清晰地理解 AdaGrad 的定位，我们将其与前两种优化算法对比：

| 对比维度           | 常规梯度下降（Vanilla GD） | 动量优化（Momentum）       | AdaGrad                        |
| ------------------ | -------------------------- | -------------------------- | ------------------------------ |
| **学习率特性**     | 单一学习率，所有参数共享   | 单一学习率，所有参数共享   | **自适应学习率，参数专属**     |
| **核心改进**       | 无                         | 累积历史梯度方向（动量）   | 累积历史梯度幅度（调整学习率） |
| **解决的问题**     | 基础下降框架               | 加速收敛、抑制震荡         | 适配稀疏数据、参数差异化更新   |
| **依赖的历史信息** | 无                         | 历史梯度的**方向**（mₜ）   | 历史梯度的**平方幅度**（Gₜ）   |
| **适用场景**       | 简单模型、调试基准         | 一般深度学习模型（如 CNN） | 稀疏数据（如 NLP、推荐系统）   |

四、关键超参数与局限性

1. 超参数解读

- **α（初始学习率）**：AdaGrad 的 “基准”，但不再直接决定更新步长（步长由 α 和 Gₜ共同决定）。通常需要比常规梯度下降的 α 设置得更大一些（如 0.01、0.1），因为后续会被$\sqrt{G_t}$衰减。
- **ε（平滑项）**：纯技术细节，防止$\sqrt{G_t}$为 0 导致分母无穷大，取值固定为 1e-8 或 1e-10，一般无需调整。

2. 主要局限性：学习率单调衰减

AdaGrad 的核心缺陷在于**梯度平方累积是 “只增不减” 的**：随着迭代次数增加，\(\sqrt{G_t}\)会持续变大，导致自适应学习率$\frac{\alpha}{\sqrt{G_t}+\varepsilon}$ **单调递减至接近 0**，最终可能使参数更新停滞（“早停”），无法收敛到最优解。

例如：在训练复杂模型（如深度 Transformer）时，迭代 10000 次后，Gₜ可能变得极大，学习率几乎为 0，参数不再更新，模型性能无法继续提升。



```python
optimizer_adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.001)
```

```python
history_adagrad =  build_and_train_model(optimizer_adagrad)
```

### RMSProp

公式：RMSProp算法
$$
\begin{aligned}
& 1.\quad s \leftarrow \rho s + (1 - \rho) \nabla_{\theta} J(\theta) \otimes \nabla_{\theta} J(\theta) \\
& 2.\quad \theta \leftarrow \theta - \eta \nabla_{\theta} J(\theta) \oslash \sqrt{s + \varepsilon}
\end{aligned}
$$

正如我们所看到的，AdaGrad有下降太快，永远不会收敛到全局最优值的风险。RMSProp算法通过只累加最近迭代中的梯度，而不是自训练开始以来的所有梯度，来解决这个问题。它通过在第一步中使用指数衰减来实现。

衰减率 $\rho$ 通常设置为0.9。它又是一个新的超参数，但是此默认值通常效果很好，因此你可能根本不需要调整它。

除非常简单的问题外，该优化器几乎总是比AdaGrad表现更好。实际上，直到Adam优化出现之前，它一直是许多研究人员首选的优化算法。

```python
optimizer_rmsprop = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)
```

```python
history_rmsprop = build_and_train_model(optimizer_rmsprop)
```

### Adam

Adam（adaptive moment estimation) 代表自适应矩估计，结合了动量优化和RMSProp的思想：像动量优化一样，它跟踪过去梯度的指数衰减平均值，就像RMSProp一样，它跟踪过去平方梯度的指数衰减平均值，相当于对梯度的均值和方差（不减平均值）估计

公式：Adam算法
$$
\begin{aligned}
&1.\quad m \leftarrow \beta_1 m + (1 - \beta_1) \nabla_{\theta} J(\theta) \\
&2.\quad s \leftarrow \beta_2 s + (1 - \beta_2) \nabla_{\theta} J(\theta) \otimes \nabla_{\theta} J(\theta) \\
&3.\quad \hat{m} = \frac{m}{1 - \beta_1^t} \\
&4.\quad \hat{s} = \frac{s}{1 - \beta_2^t} \\
&5.\quad \theta \leftarrow \theta - \eta \frac{\hat{m}}{\sqrt{\hat{s}+ \epsilon}}
\end{aligned}
$$

式中，$t$ 表示迭代次数（从1开始）。

如果只看步骤1、2和5，会注意到Adam与动量优化和RMSProp非常相似：$\beta_1$ 对应于动量优化中的 $\beta$，$\beta_2$ 对应于RMSProp中的 $\rho$。唯一的区别是步骤1计算的是指数衰减平均值，而不是指数衰减总和，但除了常数因子（衰减平均值是衰减总和的 $1-\beta_1$ 倍），Adam的第一步和动量优化的第一步实际上是等效的。第3步和第4步是技术上的细节：由于 $m$ 和 $s$ 初始化为0，因此在训练开始时它们会偏向0，这两个步骤将有助于在训练开始时提高 $m$ 和 $s$。

动量衰减超参数 $\beta_1$ 通常被初始化为0.9，而缩放衰减超参数 $\beta_2$ 通常被初始化为0.999。如前所述，平滑项 $\epsilon$ 通常被初始化为一个很小的数字，例如 $10^{-7}$。这是Adam类的默认值。

```python
# Keras创建Adam优化器的方法：
optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
```

由于Adam是一种自适应学习率算法，像AdaGrad和RMSProp一样，因此几乎很少需要对学习率超参数η进行调整。通常可以使用默认值η=0.001，这使得Adam甚至比梯度下降更易于使用。

```python
history_adam = build_and_train_model(optimizer_adam)
```

最后三个优化器：Adam的3个变体：AdaMax，Nadam和AdamW

### AdaMax

公式：AdaMax优化
$$
\begin{aligned}
&1.\quad m \leftarrow \beta_1 m + (1 - \beta_1) \nabla_{\theta} J(\theta) \\
&2.\quad s \leftarrow \max(\beta_2 s, \left| \nabla_{\theta} J(\theta) \right|) \\
&3.\quad \hat{m} = \frac{m}{1 - \beta_1^t} \\
&4.\quad \theta \leftarrow \theta - \eta \frac{\hat{m}}{s + \epsilon}
\end{aligned}
$$

Adam论文还介绍了AdaMax。请注意，在Adam的步骤2中，累加了 $s$ 中的梯度平方（对于最近的梯度，权重更大）。在第5步中，Adam将以 $s$ 的平方根按比例缩小参数更新项。简而言之，Adam以时间衰减梯度的 $\ell_2$ 范数按比例缩小参数更新项（$\ell_2$ 范数是平方和的平方根）。

AdaMax将 $\ell_2$ 范数替换为 $\ell_\infty$ 范数（一种表达最大值的方式）。具体来说，它用 $s \leftarrow \max(\beta_2 s, \left| \nabla_{\theta} J(\theta) \right|)$ 替换Adam中的步骤2，删除偏向0校正步骤（对应Adam的第4步），在第5步中，将梯度更新项按比例缩小，这是时间衰减梯度的绝对值的最大值。

实际上，这可以使AdaMax比Adam更稳定，但这取决于数据集，通常Adam的表现更好。因此，如果在某些任务中使用Adam遇到问题，那么这是可以尝试使用的另一种优化器。

### Nadam

Nadam优化是Adam优化加上Nesterov技巧，理论上收敛得比Adam稍微快一些

### AdamW

它集成了一种称为权重衰减的正则化技术。权重衰减通过将模型权重乘以衰减因子（例如0.99）来在每次训练迭代中减小权重。这可能会联想到l2正则化，

它也旨在保持较小的权重，并且确实可以从数学上证明l2正则化等同于使用SGD时的权重衰减。然而，当使用Adam或其变体时，l2正则化和权重衰减并不等效：在实践中，将Adam与l2正则化相结合会导致模型的泛化能力不如SGD生成的模型。AdamW通过适当地将Adam与权重衰减结合起来解决了这个问题。

```python
# 在keras种使用Nadam，AdaMax 和 AdamW
optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9,
                                      beta_2=0.999)
history_nadam = build_and_train_model(optimizer)
```

```python
optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9,
                                       beta_2=0.999)
history_adamax = build_and_train_model(optimizer)
```

```python
optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-5, learning_rate=0.001,
                                      beta_1=0.9, beta_2=0.999)
history_adamw = build_and_train_model(optimizer)
```

```python
import matplotlib.pyplot as plt
for loss in ("loss", "val_loss"):
    plt.figure(figsize=(12, 8))
    opt_names = "SGD Momentum Nesterov AdaGrad RMSProp Adam Adamax Nadam AdamW"
    for history, opt_name in zip((history_sgd, history_momentum, history_nesterov,
                                  history_adagrad, history_rmsprop, history_adam,
                                  history_adamax, history_nadam, history_adamw),
                                 opt_names.split()):
        plt.plot(history.history[loss], label=f"{opt_name}", linewidth=3)

    plt.grid()
    plt.xlabel("Epochs")
    plt.ylabel({"loss": "Training loss", "val_loss": "Validation loss"}[loss])
    plt.legend(loc="upper left")
    plt.axis([0, 9, 0.1, 0.7])
    plt.show()
```

自适应优化方法（包括RMSProp、Adam、AdaMax、Nadam和AdamW优化）通常非常好，可以快速收敛到一个好的解。

然而，有研究表明它们可以导致在某些数据集上泛化不佳的解。因此，当你对模型的性能感到失望时，请尝试改用动量优化/NAG：数据集可能对自适应梯度“过敏”。

表：优化器比较

| 优化器类别                       | 收敛速度 | 收敛质量      |
| :------------------------------- | :------- | :------------ |
| SGD                              | *        | ***           |
| SGD(momentum=...)                | **       | ***           |
| SGD(momentum=..., nesterov=True) | **       | ***           |
| Adagrad                          | **       | *（停止太早） |
| RMSprop                          | **       | ** 或者 ***   |
| Adam                             | **       | ** 或者 ***   |
| AdaMax                           | **       | ** 或者 ***   |
| Nadam                            | **       | ** 或者 ***   |
| AdamW                            | **       | ** 或者 ***   |

### 训练稀疏模型

刚刚讨论的所有优化算法都会产生密集模型，这意味着大多数参数都是非零的。如果在运行时需要一个非常快的模型，或者需要占用更少内存的模型，那么可能更喜欢使用一个稀疏模型。实现这一点的一个方法是像往常一样训练模型，然后去掉很小的权重（将它们设置为零）。但是，这通常不会导致非常稀疏的模型，而且可能会降低模型的性能。

一个更好的选择是在训练时使用强l1正则化，因为它会迫使优化器产生尽可能多的为零的权重。

如果这些技术仍然不够，请查看TensorFlow Model Optimization Toolkit (TF-MOT)，它提供了一个剪枝API，能够根据连接的大小在训练期间迭代地删除连接。

## 学习率调度

找到一个好的学习率非常重要。如果将学习率设置得太大，训练可能会发散；如果学习率设置得太小，训练会花费很长时间。如果学习率设置得稍大了点，它一开始会很快，但是最终会围绕最优解振荡，不会真正稳定下来

可以对模型进行数百次迭代训练，然后将学习率从很小的值成指数形式地增加到很大的值，再查看学习曲线并选择一个略低于学习曲线开始回升的学习率，然后，可以重新初始化模型，并以该学习率对其进行训练。

但是有比恒定学习率更好的做法：比如先采用一个较大的学习率，然后在训练不再取得进展后就降低它；比如先采用小的学习率再增加它，然后再降低，这些策略称为学习率调度，在介绍随机梯度下降的时候首次提到过它

$t$表示以已经经过的步数，$s$表示学习率下降需要经过的步数，这个步数在`staircase=True`时更明显，他会在步数$t$达到$s$时使学习率骤然下降



### 幂调度 (Power Scheduling)

将学习率设置为迭代次数的函数：

$$
\eta(t) = \frac{\eta_0}{(1 + t/s)^c}
$$


其中初始学习率 $\eta_0$、幂 $c$（通常设置为 1）和步骤 $s$ 是超参数。学习率在每一步都会下降。在 $s$ 个步骤之后，学习率下降到 $\eta_0/2$。再在 $s$ 个步骤之后，它下降到 $\eta_0/3$，然后下降到 $\eta_0/4$，然后是 $\eta_0/5$，以此类推。

如你所见，此调度下学习率开始迅速下降，然后越来越慢。当然，幂调度需要调整 $\eta_0$ 和 $s$（可能还有 $c$）。

```python
# learning_rate = initial_learning_rate / (1 + step / decay_steps)**power, step: 更新的次数，一个批次一次更新
# Keras 使用 power = 1

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate=0.01,
    decay_steps=10_000,
    decay_rate=1.0,
    staircase=False #  这个设置为True的话， floor(step / decay_step) 替代 step / decay_step
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
```

```python
history_power_scheduling = build_and_train_model(optimizer)
```

```python
import numpy as np
import matplotlib.pyplot as plt

initial_learning_rate = 0.01
decay_rate = 1.0
decay_steps = 10_000

steps = np.arange(100_000)
lrs = initial_learning_rate / (1 + decay_rate * steps / decay_steps)
lrs2 = initial_learning_rate / (1 + decay_rate * np.floor(steps / decay_steps))

plt.plot(steps, lrs,  "-", label="staircase=False")
plt.plot(steps, lrs2,  "-", label="staircase=True")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("Step")
plt.ylabel("Learning Rate")
plt.title("Power Scheduling", fontsize=14)
plt.legend()
plt.grid(True)
plt.show()
```

### 指数调度 (Exponential Scheduling)

将学习率设置为：

$$
\eta(t) = \eta_0 \cdot 0.1^{t/s}
$$


学习率以每 $s$ 步的速率逐渐变为原来的十分之一。幂调度降低学习率的速度越来越缓慢，而指数调度则使学习率每 $s$ 步变为原来的十分之一。

```python
# learning_rate = initial_learning_rate * decay_rate ** (step / decay_steps)
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=20_000,
    decay_rate=0.1,
    staircase=False
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
```

```python
history_exponential_scheduling = build_and_train_model(optimizer)
```

```python
initial_learning_rate = 0.01
decay_rate = 0.1
decay_steps = 20_000

steps = np.arange(100_000)
lrs = initial_learning_rate * decay_rate ** (steps / decay_steps)
lrs2 = initial_learning_rate * decay_rate ** np.floor(steps / decay_steps)

plt.plot(steps, lrs,  "-", label="staircase=False")
plt.plot(steps, lrs2,  "-", label="staircase=True")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("Step")
plt.ylabel("Learning Rate")
plt.title("Exponential Scheduling", fontsize=14)
plt.legend()
plt.grid(True)
plt.show()
```

```python
# keras提供LearningRateScheduler的回调类，让你定义自己的调度函数

def exponential_decay_fn(epoch):
    return 0.01 * 0.1 ** (epoch / 20)

# 如果想配置初始学习率 和 步数，可以创建一个返回配置函数的函数
def exponential_decay(lr0, s):
    def exponential_decay_fn(epoch):
        return lr0 * 0.1 ** (epoch / s)
    return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
```

```python
# 接下来创建一个LearningRateScheduler回调函数，为其提供调度函数，然后将此回调函数传给fit方法
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)
history = model.fit(X_train, y_train, epochs=20,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])
```

LearningRateScheduler将在每个轮次开始时更新优化器的learning_rate属性，通常一个轮次更新一次学习率足够了，如果希望更频繁地更新学习率，则需要编写自己的回调函数类

```python
class ExponentialDecay(tf.keras.callbacks.Callback):
    def __init__(self, n_steps=40_000):
        super().__init__()
        self.n_steps = n_steps

    def on_batch_begin(self, batch, logs=None):
        lr = self.model.optimizer.learning_rate.numpy()
        new_learning_rate = lr * 0.1 ** (1 / self.n_steps)
        self.model.optimizer.learning_rate = new_learning_rate

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = self.model.optimizer.learning_rate.numpy()
```

```python
lr0 = 0.01
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
```

```python
import math

batch_size = 32
n_epochs = 25

n_steps = n_epochs * math.ceil(len(X_train) / batch_size)
exp_decay = ExponentialDecay(n_steps)
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[exp_decay])
```

### 分段恒定调度(Piecewise Constant Scheduling)

对一些轮次使用恒定的学习率（例如，对某5个轮次，使用η0=0.1），对另外一些轮次使用较小的学习率（例如，对某50个轮次，使用η0=0.001），以此类推。尽管这个方法可以很好地工作，但仍需要仔细研究以找出正确的学习率顺序以及使用它们的轮次。

```python
lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries=[50_000, 80_000],
    values=[0.01, 0.005, 0.001]
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)
```

```python
history_piecewise_scheduling = build_and_train_model(optimizer)
```

```python
boundaries = [50_000, 80_000]
values = [0.01, 0.005, 0.001]

steps = np.arange(100_000)

lrs = np.full(len(steps), values[0])
for boundary, value in zip(boundaries, values[1:]):
    lrs[boundary:] = value

plt.plot(steps, lrs, "-")
plt.axis([0, steps.max(), 0, 0.0105])
plt.xlabel("Step")
plt.ylabel("Learning Rate")
plt.title("Piecewise Constant Scheduling", fontsize=14)
plt.grid(True)
plt.show()
```

```python
# 像指数调度一样，可以手动实现分段恒定调度
def piecewise_constant_fn(epoch):
    if epoch < 5:
        return 0.01
    elif epoch < 15:
        return 0.005
    else:
        return 0.001
```

```python
# 可配置的实现
def piecewise_constant(boundaries, values):
    boundaries = np.array([0] + boundaries)  # [0,5,15] > 0
    values = np.array(values)                # [0.01, 0.005, 0.001]
    def piecewise_constant_fn(epoch):
        return values[(boundaries > epoch).argmax() - 1]
    return piecewise_constant_fn

piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])
```

```python
n_epochs = 25

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)

model = build_model()
optimizer = tf.keras.optimizers.Nadam(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])

model = build_model()
optimizer = tf.keras.optimizers.Nadam(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])
```

```python
# 随堂练习：用回调的方式 实现分段恒定调度，0-50000批次 0.01, 50000-80000:0.005, 80000之后 0.001，看看和keras自带的效果是否一致；参考代码：
# class ExponentialDecay(tf.keras.callbacks.Callback):
#     def __init__(self, n_steps=40_000):
#         super().__init__()
#         self.n_steps = n_steps
#
#     def on_batch_begin(self, batch, logs=None):
#         lr = self.model.optimizer.learning_rate.numpy()
#         new_learning_rate = lr * 0.1 ** (1 / self.n_steps)
#         self.model.optimizer.learning_rate = new_learning_rate
#
#     def on_epoch_end(self, epoch, logs=None):
#         logs = logs or {}
#         logs['lr'] = self.model.optimizer.learning_rate.numpy()

class PiecewiseDecay(tf.keras.callbacks.Callback):
    def __init__(self, boundaries, values):
        super().__init__()
        self.boundaries = np.array([0] + boundaries)
        self.values = np.array(values)
        self.steps = 0

    def on_batch_begin(self, batch, logs):
        self.steps += 1
        lr = self.values[(self.boundaries > self.steps).argmax() - 1]
        self.model.optimizer.learning_rate = lr

    def on_epoch_end(self, epoch, logs):
        logs = logs or {}
        logs['lr'] = self.model.optimizer.learning_rate.numpy()

model = build_model()
optimizer = tf.keras.optimizers.Nadam(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=35,
                    validation_data=(X_valid, y_valid),
                    callbacks=[PiecewiseDecay([50000, 80000], [0.01, 0.005, 0.001])])
```

```python
# 已经见过InverseTimeDecay（幂调度）, ExponentialDecay（指数调度）, and PiecewiseConstantDecay（分段恒定调度）， 可以看下tf.keras.optimizers.schedules的完整列表
for name in sorted(dir(tf.keras.optimizers.schedules)):
    if name[0] == name[0].lower():  # must start with capital letter
        continue
    scheduler_class = getattr(tf.keras.optimizers.schedules, name)  # python的反射： 通过字符串 获取属性/方法
    print(f"• {name} – {scheduler_class.__doc__.splitlines()[0]}")
```

```python
dir(tf.keras.optimizers.schedules)
dir("a")

content = """
1111
222
3333
"""
content.splitlines()
```

### 性能调度（Performance Scheduling）

每N步测量一次验证误差（就像早停一样），并且当误差停止下降时，将学习率降为原来的λ分之一。

```python
model = build_model()
optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
```

```python
# 性能调度 使用ReduceLROnPlateau回调函数
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)  # 连续5个轮次的最好验证损失都没有改善时，将使学习率*0.5
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid),
                    callbacks=[lr_scheduler])
```

```python
plt.plot(history.epoch, history.history["lr"], "bo-")
plt.xlabel("Epoch")
plt.ylabel("Learning Rate", color='b')
plt.tick_params('y', colors='b')
plt.gca().set_xlim(0, n_epochs - 1)
plt.grid(True)

ax2 = plt.gca().twinx()
ax2.plot(history.epoch, history.history["val_loss"], "r^-")
ax2.set_ylabel('Validation Loss', color='r')
ax2.tick_params('y', colors='r')

plt.title("Reduce LR on Plateau", fontsize=14)
plt.show()
```

### 1周期调度（One Cycle Scheduling）

它首先增加初始学习率eta0，使其在训练中途线性增长至eta1。然后，它在训练啊的后半部分将学习率再次线性降低到eta0，通过将学习率降低几个数量级来完成最后几个轮次。使用与找到最优学习率相同的方法来选择最大学习率eta1（损失开始发散的学习率），而初始学习率eta0通常是原来的十分之一。

ExponentialLearningRate 自定义回调会在训练期间，即每个批次结束时更新学习率。它会将学习率乘以一个常数因子。它还会保存每个批次的学习率和损失。由于 logs["loss"] 实际上是自该周期开始以来的平均损失，而想要保存的是批次损失，因此必须计算平均值乘以自该周期开始以来的批次数，以获得迄今为止的总损失，然后减去上一个批次的总损失，以获得当前批次的损失。

```python
K = tf.keras.backend

class ExponentialLearningRate(tf.keras.callbacks.Callback):
    def __init__(self, factor):
        self.factor = factor
        self.rates = []
        self.losses = []

    def on_epoch_begin(self, epoch, logs=None):
        self.sum_of_epoch_losses = 0

    def on_batch_end(self, batch, logs=None):
        mean_epoch_loss = logs["loss"]  # 这个轮次迄今为止的平均损失
        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)
        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses
        self.sum_of_epoch_losses = new_sum_of_epoch_losses
        lr = self.model.optimizer.learning_rate.numpy()
        self.rates.append(lr)
        self.losses.append(batch_loss)
        self.model.optimizer.learning_rate = lr * self.factor
```

```python
# find_learning_rate() 函数使用 ExponentialLearningRate 回调函数训练模型，并返回学习率和相应的批次损失。最后，它将模型及其优化器恢复到初始状态。
def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,
                       max_rate=1):
    init_weights = model.get_weights()
    iterations = math.ceil(len(X) / batch_size) * epochs
    factor = (max_rate / min_rate) ** (1 / iterations)
    init_lr = K.get_value(model.optimizer.learning_rate)
    model.optimizer.learning_rate = min_rate
    exp_lr = ExponentialLearningRate(factor)
    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,
                        callbacks=[exp_lr])
    model.optimizer.learning_rate = init_lr
    model.set_weights(init_weights)
    return exp_lr.rates, exp_lr.losses
```

```python
# plot_lr_vs_loss() 函数绘制了学习率与损失的关系图。最佳学习率（可用作 1 个周期内的最大学习率）位于曲线底部附近。
def plot_lr_vs_loss(rates, losses):
    plt.plot(rates, losses, "b")
    plt.gca().set_xscale('log')
    max_loss = losses[0] + min(losses)
    plt.hlines(min(losses), min(rates), max(rates), color="k")
    plt.axis([min(rates), max(rates), 0, max_loss])
    plt.xlabel("Learning rate")
    plt.ylabel("Loss")
    plt.grid()
```

```python
model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
              metrics=["accuracy"])
```

```python
# 找一周期调度的最佳学习率
import math
import matplotlib.pyplot as plt

batch_size = 128
rates, losses = find_learning_rate(model, X_train, y_train, epochs=1,
                                   batch_size=batch_size)
plot_lr_vs_loss(rates, losses)
```

```python
# OneCycleScheduler 自定义回调会在每个批次开始时更新学习率。
# 在训练的大约一半时间内线性增加学习率，然后线性降低回初始学习率。
class OneCycleScheduler(tf.keras.callbacks.Callback):
    def __init__(self, iterations, max_lr=1e-3, start_lr=None,
                 last_iterations=None, last_lr=None):
        self.iterations = iterations
        self.max_lr = max_lr
        self.start_lr = start_lr or max_lr / 10
        self.last_iterations = last_iterations or iterations // 10 + 1
        self.half_iteration = (iterations - self.last_iterations) // 2
        self.last_lr = last_lr or self.start_lr / 1000
        self.iteration = 0

    def _interpolate(self, iter1, iter2, lr1, lr2):
        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1

    def on_batch_begin(self, batch, logs):
        if self.iteration < self.half_iteration:
            lr = self._interpolate(0, self.half_iteration, self.start_lr,
                                   self.max_lr)
        elif self.iteration < 2 * self.half_iteration:
            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,
                                   self.max_lr, self.start_lr)
        else:
            lr = self._interpolate(2 * self.half_iteration, self.iterations,
                                   self.start_lr, self.last_lr)
        self.iteration += 1
        self.model.optimizer.learning_rate = lr
```

```python
model = build_model()
model.compile(loss="sparse_categorical_crossentropy",
              optimizer=tf.keras.optimizers.SGD(),
              metrics=["accuracy"])
n_epochs = 25
onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs,
                             max_lr=0.1)
history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,
                    validation_data=(X_valid, y_valid),
                    callbacks=[onecycle])
```

## 通过正则化避免过拟合

深度神经网络通常具有数万个参数，有时有数百万个。这使它们非常灵活，意味着它们可以拟合各种各样的复杂数据集。但是，这种巨大的灵活性也使网络易于过拟合训练集。通常需要采用正则化防止这种情况。

之前已经实现了一项正则化技术：早停。而且“批量归一化”被设计用来解决不稳定梯度问题，但它也能起到很好的正则化作用。

其他流行的神经网络正则化技术：l1和l2正则化，以及dropout和最大范数正则化

### l1和l2正则化

```python
# 可以使用l2正则化约束神经网络的连接权重，如果想要稀疏模型（许多权重为0），则可以使用l1正则化
import tensorflow as tf
layer = tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal", kernel_regularizer=tf.keras.regularizers.L2(0.01)) # 用0.01的正则化因为将l2正则化应用于Keras层的连接权重
```

l2()返回一个正则化函数，在训练过程中的每个步骤都将调用该正则化函数来计算正则化损失，添加到最终损失

或者使用 l1(0.1) 进行 ℓ1 正则。化，其因子为 0.1，或者使用 l1_l2(0.1, 0.01) 进行 ℓ1 和 ℓ2 正则化，其因子分别为 0.1 和 0.01。

由于通常希望将相同的正则化函数应用于网络中的所有层，并在所有隐藏层中使用相同的激活函数和相同的初始化策略，因此你可能会发现自己在重复使用相同的参数。这使代码很难看且容易出错。为了避免这种情况，可以尝试使用循环来重构代码。另一种选择是使用Python的functools.partial()函数，该函数允许你为带有任何默认参数值的任何可调用对象创建一个小的包装函数：

```python
from functools import partial

def mul(x, y):
    return x*y

double = partial(mul, 2)
double(3)
double(9)

callable(tf.keras.layers.Dense)  # True
```

```python
from functools import partial
RegularizedDense = partial(tf.keras.layers.Dense, activation="relu", kernel_initializer="he_normal", kernel_regularizer=tf.keras.regularizers.L2(0.01))

model =tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    RegularizedDense(100),
    RegularizedDense(100),
    RegularizedDense(10, activation="softmax")
    ])
```

l2正则化在使用SGD、动量优化和Nesterov动量优化时很好，但不适用于Adam及其变体。如果想使用具有权重衰减的Adam，则不要使用l2正则化：改用AdamW。

```python
# 动量优化 / Nesterov的动量优化
# l1_l2
# partial去搭建每个层

class MyDnn(tf.keras.Model):
    def __init__(self,n_neuron,n_hidden_layers,):
        self.input_layer = tf.keras.input
        self.flatten()
```

### dropout

已被证明是非常成功的：许多先进的神经网络都使用了dropout

这是一个非常简单的算法：在每个训练步骤中，每个神经元（包括输入神经元，但始终不包括输出神经元）都有暂时被“删除”(dropped out)的概率p，这意味着在这个训练步骤中它被完全忽略，但在下一步中可能处于活动状态。超参数p称为dropout率，通常设置为10%～50%：在循环神经网络中在20%～30%，在卷积神经网络中在40%～50%。训练后，神经元不再被“删除”。这就是全部内容（除了一个技术细节）

![一层或多层中的所有神经元随机被删除，输出层除外](./images/neural_network/p14.png)



类比：不能依靠任何一个人来执行任何关键任务，必须将这种专业知识分散到多个人身上。员工必须学会与许多同事（而不仅仅是少数几个）合作。该公司将变得更具弹性。如果一个人辞职，不会有太大的影响。尚不清楚这种想法是否真的适用于公司，但它确实适用于神经网络。

经过dropout训练的神经元不能与其相邻的神经元相互适应。它们必须自己发挥最大的作用，它们也不能过分依赖少数输入神经元，它们必须注意每个输入神经元。它们最终对输入的微小变化不太敏感。最后，将获得一个更有鲁棒性的网络，该网络有更好的泛化能力。

了解dropout能力的另一种方法是认识到在每个训练步骤中都会生成一个独特的神经网络。由于每个神经元都可以存在或不存在，因此共有2^N个可能的网络（其中N是可以被“删除”的神经元的总数）。

这是一个巨大的数字，几乎不可能对同一神经网络进行两次采样。一旦运行了10000个训练步骤，实质上就已经训练了10000个不同的神经网络，每个神经网络只有一个训练实例。这些神经网络显然不是独立的，因为它们共享许多权重，但是它们是完全不同的。所得的神经网络可以看作所有这些较小的神经网络的平均集成。

有一个小而重要的技术细节。假设p=75%：在训练期间的每一步平均只有25%的神经元处于活动状态, 或者说一个神经元训练期间的输出的期望值只有正常的25%；这意味着在训练之后，不使用dropout一个神经元将连接到4倍于训练期间的输入神经元。

为了弥补这一事实，需要训练期间将Dropout层的连接权重乘以4，如果不这样做，会导致网络在预测时输出尺度发生变化，推理和训练不一致，表现不会好。更一般地说，需要在训练期间将连接权重除以保留概率（1-p）

要使用Keras实现dropout，可以使用tf.keras.layers.Dropout层。在训练期间，它会随机丢弃一些输入（将它们设置为0），然后将其余输入除以保留概率。训练之后，它什么都不做；只将输入传递到下一层。以下代码使用0.2的dropout率在每个密集层之前应用dropout正则化：

```python
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=[28, 28]),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(100, activation="relu",
                          kernel_initializer="he_normal"),
    tf.keras.layers.Dropout(rate=0.2),
    tf.keras.layers.Dense(10, activation="softmax")
])

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(X_train, y_train, epochs=10,
                    validation_data=(X_valid, y_valid))
```

训练集的准确率看起来似乎低于验证集的准确率，但这仅仅是因为dropout只在训练过程中起作用，验证集上评估会关掉dropout。

如果在训练结束后（即关闭dropout）在训练集上评估模型，会得到“真实”的训练集准确率，它比验证集准确率和测试集准确率略高：

```python
model.evaluate(X_train, y_train)

model.evaluate(X_test, y_test)
```

由于dropout仅在训练期间激活，因此比较训练损失和验证损失可能会产生误导。具体而言，模型可能会过度拟合训练集，但仍具有相似的训练损失和验证损失。因此，确保在没有使用dropout的情况下评估训练损失（比如，在训练之后）

==dropout确实会显著减慢收敛速度==，但如果调整得当，它通常会产生更好的模型。因此，通常值得为它花费一些额外的时间和精力，尤其是对于大型模型。

如果要基于SELU激活函数对自归一化网络进行正则化，则应使用alpha dropout：这是dropout的一种变体，它保留了输入的均值和标准差。它在与SELU相同的论文中介绍，因为常规的dropout会破坏自归一化。

### 蒙特卡罗（MC）dropout

这个技术可以提高任何训练后的dropout模型的性能，而无须重新训练。

它可以更好地衡量模型的不确定性，只需几行代码即可实现

```python
import numpy as np

y_probas = np.stack([model(X_test, training=True) for sample in range(100)])  # 开了dropout去对测试集做100次预测
y_probas

```

```python
y_probas.shape
y_proba = y_probas.mean(axis=0)
y_proba
```

model(X)类似于model.predict(X)，只是它返回张量而不是NumPy数组，并且它支持training参数。设置training=True可确保Dropout层保持活动状态，因此所有预测都会有所不同。

只对测试集进行100次预测，然后计算它们的平均值。更具体地说，每次调用模型都会返回一个矩阵，每个实例一行，每个类别一列。因为测试集有10000个实例和10个类别中，所以这是一个形状为[10000，10]的矩阵。

堆叠100个这样的矩阵，所以y_probas是一个形状为[100，10000，10]的三维数组。一旦对第一个维度(axis=0)进行平均，就会得到y_proba，一个形状为[10000，10]的数组，就像通过单次预测得到的一样。

在启用dropout的情况下对多次预测的结果进行平均可以得到蒙特卡罗估计值，该估计值通常比关闭dropout的单次预测的结果更可靠。例如，看看模型对Fashion MNIST测试集中第一个实例的预测（关闭dropout）：

```python
print(model.predict(X_test[:1]).round(3))
print(y_proba[0].round(3))  # MC Dropout会把置信度往下调
```

MC dropout倾向于提高模型概率估计的可靠性。-> 它意味着更不可能非常自信却做错事情，准确了解哪些其他类别最优可能也很有用（自动驾驶汽车了解识别到的可能的各种标志）

```python
# 查看概率估计的标准差
y_std = y_probas.std(axis=0)
y_std[0].round(3)
# y_std[0].round(3)

# 第9类的概率估计值存在很大差异：标准差0.175. 应该把估计的概率和标准差结合在一起 （代表概率有很大不确定性），如果构建一个风险敏感的任务，会谨慎对待这样不确定的预测
```

```python
y_pred = y_proba.argmax(axis=1)
accuracy = (y_pred == y_test).sum() / len(y_test)
accuracy
```

使用的蒙特卡罗样本数量（在此样例中为100）是可以调整的超参数。数值越高，预测及其不确定性估计的精度就越高。但是，如果将其加倍，则推理时间也将加倍。此外，多于一定数量的样本，你几乎看不到任何改善。因此，你的工作就是在延迟和精度之间找到适当的平衡，具体取决于应用。

如果模型包含在训练过程中以特殊方式运行的其他层（例如BatchNormalization层），则不应像刚才那样强制采用训练模式(model(..., training=True))。相反，应该使用以下MCDropout类来替换Dropout层

解释：
训练模式：BN 层用当前 mini-batch 的均值/方差，并且还会更新 moving mean/var。 推理模式：BN 层用累计的 moving mean/var（稳定的全局统计量），不再更新。 使用model(..., training=True)意味着BN层的推理也按训练模式进行，不符合BN层的工作性质

```python
class MCDropout(tf.keras.layers.Dropout):
    def call(self, inputs, training=False):
        return super().call(inputs, training=True)

```

在这里，只是继承了Dropout层，并覆盖call()方法来将training参数强制设置为True。

同样，可以通过继承AlphaDropout来定义MCAlphaDropout类。如果要从头开始创建模型，则只需使用MCDropout而不是Dropout。

但是，如果有一个已经使用Dropout训练过的模型，那么需要创建一个与现有模型相同的新模型（只不过不使用Dropout而是使用MCDropout），然后将现有模型的权重复制到新模型。简而言之，MC dropout是一种很棒的技术，可以提升dropout模型性能并提供更好的不确定性估计。当然，由于这只是训练期间的常规dropout，因此它也像正则化函数。

### 最大范数正则化

另一种流行的神经网络正则化技术称为**最大范数 (max-norm) 正则化**：
对于每个神经元，它会限制传入连接的权重 **w**，使得 $\|w\|_2 \leq r$，其中 *r* 是最大范数超参数，$\|\cdot\|_2$ 是 $\ell_2$ 范数。

最大范数正则化不会把正则化损失项加到总体损失函数中。取而代之的是，通常在每个训练步骤后通过计算 $\|w\|_2$ 来实现，如有需要，请重新缩放：

$$
w \leftarrow w \cdot \frac{r}{\max(r, \|w\|_2)}
$$

最大范数正则化会增加正则化的强度，并有助于降低过拟合风险。
最大范数正则化还可以帮助缓解不稳定的梯度问题（如果未使用“批量归一化”）。

```python
dense = tf.keras.layers.Dense(100, activation="relu", kernel_initializer="he_normal", kernel_constraint=tf.keras.constraints.max_norm(1.))
```

```python
# 随堂练习： 填充如下代码，最终训练 用了最大范数正则化技术的 神经网络

MaxNormDense = ... # todo：使用partial 冻结Dense的除了神经元数量的其他参数， 指定使用最大范数正则化

model = ... # todo：使用Sequential搭建 2个隐藏层，每个都100神经元的网络 （使用上MaxNormDense）
optimizer = ... # todo: 创建使用动量优化的优化器

... # todo: 编译模型
history = ... # todo: 训练模型
```

每次训练迭代后，模型的fit()方法会调用由max_norm()返回的对象，将层的权重传递给该对象，并获得返回的缩放权重，然后替换该层的权重。

如果需要，可以定义自己的自定义约束函数，并将其用作kernel_constraint。还可以通过设置bias_constraint参数来约束偏置项。

max_norm()函数的axis参数默认为0。密集层(Dense)通常具有形状为［输入数量，神经元数量］的权重，因此axis=0意味着最大范数约束将独立应用于每个神经元的权重向量。

如果要将最大范数约束用于卷积层，请确保正确设置max_norm()约束的axis参数（通常axis=[0，1，2]）。

## 总结各种技术

想知道该用哪种技术，取决于任务，没有明确共识，总结大多数情况下可以很好工作的配置，但不要把它当成规定

| 超参数     | 默认值                                             |
| ---------- | -------------------------------------------------- |
| 核初始化   | He 初始化                                          |
| 激活函数   | 对于浅层网络，采用 ELU；对于深度网络，采用 Swish   |
| 归一化     | 对于浅层网络，不需要；对于深度网络，采用批量归一化 |
| 正则化     | 早停，如有需要，可采用权重衰减                     |
| 优化器     | Nesterov 加速梯度或 AdamW                          |
| 学习率调度 | 性能调度或者 1 周期调度                            |



| 超参数     | 默认值                         |
| ---------- | ------------------------------ |
| 核初始化   | LeCun 初始化                   |
| 激活函数   | SELU                           |
| 归一化     | 不需要                         |
| 正则化     | 如果需要，则采用 Alpha dropout |
| 优化器     | Nesterov 加速梯度              |
| 学习率调度 | 性能调度或者 1 周期调度        |

使用SELU不要忘了归一化输入特征！

如果可以找到解决类似问题的神经网络，就应该尝试重用部分神经网络；如果有大量未标记的数据，则应进行无监督预训练；如果有相似任务的大量标记的数据，则应该在辅助任务上进行训练。

虽然表格应该能涵盖大多数情况，但以下是一些例外的情况：

- 如果需要稀疏模型，则可以使用 l1 正则化（可以选择在训练后将很多小的权重归零）。如果需要更稀疏的模型，则可以使用 TensorFlow 模型优化工具包。l1正则化会破坏自归一化，因此在这种情况下，应使用默认配置。

- 如果需要低延迟模型（能够快速执行预测的模型），则可能需要使用更少的层，使用快速激活函数，例如 ReLU 或 leaky ReLU，并在训练后将批量归一化层折叠到前面的层中。拥有稀疏的模型也有帮助。最后，可能想把模型浮点精度从 32 位降低到 16 位甚至 8 位。再一次检查 TF-MOT。

- 如果要构建风险感知的应用，或者推理延迟不是很重要的情况下，则可以使用 MC Dropout 来提高性能并获得更可靠的概率估计以及不确定性估计。

有了这些准则，可以训练非常深的网络了！

除了目前用过的方便的Keras API，有时候可能需要进行更多的控制，例如，编写自定义损失函数或调整训练算法。对于这种情况，则需要使用TensorFlow的较低级API

```python
# 随堂练习：实验目标：Dropout vs AlphaDropout 对比（针对 SELU 激活函数）

def build_model(use_alphaDropout=False):
  ...

```

